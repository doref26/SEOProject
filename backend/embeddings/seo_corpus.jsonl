{"id": "660fc4b6-3a72-49ab-96dc-4afe16068e25", "url": "https://developers.google.com/search/docs/essentials", "source_domain": "developers.google.com", "title": "Google Search Essentials (formerly Webmaster Guidelines) | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Google Search Essentials (formerly Webmaster Guidelines) | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nGoogle Search Essentials\nThe Google Search Essentials make up the core parts of what makes your web-based content\n(web pages, images, videos, or other publicly-available material that Google finds on the web)\neligible to appear and perform well on Google Search:\nTechnical requirements\n: What Google needs from a web page to show it in Google Search.\nSpam policies\n: The behaviors and tactics that can lead to lower ranking or being completely omitted from Google Search results.\nKey best practices\n: The main things that can help improve how your site appears in Google Search results.\nIt doesn't cost any money to appear in Google Search results, no matter what anyone tries to tell you.\nIt's important to note that just because a page meets all of these requirements and\nbest practices, doesn't mean that Google will crawl, index, or serve its content. Learn more\nabout\nHow Search Works\n.\nTechnical requirements\nThe\ntechnical requirements\ncover the bare\nminimum that Google Search needs from a web page in order to show it in search results. There\nare actually very few technical things you need to do to a web page; most sites pass the\ntechnical requirements without even realizing it.\nSpam policies\nThe\nspam policies\ndetail the behaviors\nand tactics that can lead to a page or an entire site being ranked lower or completely omitted\nfrom Google Search. Sites that focus on providing the best content and experience for people", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1950}
{"id": "5f636a7a-d0ff-4c0d-a28f-cacbe912a3e5", "url": "https://developers.google.com/search/docs/essentials", "source_domain": "developers.google.com", "title": "Google Search Essentials (formerly Webmaster Guidelines) | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "and uphold the spirit of our principles are more likely to do well in Google Search results.\nKey best practices\nWhile there are many things you can do to\nimprove your site's SEO\n,\nthere are a few core practices that can have the most impact on your web content's ranking and\nappearance on Google Search:\nCreate helpful, reliable, people-first content\n.\nUse words that people would use to look for your content, and place those words in\nprominent locations on the page, such as the\ntitle and main heading\nof a page, and other descriptive locations such as\nalt text\nand link text.\nMake your links crawlable\nso that Google can find other pages on your site via the links on your page.\nTell people about your site. Be active in communities where you can tell like-minded people\nabout your services and products that you mention on your site.\nIf you have other content, such as\nimages\n,\nvideos\n,\nstructured data\n,\nand\nJavaScript\n,\nmake sure you're following those specific best practices so that we can understand those parts\nof your page too.\nEnhance\nhow your site appears\non Google Search\nby enabling features that make sense for your site.\nIf you have content that shouldn't be found in search results or you want to opt out entirely,\nuse the appropriate method for\ncontrolling how your content appears in Google Search\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-02-04 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1951, "chunk_char_end": 3610}
{"id": "737d021a-4093-42cf-add5-2051cdb96c93", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nSpam policies for Google web search\nIn the context of Google Search, spam refers to techniques used to deceive users or manipulate\nour Search systems into ranking content highly. Our spam policies help protect users and improve the\nquality of Search results. To be eligible to appear in Google web search results (web pages,\nimages, videos, news content or other material that Google finds from across the web), content\nshouldn't violate\nGoogle Search's overall policies\nor the spam policies listed on this page. These policies apply to all web search results,\nincluding those from Google's own properties.\nWe detect policy-violating practices both through automated systems and, as needed,\nhuman review that can result in a\nmanual action\n.\nSites that violate our policies may rank lower in results or not appear in results at all.\nIf you believe that a site is violating Google's spam policies, let us know by\nfiling a search quality user report\n.\nWe're focused on developing scalable and automated solutions to problems, and we'll use these\nreports to further improve our spam detection systems.\nOur policies cover common spam practices, but Google may act against any type of spam practices\nwe detect.\nCloaking\nCloaking refers to the practice of presenting different content to users and search engines\nwith the intent to manipulate search rankings and mislead users. Examples of cloaking include:\nShowing a page about travel destinations to search engines while showing a page about", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1985}
{"id": "05b8a528-c985-4a67-aad4-daded832a5c3", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "discount drugs to users\nInserting text or keywords into a page only when the user agent that is requesting the\npage is a search engine, not a human visitor\nIf your site uses technologies that search engines have difficulty accessing, like\nJavaScript\nor\nimages\n,\nsee our recommendations for making that content accessible to search engines and users without cloaking.\nIf a site is hacked, it's not uncommon for the hacker to use cloaking to make the hack harder\nfor the site owner to detect. Read more about\nfixing hacked sites\nand avoiding being hacked.\nIf you operate a paywall or a content-gating mechanism, we don't consider this to be cloaking\nif Google can see the full content of what's behind the paywall just like any person who has\naccess to the gated material and if you follow our\nFlexible Sampling general guidance\n.\nDoorway abuse\nDoorway abuse is when sites or pages are created to rank for specific, similar search queries. They lead\nusers to intermediate pages that are not as useful as the final destination. Examples of\ndoorway abuse include:\nHaving multiple websites with slight variations to the URL and home page to maximize their reach for any specific query\nHaving multiple domain names or pages targeted at specific regions or cities that funnel users to one page\nGenerating pages to funnel visitors into the actual usable or relevant portion of a site\nCreating substantially similar pages that are closer to search results than a clearly defined, browseable hierarchy\nExpired domain abuse\nExpired domain abuse is where an expired domain name is purchased and repurposed primarily to\nmanipulate search rankings by hosting content that provides little to no value to users. Illustrative\nexamples include, but are not limited to:\nAffiliate content on a site previously used by a government agency\nCommercial medical products being sold on a site previously used by a non-profit medical charity\nCasino-related content on a former elementary school site\nHacked content", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1986, "chunk_char_end": 3973}
{"id": "a9873415-1ea6-4f50-bffa-57d0a04bb0c1", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Hacked content is any content placed on a site without permission, due to vulnerabilities in a\nsite's security. Hacked content gives poor search results to our users and can potentially\ninstall malicious content on their machines. Examples of hacking include:\nCode injection\n: When hackers gain access to your website, they might try\nto inject malicious code into existing pages on your site. This often takes the form of\nmalicious JavaScript injected directly into the site, or into iframes.\nPage injection\n: Sometimes, due to security flaws, hackers are able to add new\npages to your site that contain spammy or malicious content. These pages are often meant to\nmanipulate search engines or to\nattempt phishing\n.\nYour existing pages might not show signs of hacking, but these newly-created pages could\nharm your site's visitors or your site's performance in search results.\nContent injection\n: Hackers might also try to subtly manipulate existing pages on\nyour site. Their goal is to add content to your site that search engines can see but which\nmay be harder for you and your users to spot. This can involve adding\nhidden links or hidden text\nto a page by using CSS or HTML, or it can involve more complex changes like\ncloaking\n.\nRedirects\n: Hackers might inject malicious code to your website that redirects some\nusers to harmful or spammy pages. The kind of redirect sometimes depends on the referrer,\nuser agent, or device. For example, clicking a URL in Google Search results could redirect\nyou to a suspicious page, but there is no redirect when you visit the same URL directly\nfrom a browser.\nHere are our tips on\nfixing hacked sites\nand avoiding being hacked.\nHidden text and link abuse\nHidden text or link abuse is the practice of placing content on a page in a way solely to manipulate\nsearch engines and not to be easily viewable by human visitors. Examples of hidden text or\nlink abuse include:\nUsing white text on a white background\nHiding text behind an image", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3974, "chunk_char_end": 5950}
{"id": "d65d52fd-1451-428b-9492-5ed2350d3d64", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Using CSS to position text off-screen\nSetting the font size or opacity to 0\nHiding a link by only linking one small character (for example, a hyphen in the middle of a paragraph)\nThere are many web design elements today that utilize showing and hiding content in a dynamic\nway to improve user experience; these elements don't violate our policies:\nAccordion or tabbed content that toggle between hiding and showing additional content\nSlideshow or slider that cycles between several images or text paragraphs\nTooltip or similar text that displays additional content when users interact with over an element\nText that's only accessible to screen readers and is intended to improve the experience\nfor those using screen readers\nKeyword stuffing\nKeyword stuffing refers to the practice of filling a web page with keywords or numbers in an\nattempt to manipulate rankings in Google Search results. Often these keywords appear in a list\nor group, unnaturally, or out of context. Examples of keyword stuffing include:\nLists of phone numbers without substantial added value\nBlocks of text that list cities and regions that a web page is trying to rank for\nRepeating the same words or phrases so often that it sounds unnatural. For example:\nUnlimited app store credit. There are so many sites that claim to offer app store\ncredit for $0 but they're all fake and always mess up with users looking for unlimited app\nstore credits. You can get limitless credits for app store right here on this website.\nVisit our unlimited app store credit page and get it today!\nLink spam\nLink spam is the practice of creating links to or from a site primarily for the purpose of\nmanipulating search rankings. The following are examples of link spam:\nBuying or selling links for ranking purposes. This includes:\nExchanging money for links, or posts that contain links\nExchanging goods or services for links\nSending someone a product in exchange for them writing about it and including a link", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5951, "chunk_char_end": 7914}
{"id": "c7d899b3-a446-4cc8-a8e4-056109846b23", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Excessive link exchanges (\"Link to me and I'll link to you\") or partner pages exclusively for the sake of cross-linking\nUsing automated programs or services to create links to your site\nRequiring a link as part of a Terms of Service, contract, or similar arrangement without\nallowing a third-party content owner the choice of\nqualifying the outbound link\nText advertisements or text links that don't block ranking credit\nAdvertorials or native advertising where payment is received for articles that include\nlinks that pass ranking credit, or links with optimized anchor text in articles, guest posts,\nor press releases distributed on other sites. For example:\nThere are many\nwedding rings\non the market.\nIf you want to have a\nwedding\n, you will have to pick\nthe\nbest ring\n. You will also need to\nbuy flowers\nand a\nwedding dress\n.\nLow-quality directory or bookmark site links\nKeyword-rich, hidden, or low-quality links embedded in widgets that are distributed across various sites\nWidely distributed links in the footers or templates of various sites\nForum comments with optimized links in the post or signature, for example:\nThanks, that's great info!\n- Paul\npaul's pizza\nsan diego pizza\nbest pizza san diego\nCreating low-value content primarily for the purposes of manipulating linking and ranking signals\nGoogle does understand that buying and selling links is a normal part of the economy of the\nweb for advertising and sponsorship purposes. It's not a violation of our policies to have\nsuch links as long as they are\nqualified\nwith a\nrel=\"nofollow\"\nor\nrel=\"sponsored\"\nattribute value to the\n<a>\ntag.\nMachine-generated traffic\nMachine-generated traffic (also called\nautomated traffic\n)\nrefers to the practice of sending automated queries to Google. This includes scraping results for\nrank-checking purposes or other types of automated access to Google Search conducted without\nexpress permission. Machine-generated traffic consumes resources and interferes with our ability", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7915, "chunk_char_end": 9892}
{"id": "95ea705e-7bde-4395-bb75-1487ca494650", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "to best serve users. Such activities violate our spam policies and the\nGoogle Terms of Service\n.\nMalware and malicious practices\nGoogle checks to see whether websites host malware or unwanted software that negatively\naffects the user experience.\nMalware\nis any\nsoftware or mobile application specifically designed to harm a computer, a mobile device, the\nsoftware it's running, or its users. Malware exhibits malicious behavior that can include\ninstalling software without user consent and installing harmful software such as viruses. Site\nowners sometimes don't realize that their downloadable files are considered malware, so these\nbinaries might be hosted inadvertently.\nUnwanted software\nis an executable file or mobile application that engages in behavior that is deceptive, unexpected,\nor that negatively affects the user's browsing or computing experience. Examples include\nsoftware that switches your home page or other browser settings to ones you don't want, or\napps that leak private and personal information without proper disclosure.\nSite owners should make sure they don't violate the\nUnwanted Software Policy\nand\nfollow our guidelines\n.\nMisleading functionality\nMisleading functionality refers to the practice of intentionally creating sites that trick users\ninto thinking they would be able to access some content or services but in reality can't. Examples\nof misleading functionality include:\nA site with a fake generator that claims to provide app store credit but doesn't actually provide the credit\nA site that claims to provide certain functionality (for example, PDF merge, countdown\ntimer, online dictionary service), but intentionally leads users to deceptive ads rather\nthan providing the claimed services\nScaled content abuse\nScaled content abuse is when many pages are generated for the primary purpose of manipulating\nsearch rankings and not helping users. This abusive practice is typically focused on creating large", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9893, "chunk_char_end": 11838}
{"id": "5a67374e-d2e2-41c8-b5cd-23e3b40ff081", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "amounts of unoriginal content that provides little to no value to users, no matter how it's created.\nExamples of scaled content abuse include, but are not limited to:\nUsing generative AI tools or other similar tools to generate many pages without adding value\nfor users\nScraping feeds, search results, or other content to generate many pages (including through\nautomated transformations like synonymizing, translating, or other obfuscation techniques), where\nlittle value is provided to users\nStitching or combining content from different web pages without adding value\nCreating multiple sites with the intent of hiding the scaled nature of the content\nCreating many pages where the content makes little or no sense to a reader but contains search\nkeywords\nIf you're hosting such content on your site,\nexclude it from Search\n.\nScraping\nScraping refers to the practice of taking content from other sites, often through automated means,\nand hosting it with the purpose of manipulating search rankings. Examples of abusive scraping include:\nRepublishing content from other sites without adding any original content\nor value, or even citing the original source\nCopying content from other sites, modify it only slightly (for example, by\nsubstituting synonyms or using\nautomated techniques\n),\nand republish it\nReproducing content feeds from other sites without providing some type of unique\nbenefit to the user\nCreating sites dedicated to embedding or compiling content, such as videos, images, or other media\nfrom other sites, without substantial added value to the user\nSite reputation abuse\nSite reputation abuse is a tactic where third-party content is published on a host site mainly\nbecause of that host's already-established ranking signals, which it has earned primarily from its\nfirst-party content. The goal of this tactic is for the content to rank better than it could\notherwise on its own.\nHaving third-party content alone isn't a violation of the site reputation abuse policy; it's only", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11839, "chunk_char_end": 13833}
{"id": "65b62f58-a42a-43ef-82c3-33fea708b71d", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "a violation if the third-party content is published on a host site mainly because of that host\nsite's already-established ranking signals. Examples of site reputation abuse include, but are not\nlimited to:\nAn educational site hosting a page about  sponsored reviews of payday loans written by a third-party that\ndistributes the same page to other sites across the web\nA medical site hosting a third-party advertising page about \"best casinos\" that readers\nwouldn't expect and that's being placed on the site to rank better due to the established site's\nranking signals\nA movie review site hosting third-party pages about topics that would be confusing to users to\nfind on a movie review site (such as \"ways to buy followers on social media sites\", the \"best\nfortune teller sites\", and the \"best essay writing services\")\nA news site hosting coupons provided by a third-party white-label service where the main\nreason for publishing the coupons on the news site is to capitalize on the news\nsite's reputation\nAn established first party site branches out into a new area primarily using freelance content\nbecause this content will rank better on the first-party site than it would have otherwise\nIf you're hosting pages that violate this policy, learn how to\ncorrect this issue\n.\nExamples that are\nNOT\nconsidered site reputation abuse include:\nWire service or press release service sites\nNews publications that have syndicated news content from other news publications\nSites designed to allow user-generated content, such as a forum website or comment sections\nColumns, opinion pieces, articles, and other work of an editorial nature\nThird-party content (for example, \"advertorial\" or \"native advertising\" type pages)\nwhere the purpose is to share content directly\nto readers (such as through promotion within the publication itself), rather than hosting the\ncontent to manipulate search rankings\nUsing affiliate links throughout a page, with\nlinks treated appropriately\n,", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13834, "chunk_char_end": 15803}
{"id": "86d8c37e-e4e0-4c35-a3cf-e571516c7887", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "or embedding third-party ad units throughout a page\nCoupons that are sourced directly from merchants and other businesses that serve consumers\nSneaky redirects\nRedirecting is the act of sending a visitor to a different URL than the one they initially\nrequested. Sneaky redirecting is the practice of doing this maliciously in order to either show users and search\nengines different content or show users unexpected content that does not fulfill their\noriginal needs. Examples of sneaky redirects include:\nShowing search engines one type of content while redirecting users to something significantly\ndifferent\nShowing desktop users a normal page while redirecting mobile users to a completely\ndifferent spam domain\nWhile sneaky redirection is a type of spam,  there are many legitimate, non-spam reasons to\nredirect one URL to another. Examples of legitimate redirects include:\nMoving your site to a new address\nConsolidating several pages into one\nRedirecting users to an internal page once they are logged in\nWhen examining if a redirect is sneaky, consider whether or not the redirect is intended to\ndeceive either the users or search engines. Learn more about how to appropriately\nemploy redirects on your site\n.\nThin affiliation\nThin affiliation is the practice of publishing content with product affiliate links where the\nproduct descriptions and reviews are copied directly from the original merchant without any\noriginal content or added value.\nAffiliate pages can be considered thin if they are a part of a program that distributes its\ncontent across a network of affiliates without providing additional value. These sites often\nappear to be cookie-cutter sites or templates with the same or similar content replicated\nwithin the same site or across multiple domains or languages. If a Search results page\nreturned several of these sites, all with the same content, thin affiliate pages would create\na frustrating user experience.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15804, "chunk_char_end": 17742}
{"id": "53acb110-5504-494c-8595-63f0b880a582", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Not every site that participates in an affiliate program is a thin affiliate. Good affiliate\nsites add value by offering meaningful content or features. Examples of good affiliate pages include offering\nadditional information about price, original product reviews, rigorous testing and ratings,\nnavigation of products or categories, and product comparisons.\nUser-generated spam\nUser-generated spam is spammy content added to a site by users through a channel intended\nfor user content. Often site owners are unaware of the spammy content. Examples of spammy\nuser-generated content include:\nSpammy accounts on hosting services that anyone can register for\nSpammy posts on forum threads\nComment spam on blogs\nSpammy files uploaded to file hosting platforms\nHere are several tips on how to\nprevent abuse of your site's public areas\n.\nHere are our tips on\nfixing hacked sites\nand avoiding being hacked.\nOther practices that can lead to demotion or removal\nLegal removals\nWhen we receive a significant volume of\nvalid copyright removal requests\ninvolving a given site,\nwe are able to use that\nto demote other content from the site in our results. This way, if there is other\ninfringing content, people are less likely to encounter it versus the original content. We\napply similar demotion signals to complaints involving defamation, counterfeit goods, and\ncourt-ordered removals. In the case of child sexual abuse material (CSAM), we always remove\nsuch content when it is identified and we demote all content from sites with a significant\nproportion of CSAM content.\nPersonal information removals\nIf we process a significant volume of personal information removals involving a site with\nexploitative removal practices\n,\nwe demote other content from the site in our results.\nWe also look to see\nif the same pattern of behavior is happening with other sites and, if so, apply demotions to\ncontent on those sites. We may apply similar demotion practices for sites that receive a significant", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17743, "chunk_char_end": 19725}
{"id": "d81015c3-b812-4653-b3e2-ccee68f1f971", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "volume of removals of content involving\ndoxxing content\n,\nexplicit\npersonal imagery created or shared without consent\n, or\nexplicit\nnon-consensual fake content\n.\nPolicy circumvention\nIf a site continues to engage in actions intended to bypass our spam policies or\ncontent policies for Google Search\n,\nwe may take appropriate action which may include restricting or removing eligibility for\nsome of our search features (for example, Top Stories, Discover) and taking broader action in\nGoogle Search (for example, removing more sections of a site from Search results). Circumvention\nincludes but is not limited to:\nUsing existing or creating new subdomains, subdirectories, or sites with the intention of\ncontinuing to violate our policies\nUsing other methods intended to continue distributing content or engaging in a behavior\nthat aims to violate our policies\nScam and fraud\nScam and fraud come in many forms, including but not limited to impersonating an official\nbusiness or service through imposter sites, intentionally displaying false information about\na business or service, or otherwise attracting users to a site on false pretenses. Using\nautomated systems, Google seeks to identify pages with scammy or fraudulent content and\nprevent them from showing up in Google Search results. Examples of online scams and fraud include:\nImpersonating a well-known business or service provider to trick users into paying money\nto the wrong party\nCreating deceptive sites pretending to provide official customer support on behalf of a\nlegitimate business or provide fake contact information of such business\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-10 UTC.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19726, "chunk_char_end": 21668}
{"id": "b1ef182b-1b7c-4643-a0f7-e1d5bdccba26", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nSearch Engine Optimization (SEO) Starter Guide\nWhen you built your website, you likely created it with your users in mind, trying to make it easy\nfor them to find and explore your content. One of those users is a search engine, which helps\npeople discover your content. SEO—short for search engine optimization—is about\nhelping search engines understand your content, and helping users find your site and make a\ndecision about whether they should visit your site through a search engine.\nThe\nSearch Essentials\noutline the most important elements of\nwhat makes your website eligible to appear on Google Search. While there's no guarantee that any\nparticular site will be added to Google's index, sites that follow the\nSearch Essentials are\nmore likely to show up in Google's search results\n. SEO is about taking the next step and\nworking on improving your site's presence in Search\n. This guide will walk you through some\nof the most common and effective improvements you can do on your site.\nThere are no secrets here that'll automatically rank your site first in Google (sorry!). In fact\nsome of the suggestions might not even apply to your business, but following the best practices\nwill hopefully make it easier for search engines (not just Google) to crawl, index, and understand\nyour content.\nHow does Google Search work?\nGoogle is a fully automated search engine that uses programs called crawlers to explore the web", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1915}
{"id": "1dab38fb-e487-4fb4-a5b3-75242e58b746", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "constantly, looking for pages to add to our index. You usually don't need to do anything except\npublish your site on the web. In fact, the vast majority of sites listed in our results are found\nand added automatically as we crawl the web. If you're hungry for more, we have documentation\nabout how\nGoogle discovers, crawls, and serves web pages\n.\nHow long until I see impact in search results?\nEvery change you make will take some time to be reflected on Google's end. Some changes might take\neffect in a few hours, others could take several months. In general, you likely want to wait a few\nweeks to assess whether your work had beneficial effects in Google Search results. Keep in mind\nthat not all changes you make to your website will result in noticeable impact in search results;\nif you're not satisfied with your results and your business strategies allow it, try\niterating with the changes and see if they make a difference.\nHelp Google find your content\nBefore you actually do anything mentioned in this section, check if Google has already found your\ncontent (maybe you don't need to do anything!). Try searching on Google for your site with the\nsite: search operator\n.\nIf you see results pointing to your site, you're in the index. For example, a search for\nsite:wikipedia.org\nreturns\nthese results\n.\nIf you don't see your site, check out the\ntechnical requirements\nto make sure there's nothing technically preventing your site from showing in Google Search, and\nthen come back here.\nGoogle primarily finds pages through links from other pages it already crawled. In many cases,\nthese are other websites that are linking to your pages. Other sites linking to you is something\nthat happens naturally over time, and you can also encourage people to discover your content by\npromoting your site\n.\nIf you're open to a little technical challenge, you could also\nsubmit a sitemap\n—which is a\nfile that contains all the URLs on your site that you care about. Some content management systems", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1916, "chunk_char_end": 3910}
{"id": "a0be6fcf-b405-4805-b70a-1db82a2fa309", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "(CMS) may even do this automatically for you. However this isn't required, and you should first\nfocus on making sure\npeople know about your site\n.\nCheck if Google can see your page the same way a user does\nWhen Google crawls a page, it should ideally\nsee the page the same way an average user does\n.\nFor this, Google needs to be able to access the same resources as the user's browser. If your site\nis hiding important components that make up your website (like\nCSS\nand\nJavaScript\n), Google might\nnot be able to understand your pages, which means they might not show up in search results or rank\nwell for the terms you're targeting.\nIf your pages have different information depending on the user's physical location, make sure\nyou're satisfied with the information that Google sees from its crawler's location, which is generally\nthe US.\nTo check how Google sees your page, use the\nURL Inspection Tool in Search Console\n.\nDon't want a page in Google's search results?\nIt might be important for you to opt out your site as a whole or sections of it from appearing in\nsearch results. For example, you might not want your posts about your new embarrassing haircut to\nshow up in search results. Google supports various ways that lets you opt out of crawling and\nindexing of your URLs. If you need to block some files, directories, or even your whole site from\nGoogle Search, check out our guide about\nways to prevent content from appearing in search results\n.\nOrganize your site\nWhen you're setting up or redoing your site, it can be good to organize it in a logical way because it\ncan help search engines and users understand how your pages relate to the rest of your site. Don't\ndrop everything and start reorganizing your site right now though: while these suggestions can be\nhelpful long term (especially if you're working on a larger website), search engines will likely\nunderstand your pages as they are right now, regardless of how your site is organized.\nUse descriptive URLs", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3911, "chunk_char_end": 5890}
{"id": "12ab2ad8-ef5b-43c2-a9c8-42fd3abb296a", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Parts of the URL can be displayed in search results as breadcrumbs, so users can also use the URLs\nto understand whether a result will be useful for them.\nAn illustration that shows a text result in Google Search with callouts that label specific visible URL visual elements, including the domain and breadcrumb\nDomain\nBreadcrumb\nGoogle learns breadcrumbs automatically based on the words in the URL, but you can also influence\nthem with\nstructured data\nif you\nlike a technical challenge. Try to include words in the URL that may be useful for users; for\nexample:\nhttps://www.example.com/pets/cats.html\nA URL that only contains random identifiers is less helpful for users; for example:\nhttps://www.example.com/2/6772756D707920636174\nGroup topically similar pages in directories\nIf you have more than a few thousand URLs on your site, how you organize your content may have\neffects on how Google crawls and indexes your site. Specifically, using directories (or folders)\nto group similar topics can help Google learn how often the URLs in individual directories\nchange.\nFor example, consider the following URLs:\nhttps://www.example.com/policies/return-policy.html\nhttps://www.example.com/promotions/new-promos.html\nThe content in the\npolicies\ndirectory seldomly changes, however the content in the\npromotions\ndirectory likely changes very often. Google can learn this information\nand crawl the different directories at different frequencies. To learn more about search-friendly\nsite structures, check out our\nguide for ecommerce sites\n,\nfor which a good URL structure is more important as they tend to be larger.\nReduce duplicate content\nSome websites show the same content under different URLs, which is called\nduplicate content\n.\nSearch engines choose a single URL (the\ncanonical\nURL) to show users, per piece of content.\nHaving duplicate content on your site is not a violation of our spam policies, but it can be a bad", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5891, "chunk_char_end": 7813}
{"id": "a8a79494-b4fa-4bd0-b177-d6422b44765e", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "user experience and search engines might waste crawling resources on URLs that you don't even care\nabout. If you're feeling adventurous, it's worth figuring out if you can\nspecify a canonical version\nfor your pages. But if you don't canonicalize your URLs yourself, Google will try to automatically\ndo it for you.\nWhen working on canonicalization, try to ensure that each piece of content on your site is only\naccessible through one individual URL; having two pages that contain the same\ninformation about your promotions can be a confusing user experience (for example, people\nmight wonder which is the right page, and whether there's a difference between the two).\nIf you have multiple pages that have the same information, try setting up a\nredirect\nfrom non-preferred URLs to a\nURL that best represents that information. If you can't redirect, use the\nrel=\"canonical\"\nlink\nelement instead. But again, don't worry too much about this;\nsearch engines can generally figure this out for you on their own most of the time.\nMake your site interesting and useful\nCreating content that people find compelling and useful will likely influence your website's presence in search\nresults more than any of the other suggestions in this guide. While \"compelling and useful content\" can\nmean different things to different people, content like this generally shares some common attributes,\nsuch as:\nThe text is easy-to-read and well organized\n: Write content naturally and make sure the\ncontent is well written, easy to follow, and free of spelling and grammatical mistakes. Break\nup long content into paragraphs and sections, and provide headings to help users navigate your\npages.\nThe content is unique\n:\nWhen you're writing new content, don't copy others' content in part or in its entirety: create\nthe content yourself based on what you know about the topic. Don't just rehash what others\nalready published.\nThe content is up-to-date\n: Check in on previously published content and update it as", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7814, "chunk_char_end": 9798}
{"id": "d9b96451-75f9-407d-a332-727922fff737", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "needed, or even delete it if it's not relevant anymore.\nThe content is\nhelpful, reliable, and people-first\n:\nBe sure that you're writing content that your readers will find helpful and reliable. For\nexample, providing expert or experienced sources can help people understand your articles' expertise.\nExpect your readers' search terms\nThink about the words that a user might search for to find a piece of your content. Users who know\na lot about the topic might use different keywords in their search queries than someone who is new\nto the topic. For example, some users might search for \"charcuterie\", while others might search\nfor \"cheese board\". Anticipating these differences in search behavior and writing with your\nreaders in mind could produce positive effects on how your site performs in search results.\nHowever, don't worry if you don't anticipate every variation of how someone might seek your\ncontent. Google's language matching systems are sophisticated and can understand how your page\nrelates to many queries, even if you don't explicitly use the exact terms in them.\nAvoid distracting advertisements\nWhile ads are a part of the internet and are meant to be seen by users, don't let them become\noverly distracting or prevent your users from reading your content. For example, advertisements,\nor\ninterstitial pages\n(pages\ndisplayed before or after the content you're expecting) that make it difficult to use the website.\nLink to relevant resources\nLinks are a great way to connect your users and search engines to other parts of your site, or\nrelevant pages on other sites. In fact, the vast majority of the new pages Google finds every day\nare through links, making links a crucial resource you need to consider to help your pages be\ndiscovered by Google and potentially shown in search results. Additionally, links can also add\nvalue by connecting users (and Google) to another resource that corroborates what you're writing\nabout.\nWrite good link text\nLink text\n(also known as", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9799, "chunk_char_end": 11792}
{"id": "de11fa8c-9342-46a8-9aae-c85bfa3a3cea", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "anchor text\n)  is the text part of a link that you can see.\nThis text tells users and Google something about the page you're linking to. With\nappropriate anchor text\n,\nusers and search engines can easily understand what your linked pages contain before they visit.\nLink when you need to\nLinks can provide more context on a topic, both for users and search engines, which may help\ndemonstrate your knowledge on a topic. However when you're linking to pages outside of your\ncontrol, for example content on other sites, make sure you trust the resource you're linking to.\nIf you can't trust the content and you still want to link to them, add a\nnofollow\nor similar annotation\nto the link to avoid search engines associating your site with the site you're linking to. This\nhelps avoid potential negative consequences in your rankings in Google Search.\nIf you're accepting user-generated content on your site, such as forum posts or comments, make\nsure every link that's posted by users has a\nnofollow\nor similar annotation\nautomatically added by your CMS. Since you're not creating the content in this case, you likely\ndon't want your site to be blindly associated with the sites users are linking to. This can also\nhelp discourage spammers from abusing your website.\nInfluence how your site looks in Google Search\nA typical Google Search results page consists of a\nfew different visual elements\nthat\nyou can influence to help users decide whether they should visit your site through those search\nresults. In this section, we're focusing on the\ntitle link\nand the\nsnippet\nbecause\nthese are the more visually significant elements.\nInfluence your title links\nThe\ntitle link\nis the headline part of the search result and it can help people decide\nwhich search result to click. There are a few sources that Google uses to generate this title\nlink, including the words inside the\n<title>\nelement (also called the title", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11793, "chunk_char_end": 13702}
{"id": "992efaf4-b639-403c-9eed-23377ea27666", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "text) and other headings on the page. This title text can also be used for the title that's shown\nin browsers and bookmarks.\nAn illustration of a text result in Google Search, with a highlighted box around the title link part\nHow to make your own chili oil\nYou can influence the title links in Search by writing good titles: a good title is unique to the\npage, clear and concise, and accurately describes the contents of the page. For example, your\ntitle could include the name of your website or business, other bits of important information like\nthe physical location of the business, and maybe some information about what the particular page\nhas to offer for users. Our\ndocumentation about title links\nhas more tips about how to create good titles and how to influence your site's search results'\ntitle links.\nControl your snippets\nBelow the title link, a search result typically has a description of the target page to help users\ndecide whether they should click the search result. This is called a\nsnippet\n.\nAn illustration of a text result in Google Search, with a highlighted box around the snippet part\nLearn how to cook eggs with this complete guide in less\nthan 5 minutes. We cover all the methods, including sunny side up, boiled, and poached.\nThe snippet is sourced from the actual content of the page the search result is linking to, thus\nyou have complete control over the words that can be used to generate the snippet. Occasionally\nthe snippet may be sourced from the contents of the meta description tag, which is typically a\nsuccinct, one- or two-sentence summary of the page. A good meta description is short, unique to one\nparticular page, and includes the most relevant points of the page. Check out our tips for\nwriting good meta descriptions\nfor\nmore inspiration.\nAdd images to your site, and optimize them\nMany people search visually, and images can be how people find your website for the first time.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13703, "chunk_char_end": 15628}
{"id": "95ccde1a-402a-4585-a535-e96a9d9ecf15", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "For example, if you have a recipe blog, people might find your content by searching for \"fruit\ntart recipes\" and browsing photos of various types of fruit tarts.\nAs you add images to your site, make sure that people and search engines can find and understand them.\nAdd high-quality images near relevant text\nWhen you use high quality images, you give users enough context and detail to decide which image\nbest matches what they were looking for. For example, if people are looking for \"daisies\" and\ncome across a rogue edelweiss in search results, a higher quality image would help them distinguish\nthe type of flower.\nUse images that are sharp and clear, and place them near text that's relevant to the image. The\ntext that's near images can help Google better understand what the image is about and what it\nmeans in context to your page.\nFor example, if the page is reviewing yarn shops in London, then it would make sense to embed one\nof your photos of the yarn shop in the section that details the location, description, and review\ninformation for that yarn shop. This helps Google and users associate the image with text that\nprovides more context to what the page is about.\nAdd descriptive alt text to the image\nAlt text is a short, but descriptive piece of text that explains the relationship between the\nimage and your content. It helps search engines understand what your image is about and the\ncontext of how your image relates to your page, so writing\ngood alt text\nis quite important. You can add this to your HTML with the\nalt\nattribute of the\nimg\nelement, or your CMS may have an easy way to specify a description for an image\nwhen you're uploading it to your site. Learn more about\nhow to write good alt text\n,\nand how to add it to your images.\nOptimize your videos\nIf your website includes pages that are primarily about individual videos, people may also be able\nto discover your site through video results in Google Search. Many of the best practices for", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15629, "chunk_char_end": 17601}
{"id": "28c17787-3a11-49d8-8a48-ac784dbc4a03", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "images and text also apply to videos:\nCreate high-quality video content, and embed the video on a standalone page, near text that's\nrelevant to that video.\nWrite descriptive text in the titles and description fields of a video (the title of a video\nis still a title, and so you can apply the best practices for writing titles here too).\nIf your site is particularly video-focused, then continue reading about more things you can do to\noptimize your videos for search engines\n.\nPromote your website\nEffectively promoting your new content will lead to faster discovery by those who are interested\nin the same subject, and also by search engines. You can do this in many ways:\nSocial media promotion\nCommunity engagement\nAdvertisement, both offline and online\nWord of mouth, and many other methods\nOne of the\nmost effective and lasting ways\nis word of mouth: that is, people familiar with your site tell their friends about it, who in turn\nvisit your site. This can take time, and usually you need to invest some time and effort in other\npractices first, such as community engagement. Our friends over at Google for Creators have excellent\nresources about\nbuilding and engaging your audience\n.\nPutting effort into the offline promotion of your company or site can also be rewarding. For\nexample, if you have a business site, make sure its URL is listed on your business cards,\nletterhead, posters, and other materials. With their permission, you could also send out recurring\nnewsletters to your audience letting them know about new content on your website.\nAs with everything in life, you can overdo promoting your site and actually harm it: people may get\nfatigued of your promotions, and search engines may perceive some of the practices as\nmanipulation of search results\n.\nThings we believe you shouldn't focus on\nAs SEO has evolved, so have the ideas and practices (and at times, misconceptions) related to it.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17602, "chunk_char_end": 19514}
{"id": "78386287-1090-4d0e-9f00-279204280d42", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "What was considered best practice or top priority in the past may no longer be relevant or\neffective due to the way search engines (and the internet) have developed over time.\nTo help you focus on the things that are actually important when it comes to SEO, we collected\nsome of the most common and  prominent topics we've seen circulating the internet. In general, our\nmessage on these topics is that you should do what's best for your business area; we will\nelaborate on a few specific points here:\nMeta keywords\nGoogle Search\ndoesn't use the keywords meta tag\n.\nKeyword stuffing\nExcessively repeating the same words over and over (even in\nvariations) is tiring for users, and\nkeyword stuffing is against Google's spam policies\n.\nKeywords in the domain name or URL path\nWhen picking the name of your site, do what's\nbest for your business. Users will use this name to find you, so we recommend following general\nmarketing best practices. From a ranking perspective, the keywords in the name of the domain (or\nURL path) alone have hardly any effect beyond appearing in\nbreadcrumbs\n.\nAnd while still on the topic of domain names: the TLD (the domain name ending like \".com\" or\n\".guru\") only matters if you're targeting a specific country's users, and even then it's\nusually a low impact signal. For example, if you're trying to sell Dutch cheese to people\nsearching from Switzerland, it makes some sense (both from business and\nSEO point of view\n)\nto use a .ch domain name. Otherwise Google Search doesn't care which TLD you're using (whether\nit's a .com or .org or .asia).\nMinimum or maximum content length\nThe length of the content alone doesn't matter for\nranking purposes (there's no magical word count target, minimum or maximum, though you\nprobably want to have at least one word). If you are varying the words (writing naturally to\nnot be repetitive), you have more chances to show up in Search simply because you are using\nmore keywords.\nSubdomains versus subdirectories", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19515, "chunk_char_end": 21493}
{"id": "f57a57dc-9971-43b0-8a9a-775859eea267", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "From a business point of view, do whatever makes sense\nfor your business. For example, it might be easier to manage the site if it's segmented by\nsubdirectories, but other times it might make sense to partition topics into subdomains,\ndepending on your site's topic or industry.\nPageRank\nWhile\nPageRank\nuses links and is one of the fundamental algorithms at Google, there's much more to Google Search\nthan just links. We have many ranking signals, and PageRank is just one of those.\nDuplicate content \"penalty\"\nIf you have some content that's accessible under multiple\nURLs, it's fine; don't fret about it. It's inefficient, but it's not something that will cause\na manual action.\nCopying others' content, however, is a different story.\nNumber and order of headings\nHaving your headings in semantic order is\nfantastic for screen readers, but from Google Search perspective, it doesn't matter if\nyou're using them out of order. The web in general is not valid HTML, so Google Search can\nrarely depend on semantic meanings hidden in the HTML specification.\nThere's also no magical, ideal amount of headings a given page should have. However, if you\nthink it's too much, then it probably is.\nThinking E-E-A-T is a ranking factor\nNo, it's not.\nNext steps\nGet started with Search Console\n: Setting up a Search Console account helps you monitor\nand optimize how your website performs on Google Search. Learn how to\nset up your account and what reports to check out first\n.\nMaintain your website's SEO over time\n: Learn more about\nmanaging your site's presence in the long term\n,\nincluding more in-depth SEO tasks and scenarios, such as preparing for a site move, or managing\na multi-lingual site.\nEnhance how your site looks in Google Search results\n: Valid\nstructured data\non\nyour pages also makes your pages eligible for many special features in Google Search results,\nincluding review stars, carousels, and more. Explore the\ngallery of search result types\nthat your page can be eligible for.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 11, "chunk_char_start": 21494, "chunk_char_end": 23482}
{"id": "b5ed7800-d2c7-4cfc-80dd-4a3e6e6337e3", "url": "https://developers.google.com/search/docs/fundamentals/seo-starter-guide", "source_domain": "developers.google.com", "title": "SEO Starter Guide: The Basics | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Stay informed and ask questions\nAs you embark on your SEO journey, here are some resources that can help you stay on top of\nchanges and new resources we publish:\nGoogle Search Central blog\n: Get the latest information from our\nGoogle Search Central blog. You can find information about updates to Google Search, new\nSearch Console features, and much more.\nGoogle Search Central on\nLinkedIn\nand\nX (Twitter)\n:\nFollow us for updates on Google Search and resources to help you make a great site.\nGoogle Search Central Help Forum\n:\nPost questions about your site's SEO issues and find tips to create high quality sites from the\nproduct forum for website owners. There are many experienced contributors in the forum, including\nProduct Experts\nand\noccasionally Googlers.\nGoogle Search Central YouTube Channel\n:\nWatch hundreds of helpful videos created for website owners.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-02-04 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 12, "chunk_char_start": 23483, "chunk_char_end": 24687}
{"id": "8e67cb9f-5d5d-4735-ad3d-9360af6133c9", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nCreating helpful, reliable, people-first content\nGoogle's\nautomated ranking systems\nare designed to prioritize helpful, reliable information that's created to benefit people, and\nnot content that's created to manipulate search engine rankings. This page is designed to help\ncreators evaluate if they're producing such content.\nSelf-assess your content\nEvaluating your own content against these questions can help you gauge if the content you're\nmaking is helpful and reliable. Beyond asking yourself these questions, consider having others\nyou trust but who are unaffiliated with your site provide an honest assessment.\nAlso consider an audit of the drops you may have experienced. What pages were most impacted\nand for what types of searches? Look closely at these to understand how they're assessed\nagainst some of the questions outlined here.\nContent and quality questions\nDoes the content provide original information, reporting, research, or analysis?\nDoes the content provide a substantial, complete, or comprehensive description of the topic?\nDoes the content provide insightful analysis or interesting information that is beyond the\nobvious?\nIf the content draws on other sources, does it avoid simply copying or rewriting those\nsources, and instead provide substantial additional value and originality?\nDoes the main heading or page title provide a descriptive, helpful summary of the content?\nDoes the main heading or page title avoid exaggerating or being shocking in nature?", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1998}
{"id": "67184143-d7e9-4e47-ac94-0d47b3a4ba46", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Is this the sort of page you'd want to bookmark, share with a friend, or recommend?\nWould you expect to see this content in or referenced by a printed magazine, encyclopedia, or book?\nDoes the content provide substantial value when compared to other pages in search results?\nDoes the content have any spelling or stylistic issues?\nIs the content produced well, or does it appear sloppy or hastily produced?\nIs the content mass-produced by or outsourced to a large number of creators, or spread\nacross a large network of sites, so that individual pages or sites don't get as much attention or care?\nExpertise questions\nDoes the content present information in a way that makes you want to trust it, such as clear\nsourcing, evidence of the expertise involved, background about the author or the site that\npublishes it, such as through links to an author page or a site's About page?\nIf someone researched the site producing the content, would they come away with an impression\nthat it is well-trusted or widely-recognized as an authority on its topic?\nIs this content written or reviewed by an expert or enthusiast who demonstrably knows the topic well?\nDoes the content have any easily-verified factual errors?\nProvide a great page experience\nGoogle's core ranking systems look to reward content that provides a good page experience.\nSite owners seeking to be successful with our systems should not focus on only one or two\naspects of page experience. Instead, check if you're providing an overall great page\nexperience across many aspects. For more advice, see our page,\nUnderstanding page experience in Google Search results\n.\nFocus on people-first content\nPeople-first content means content that's created primarily for people, and not to manipulate\nsearch engine rankings. How can you evaluate if you're creating people-first content? Answering\nyes to the questions below means you're probably on the right track with a people-first approach:", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1999, "chunk_char_end": 3943}
{"id": "e25e3eb6-b5c9-471c-89ee-46f5927c54c9", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Do you have an existing or intended audience for your business or site that would find the\ncontent useful if they came directly to you?\nDoes your content clearly demonstrate first-hand expertise and a depth of knowledge (for\nexample, expertise that comes from having actually used a product or service, or visiting\na place)?\nDoes your site have a primary purpose or focus?\nAfter reading your content, will someone leave feeling they've learned enough about a\ntopic to help achieve their goal?\nWill someone reading your content leave feeling like they've had a satisfying experience?\nAvoid creating search engine-first content\nWe recommend that you focus on creating people-first content to be successful with Google\nSearch, rather than search engine-first content made primarily to gain search engine rankings.\nAnswering yes to some or all of the questions below is a warning sign that you should\nreevaluate how you're creating content:\nIs the content primarily made to attract visits from search engines?\nAre you producing lots of content on many different topics in hopes that some of it might\nperform well in search results?\nAre you using extensive automation to produce content on many topics?\nAre you mainly summarizing what others have to say without adding much value?\nAre you writing about things simply because they seem trending and not because you'd write\nabout them otherwise for your existing audience?\nDoes your content leave readers feeling like they need to search again to get better\ninformation from other sources?\nAre you writing to a particular word count because you've heard or read that Google has a\npreferred word count? (No, we don't.)\nDid you decide to enter some niche topic area without any real expertise, but instead\nmainly because you thought you'd get search traffic?\nDoes your content promise to answer a question that actually has no answer, such as\nsuggesting there's a release date for a product, movie, or TV show when one isn't confirmed?", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3944, "chunk_char_end": 5920}
{"id": "25c09263-addc-4a1e-a08a-ef16ea37c060", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Are you changing the date of pages to make them seem fresh when the content has not\nsubstantially changed?\nAre you adding a lot of new content or removing a lot of older content primarily because\nyou believe it will help your search rankings overall by somehow making your site seem\n\"fresh?\" (No, it won't)\nWhat about SEO? Isn't that search engine-first?\nThere are some things you could do that are specifically meant to help search engines better\ndiscover and understand your content. Collectively, this is called \"search engine optimization\"\nor SEO, for short.\nGoogle's own SEO guide\ncovers best practices to consider. SEO can be a helpful activity when it is applied to\npeople-first content, rather than search engine-first content.\nGet to know E-E-A-T and the quality rater guidelines\nGoogle's automated systems are designed to use\nmany different factors\nto rank great content. After identifying relevant content, our systems aim to prioritize those\nthat seem most helpful. To do this, they identify a mix of factors that can help determine\nwhich content demonstrates aspects of experience, expertise, authoritativeness, and trustworthiness,\nor what we call E-E-A-T.\nOf these aspects, trust is most important. The others contribute to trust, but content doesn't\nnecessarily have to demonstrate all of them. For example, some content might be helpful based\non the experience it demonstrates, while other content might be helpful because of the\nexpertise it shares.\nWhile E-E-A-T itself isn't a specific ranking factor, using a mix of factors that can identify\ncontent with good E-E-A-T is useful. For example, our systems give even more weight to content\nthat aligns with strong E-E-A-T for topics that could significantly impact the health,\nfinancial stability, or safety of people, or the welfare or well-being of society. We call these\n\"Your Money or Your Life\" topics, or YMYL for short.\nSearch quality raters", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5921, "chunk_char_end": 7837}
{"id": "ba6e9616-97e2-4f6a-95af-72c64035efec", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "are people who give us insights on if our algorithms seem to be providing good results, a way\nto help confirm our changes are working well. In particular, raters are trained to understand\nif content has strong E-E-A-T. The criteria they use to do this is outlined in our\nsearch quality rater guidelines\n.\nReading the guidelines may help you self-assess how your content is doing from an E-E-A-T\nperspective, improvements to consider, and help align it conceptually with the different\nsignals that our automated systems use to rank content.\nAsk \"Who, How, and Why\" about your content\nConsider evaluating your content in terms of \"Who, How, and Why\" as a way to stay on course\nwith what our systems seek to reward.\nWho (created the content)\nSomething that helps people intuitively understand the E-E-A-T of content is when it's clear who\ncreated it. That's the\n\"Who\"\nto consider. When creating content, here are some who-related\nquestions to ask yourself:\nIs it self-evident to your visitors who authored your content?\nDo pages carry a byline, where one might be expected?\nDo bylines lead to further information about the author or authors involved, giving\nbackground about them and the areas they write about?\nIf you're clearly indicating who created the content, you're likely aligned with the concepts of\nE-E-A-T and on a path to success. We strongly encourage adding accurate authorship information,\nsuch as bylines to content where readers might expect it.\nHow (the content was created)\nIt's helpful to readers to know how a piece of content was produced: this is the\n\"How\"\nto consider including in your content.\nFor example, with product reviews, it can build trust with readers when they understand the\nnumber of products that were tested, what the test results were, and how the tests were\nconducted, all accompanied by evidence of the work involved, such as photographs. It's\nadvice we share more about in our\nWrite high quality product reviews\nhelp page.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7838, "chunk_char_end": 9800}
{"id": "fbc2efdf-04b7-48d4-93e3-da7aa6374e3d", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Many types of content may have a \"How\" component to them. That can include automated,\nAI-generated, and AI-assisted content. Sharing details about the processes involved can help\nreaders and visitors better understand any unique and useful role automation may have served.\nIf automation is used to substantially generate content, here are some questions to ask yourself:\nIs the use of automation, including AI-generation, self-evident to visitors through disclosures or in other ways?\nAre you providing background about how automation or AI-generation was used to create content?\nAre you explaining why automation or AI was seen as useful to produce content?\nOverall, AI or automation disclosures are useful for content where someone might think\n\"How was this created?\" Consider adding these when it would be reasonably expected. For more,\nsee our blog post and FAQ:\nHow Google Search views AI-generated content\n.\nWhy (was the content created)\n\"Why\"\nis perhaps the most important question to answer about your content. Why is it\nbeing created in the first place?\nThe \"why\" should be that you're creating content primarily to help people, content that is\nuseful to visitors if they come to your site directly. If you're doing this, you're aligning\nwith E-E-A-T generally and what our\ncore ranking systems\nseek to reward.\nIf the \"why\" is that you're primarily making content to attract search engine visits, that's\nnot aligned with what our systems seek to reward. If you use automation, including AI-generation,\nto produce content for the primary purpose of manipulating search rankings, that's a\nviolation of our spam policies\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-09-22 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9801, "chunk_char_end": 11769}
{"id": "e9bce08f-f75d-4a3d-b09b-f3e4abae5054", "url": "https://developers.google.com/search/docs/fundamentals/get-started-developers", "source_domain": "developers.google.com", "title": "SEO Guide for Web Developers | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "SEO Guide for Web Developers | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nGet started with Search: a developer's guide\nMaking your content search-friendly matters because it's how you get more relevant users viewing\nyour content. This is called search engine optimization (SEO), which can result in more\ninterested users coming to your site. If Google Search has trouble understanding your page,\nyou're possibly missing out on an important source of traffic.\nThis guide covers what developers can do to make sure that their sites work well with Google\nSearch. In addition to the items in this guide, make sure that your site is\nsecure\n,\nfast\n,\naccessible to all\n, and\nworks on all devices\n.\nFind out how Google sees your site\nTo get started, test your site in the\nURL inspection tool\nor\nRich Results Test\nto\nsee how Google sees your site.\nGooglebot is Google's web\ncrawling bot\nthat discovers new and updated pages for the Google index. For\nmore information about the process, go to\nHow Google Search Works\n.\nYou may be surprised to find that Google doesn't always see everything that you see in the\nbrowser. In the following example, Google doesn't know there are images on this page\nbecause the page uses a JavaScript feature that isn't supported by Google.\nHere's how a user views the page. Users can view the images and text in the browser.\nHere's how Google views the page. Google doesn't know there are images on this page\nbecause the page uses a JavaScript feature that isn't supported by Google.\nCheck your links", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1938}
{"id": "e3caa776-1d2a-4dd5-93f8-e6f6da3b4b49", "url": "https://developers.google.com/search/docs/fundamentals/get-started-developers", "source_domain": "developers.google.com", "title": "SEO Guide for Web Developers | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Googlebot navigates from URL to URL by fetching and parsing links, sitemaps, and redirects. Googlebot\ntreats every URL as if it's the first and only URL it has seen from your site. To make sure\nthat Googlebot can find all the URLs on your site:\nUse\n<a>\nelements that Google can crawl\n.\nEnsure that all pages on the site can be reached by a link from another findable page. Make\nsure the referring link includes either text or, for images, an alt\nattribute, that is relevant to the target page.\nBuild and submit a sitemap\nto help Googlebot more intelligently crawl your site. A sitemap is a file where\nyou provide information about the pages, videos, and other files on your site, and the\nrelationships between them.\nFor JavaScript apps that have only one HTML page, make sure that each screen or piece of\nindividual content has a URL.\nCheck how you're using JavaScript\nWhile Google does run JavaScript, there are some differences and limitations that you need\nto account for when designing your pages and applications to accommodate how crawlers access\nand render your content.\nLearn more about the\nbasics of JavaScript SEO\nor how to\nfix\nSearch-related JavaScript problems\n.\nTo learn more about how Google handles JavaScript when crawling, rendering, and indexing, see the following video.\nKeep Google updated when content changes\nTo make sure that Google finds your new or updated pages quickly:\nSubmit sitemaps\n.\nAsk Google to recrawl your\nURLs\n.\nIf you're still having trouble getting your page indexed, check your server logs for errors.\nDon't forget about the words on the page\nGooglebot can only find content that is textually visible. For example, text in videos is\ninvisible to Googlebot. To make sure that Google Search understands what your page is about:\nMake sure that your visual content is expressed in text form.\nFor example, a product category page that contains a list of images of shirts with no textual", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1939, "chunk_char_end": 3860}
{"id": "9f5f5218-f6d9-497b-9e4a-f390993ae765", "url": "https://developers.google.com/search/docs/fundamentals/get-started-developers", "source_domain": "developers.google.com", "title": "SEO Guide for Web Developers | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "context about each image is suboptimal. The product category page should include some\ntextual explanation for each image.\nMake sure that every page has a\ndescriptive title\nand\nmeta description\n.\nUnique titles and meta descriptions help Google show how your\npages are relevant to users, which in turn can increase your search traffic.\nUse semantic HTML\n. While Google indexes HTML, PDF content, images,\nand videos, it doesn't index content that requires plugins (for example, Java or Silverlight)\nor content that is rendered in a canvas. Instead of using a plugin, use semantic HTML markup\nfor your content whenever possible.\nMake sure your text content is accessible in the\nDOM.\nFor example, content that is added via the\nCSS\ncontent\nproperty\nis not part of the DOM and Google Search ignores it at the moment. It's fine to use the\ncontent\nproperty for decorative content; Google Search may not index this content.\nTell Google about other versions of your content\nGoogle doesn't automatically know that there are multiple versions of your site or content. For\nexample, a mobile and desktop version, or international versions of your site. To make sure\nthat Google serves the right version to users, you can:\nConsolidate duplicate\nURLs\n.\nTell\nGoogle about localized versions of your site\n.\nMake your AMP pages\ndiscoverable\n.\nControl what content Google sees\nThere are several ways to block Googlebot:\nTo block Google from finding your page, restrict access to your content to logged in\nusers (for example, use a login page or\npassword-protect your\npage\n).\nTo block Googlebot from crawling your page,\ncreate a robots.txt\n.\nTo block Google from indexing your page but still allow crawling,\nadd a\nnoindex\ntag\n.\nIf your content isn't showing up in Google Search and you want it to show up, follow these\nsteps:\nCheck if Googlebot can access the page with the\nURL inspection tool\n.\nTest your robot.txt\nfile to see if you're unintentionally blocking Googlebot from crawling your\nsite.\nCheck your HTML for", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3861, "chunk_char_end": 5856}
{"id": "7094c5de-cae9-40a2-99f7-ef5bb25d355e", "url": "https://developers.google.com/search/docs/fundamentals/get-started-developers", "source_domain": "developers.google.com", "title": "SEO Guide for Web Developers | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "noindex\nrules in\nmeta\ntags.\nEnable rich results for your site\nA rich result can include styling, images, or other interactive features that can help your\nsite stand out more in Search results. You can help Google understand your page better and\nshow rich results for it in Search by providing explicit clues about the meaning of a page\nwith structured data on the page\n. If\nyou're not sure where to start,\nexplore our gallery of available features\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-02-04 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5857, "chunk_char_end": 6646}
{"id": "ccc8d0f4-cbd1-4342-a31b-6d96776de6bf", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nBuild and submit a sitemap\nThis page describes how to build a sitemap and make it available to Google. If you're new to\nsitemaps,\nread our introduction first\n.\nGoogle supports the sitemap formats defined by the\nsitemaps protocol\n.\nEach format has its own benefits and shortcomings; choose the one that is the most appropriate for\nyour site and setup (Google doesn't have a preference). The following table compares the different\nsitemap formats:\nSitemaps comparison\nXML sitemap\nXML sitemaps are the most versatile of the sitemaps formats. It's extensible and can be used\nto supply additional data about\nimages\n,\nvideo\n, and\nnews\ncontent, as well\nas the\nlocalized versions\nof your pages.\nPros:\nExtensible and versatile.\nIt can provide the most information about your URLs.\nMost content management systems (CMS) automatically generate sitemaps or CMS users\ncan find plenty of sitemap plugins.\nCons:\nCan be cumbersome to work with.\nCan be complex to maintain the mapping on larger sites, or sites where the URLs\nchange often.\nRSS, mRSS, and Atom 1.0\nRSS, mRSS, and Atom 1.0 sitemaps are similar in structure to XML sitemaps, however they\nare often the easiest to provide because CMSes automatically create them.\nPros:\nMost CMSes automatically generate RSS and Atom feeds.\nCan be used to provide Google information about your videos.\nCons:\nBesides\nHTML and other indexable textual content\n,\nit can only provide information about videos, not images or news.\nCan be cumbersome to work with.\nText sitemap", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1987}
{"id": "a883a1df-646f-4981-ad18-c63fc12483da", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "The simplest of sitemap formats, it can only list URLs to HTML and other indexable\npages.\nPros:\nSimple to do and maintain, especially on large sites.\nCons:\nLimited to HTML and other indexable textual content.\nSitemap best practices\nThe best practices for sitemaps are defined by the\nsitemaps protocol\n.\nThe most overlooked best practices are related to the size limits, sitemap location, and the\nURLs included in the sitemaps.\nSitemap size limits:\nAll formats limit a single sitemap to 50MB (uncompressed) or 50,000 URLs. If you have a larger\nfile or more URLs, you must break your sitemap into multiple sitemaps. You can optionally\ncreate a\nsitemap index\nfile and submit that single index file to Google. You can submit multiple sitemaps and sitemap\nindex files to Google. This may be useful if you want to track the search performance of each\nindividual sitemap in Search Console.\nSitemap file encoding and location:\nThe sitemap file must be UTF-8 encoded. You can host your sitemaps anywhere on your site, but\nunless you submit your sitemap through\nSearch Console\n, a sitemap\naffects only descendants of the parent directory. Therefore, a sitemap posted at the site root\ncan affect all files on the site, which is where we recommend posting your sitemaps.\nReferenced URLs' properties:\nUse fully-qualified, absolute URLs in your sitemaps. Google will attempt to crawl your URLs\nexactly as listed. For example, if your site is at\nhttps://www.example.com/\n, don't\nspecify a URL such as\n/mypage.html\n(a relative URL), use the complete, absolute URL:\nhttps://www.example.com/mypage.html\n.\nInclude the URLs in your sitemap that you want to see in Google's search results. Google\ngenerally shows the\ncanonical URLs\nin its\nsearch results, which you can influence with sitemaps. If you have different URLs for mobile\nand desktop versions of a page, we recommend pointing to only one version in a sitemap.\nHowever, if you want to point to both URLs,\nannotate", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1988, "chunk_char_end": 3939}
{"id": "6cbb7e12-dd50-4d94-b6be-edc4e4867e09", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "your URLs to indicate the desktop and mobile versions.\nFor a complete list of best practices, check out the\nsitemaps protocol\n.\nXML sitemap\nThe XML sitemap format is the most versatile of the supported formats. Using the Google\nsupported sitemap extensions, you can also provide additional information about your\nimages\n,\nvideo\n, and\nnews\ncontent, as well as\nthe\nlocalized versions\nof your pages.\nHere is a very basic XML sitemap that includes the location of a single URL:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<url>\n<loc>https://www.example.com/foo.html</loc>\n<lastmod>2022-06-04</lastmod>\n</url>\n</urlset>\nYou can find more complex examples and full documentation at\nsitemaps.org\n.\nAdditional notes about XML sitemaps\nAs with all XML files, all tag values must be\nentity escaped\n.\nGoogle ignores\n<priority>\nand\n<changefreq>\nvalues.\nGoogle uses the\n<lastmod>\nvalue if it's consistently and verifiably (for\nexample by comparing to the last modification of the page) accurate.\nRSS, mRSS, and Atom 1.0\nIf your CMS generates an RSS or Atom feed, you can submit the feed's URL as a sitemap.\nMost CMSes create a feed for you, however keep in mind that this feed only provides information on\nrecent URLs.\nAdditional notes about RSS, mRSS, and Atom 1.0\nGoogle accepts RSS 2.0 and Atom 1.0 feeds.\nYou can use an\nmRSS (media RSS) feed\nto provide Google details about video content on your site.\nAs with all XML files, all tag values must be\nentity escaped\n.\nText sitemap\nIf you only want to provide web page URLs, you can create a common text file that contains one URL\nper line and submit that to Google. For example, if you have two pages on your site, you could add\nthem to your text sitemap located at\nhttps://www.example.com/sitemap.txt\nas follows:\nhttps://www.example.com/file1.html\nhttps://www.example.com/file2.html\nAdditional notes for text file sitemaps\nDon't put anything other than URLs in the sitemap file.", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3940, "chunk_char_end": 5913}
{"id": "9deb6d52-c872-401c-92bf-d478a55035fe", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "You can name the text file anything you want, provided it has a\n.txt\nextension\n(for instance, sitemap.txt).\nHow to create a sitemap\nWhen creating a sitemap, you're telling search engines about which URLs you prefer to show in\nsearch results. These are the\ncanonical URLs\n. If you\nhave the same content accessible under different URLs, choose the URL you prefer\nand include that in the sitemap instead of all URLs that lead to the same content.\nOnce you've decided which URLs to include in the sitemap, pick one of the following ways to\ncreate a sitemap, depending on your site architecture and size:\nLet your CMS generate a sitemap for you\n.\nFor sitemaps with less than a few dozen URLs,\nyou can\nmanually create a sitemap\n.\nFor sitemaps with more than a few dozen URLs,\nautomatically generate a sitemap\n.\nLet your CMS generate a sitemap for you\nIf you're using a CMS such as WordPress, Wix, or Blogger, it's likely that your CMS has\nalready made a sitemap available to search engines. Try searching for information about how\nyour CMS generates sitemaps, or how to create a sitemap if your CMS doesn't generate a sitemap\nautomatically. For example, in case of Wix, search for \"wix sitemap\", or in case of Blogger,\nsearch for \"Blogger RSS\".\nManually create a sitemap\nFor sitemaps with less than a few dozen URLs, you may be able to manually create a sitemap.\nFor this, open a text editor such as\nWindows Notepad\nor\nNano (Linux, MacOS)\n,\nand follow a syntax described in the\nSitemap Formats\nsection. You\ncan name the file anything you like as long as\nthe characters are allowed in a URL\n.\nYou can manually create larger sitemaps, but it's a tedious process and hard to maintain long\nterm.\nAutomatically generate a sitemap with tools\nFor sitemaps with more than a few dozen URLs, you will need to generate the sitemap. There are\nvarious tools that can\ngenerate a sitemap\n.\nHowever, the best way is to have your website software generate it for you. For example, you", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5914, "chunk_char_end": 7875}
{"id": "ec0267da-4ebd-44a4-bae0-bbdbc5d9a54e", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "can extract your site's URLs from your website's database and then export the URLs to either\nthe screen or actual file on your web server. Talk to your developers or server manager about\nthis solution. If you need inspiration for the code, check out our old, unmaintained\ncollection of\nthird-party sitemap generators\n.\nYou don't have to worry about the order of the URLs in your sitemap, it doesn't matter to\nGoogle. Note the\nsize requirements for sitemaps\n; if the sitemap becomes too\nlarge, you must split it into smaller sitemaps. Learn more about\nmanaging large sitemaps\n.\nSubmit your sitemap to Google\nKeep in mind that submitting a sitemap is merely a hint: it doesn't guarantee that Google will\ndownload the sitemap or use the sitemap for crawling URLs on the site. There are a few\ndifferent ways to make your sitemap available to Google.\nSubmit a sitemap in Search Console\nusing the\nSitemaps report\n.\nThis will allow you to see when Googlebot accessed the sitemap and also potential processing\nerrors.\nUse the Search Console API\nto programmatically\nsubmit a sitemap\n.\nInsert the following line anywhere in your robots.txt file\n, specifying the path to\nyour sitemap. We will find it the next time we crawl your robots.txt file:\nSitemap: https://example.com/my_sitemap.xml\nIf you use Atom or RSS, you can use\nWebSub\nto broadcast your\nchanges to search engines, including Google.\nHow to cross-submit sitemaps for multiple sites\nIf you have multiple websites, you can simplify the submission process by creating one or\nmore sitemaps that include URLs for all your verified sites, and saving the sitemaps to a\nsingle location. You can choose to use:\nA single sitemap that includes URLs for multiple websites, including sites from different\ndomains. For example, the sitemap located at\nhttps://host1.example.com/sitemap.xml\ncan include the following URLs.\nhttps://host1.example.com\nhttps://host2.example.com\nhttps://host3.example.com\nhttps://host1.example1.com\nhttps://host1.example.ch", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7876, "chunk_char_end": 9863}
{"id": "65b72e9c-571d-4070-8e56-28d59ab5a949", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Individual sitemaps (one for each site) that all reside in a single location. For example,\nhttps://host1.example.com\nmay host all of these sitemaps:\nhttps://host1.example.com/host1-example-sitemap.xml\nhttps://host1.example.com/host2-example-sitemap.xml\nhttps://host1.example.com/host3-example-sitemap.xml\nhttps://host1.example.com/host1-example1-sitemap.xml\nhttps://host1.example.com/host1-example-ch-sitemap.xml\nTo submit cross-site sitemaps that are hosted in a single location, you can either use Search\nConsole or robots.txt.\nSitemap cross-submission with Search Console\nMake sure that you have\nverified ownership\nof all the sites that you will add in the sitemap.\nCreate a sitemap\n(or more if you prefer) that includes URLs\nfrom all the sites that you want to cover. You can include the sitemaps in a\nsitemap index\nfile if\nyou prefer and work with that sitemap index from here on.\nUsing Google Search Console,\nsubmit your sitemaps or sitemap index file\n.\nSitemap cross-submission with robots.txt\nCreate one or more sitemaps\nfor each individual site. For each\nindividual sitemap file, make sure you include only URLs from that particular site.\nUpload all sitemaps to a single site you have control over, for example\nhttps://sitemaps.example.com\n.\nFor each individual site, make sure that the robots.txt file references the sitemap for that\nindividual site. For example, if you created a sitemap for\nhttps://example.com/\nand you're hosting the sitemap at\nhttps://sitemaps.example.com/sitemap-example-com.xml\n, reference the sitemap in\nthe robots.txt file at\nhttps://example.com/robots.txt\n.\n# robots.txt file of https://example.com/\nsitemap: https://sitemaps.example.com/sitemap-example-com.xml\nTroubleshooting sitemaps\nIf you're having trouble with your sitemap, you can investigate the errors with Google Search Console.\nSee Search Console's\nsitemaps troubleshooting guide\nfor help.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9864, "chunk_char_end": 11839}
{"id": "58bcc50b-9791-4c98-91e9-a469da144fcf", "url": "https://developers.google.com/search/docs/crawling-indexing/sitemaps/build-sitemap", "source_domain": "developers.google.com", "title": "Build and Submit a Sitemap | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Creative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-10-28 UTC.", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11840, "chunk_char_end": 12091}
{"id": "66c08b9e-64c1-4814-aabf-a8f838bf8e95", "url": "https://developers.google.com/search/docs/crawling-indexing/robots/intro", "source_domain": "developers.google.com", "title": "Robots.txt Introduction and Guide | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Robots.txt Introduction and Guide | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nIntroduction to robots.txt\nA robots.txt file tells search engine crawlers which URLs the crawler can access on your site.\nThis is used mainly to avoid overloading your site with requests;\nit is not a\nmechanism for keeping a web page out of Google\n. To keep a web page out of Google,\nblock indexing with\nnoindex\nor password-protect the page.\nWhat is a robots.txt file used for?\nA robots.txt file is used primarily to manage crawler traffic to your site, and\nusually\nto keep a file off Google, depending on the file type:\nrobots.txt effect on different file types\nWeb page\nYou can use a robots.txt file for web pages (HTML, PDF, or other\nnon-media formats that Google can read\n),\nto manage crawling traffic if you think your server will be overwhelmed by requests\nfrom Google's crawler, or to avoid crawling unimportant or similar pages on your site.\nIf your web page is blocked with a robots.txt file\n, its URL can still\nappear in search results, but the search result\nwon't have a description\n.\nImage files, video files, PDFs, and other non-HTML files embedded in the blocked page will\nbe excluded from crawling, too, unless they're referenced by other pages that are allowed\nfor crawling. If you see this search result for your page and want to fix it, remove the\nrobots.txt entry blocking the page. If you want to hide the page completely from Search,\nuse\nanother method\n.\nMedia file\nUse a robots.txt file to manage crawl traffic, and also to prevent image, video, and", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1967}
{"id": "29bc9997-da97-4aef-afcf-a311afe7f13f", "url": "https://developers.google.com/search/docs/crawling-indexing/robots/intro", "source_domain": "developers.google.com", "title": "Robots.txt Introduction and Guide | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "audio files from appearing in Google Search results. This won't prevent other pages or\nusers from linking to your image, video, or audio file.\nRead more about preventing images from appearing on Google.\nRead more about how to remove or restrict your video files from appearing on Google.\nResource file\nYou can use a robots.txt file to block resource files such as unimportant image, script,\nor style files,\nif you think that pages loaded without these resources won't\nbe significantly affected by the loss\n. However, if the absence of these\nresources make the page harder for Google's crawler to understand the page, don't block\nthem, or else Google won't do a good job of analyzing pages that depend on\nthose resources.\nUnderstand the limitations of a robots.txt file\nBefore you create or edit a robots.txt file, you should know the limits of this URL blocking\nmethod. Depending on your goals and situation, you might want to consider other mechanisms to\nensure your URLs are not findable on the web.\nrobots.txt rules may not be supported by all search engines.\nThe instructions in robots.txt files cannot enforce crawler behavior to your site; it's up\nto the crawler to obey them. While Googlebot and other respectable web crawlers obey the\ninstructions in a robots.txt file, other crawlers might not. Therefore, if you want to keep\ninformation secure from web crawlers, it's better to use other blocking methods, such as\npassword-protecting private files on your server\n.\nDifferent crawlers interpret syntax differently.\nAlthough respectable web crawlers follow the rules in a robots.txt file, each crawler\nmight interpret the rules differently. You should know the\nproper syntax\nfor addressing\ndifferent web crawlers as some might not understand certain instructions.\nA page that's disallowed in robots.txt can\nstill be indexed if linked to from other sites.\nWhile Google won't crawl or index the content blocked by a robots.txt file, we might still", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1968, "chunk_char_end": 3921}
{"id": "14cfcc61-fb8e-430f-b462-3a2c1d3d6b4c", "url": "https://developers.google.com/search/docs/crawling-indexing/robots/intro", "source_domain": "developers.google.com", "title": "Robots.txt Introduction and Guide | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "find and index a disallowed URL if it is linked from other places on the web. As a result,\nthe URL address and, potentially, other publicly available information such as anchor text\nin links to the page can still appear in Google Search results. To properly prevent your URL\nfrom appearing in Google Search results,\npassword-protect the files on your server\n,\nuse the\nnoindex\nmeta\ntag or response header\n,\nor remove the page entirely.\nCreate or update a robots.txt file\nIf you decided that you need one, learn how to\ncreate a robots.txt file\n. Or if\nyou already have one, learn how to\nupdate it\n.\nWant to learn more? Check out the following resources:\nHow to write and submit a robots.txt file\nUpdate your robots.txt file\nHow Google interprets the robots.txt specification\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-20 UTC.", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3922, "chunk_char_end": 5034}
{"id": "bb006a76-6505-4975-bacc-05952168e3c7", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers\nSkip to main content\nCrawling infrastructure\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nHome\nCrawling infrastructure\nDocs\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nHow Google interprets the robots.txt specification\nGoogle's automated\ncrawlers\nsupport the\nRobots Exclusion Protocol (REP)\n.\nThis means that before crawling a site, Google's crawlers download and parse the site's\nrobots.txt file to extract information about which parts of the site may be crawled. The REP\nisn't applicable to Google's crawlers that are controlled by users (for example, feed\nsubscriptions), or crawlers that are used to increase user safety (for example, malware\nanalysis).\nThis page describes Google's interpretation of the REP. For the original\nstandard, check\nRFC 9309\n.\nWhat is a robots.txt file\nIf you don't want crawlers to access sections of your site, you can create a robots.txt file\nwith appropriate rules. A robots.txt file is a text file containing rules about which\ncrawlers may access which parts of a site. For example, the robots.txt file for example.com\nmay look like this:\n# This robots.txt file controls crawling of URLs under https://example.com.\n# All crawlers are disallowed to crawl files in the \"includes\" directory, such\n# as .css, .js, but Google needs them for rendering, so Googlebot is allowed\n# to crawl them.\nUser-agent: *\nDisallow: /includes/\nUser-agent: Googlebot\nAllow: /includes/\nSitemap: https://example.com/sitemap.xml\nIf you're new to robots.txt, start with our\nintro to robots.txt\n. You can also find\ntips for creating a robots.txt file\n.\nFile location and range of validity", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1936}
{"id": "db35d513-1be7-4b51-b335-b2dd1477ee0c", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "You must place the robots.txt file in the top-level directory of a site, on a supported\nprotocol. The URL for the robots.txt file is (like other URLs) case-sensitive. In case of\nGoogle Search, the supported protocols are HTTP, HTTPS, and FTP. On HTTP and HTTPS, crawlers\nfetch the robots.txt file with an HTTP non-conditional\nGET\nrequest; on FTP,\ncrawlers use a standard\nRETR (RETRIEVE)\ncommand, using anonymous login.\nThe rules listed in the robots.txt file apply only to the host, protocol, and port number\nwhere the robots.txt file is hosted.\nExamples of valid robots.txt URLs\nThe following table contains examples of robots.txt URLs and what URL paths they're valid for.\nColumn one contains the URL of a robots.txt file, and column two contains domains that that robots.txt file would and wouldn't apply to.\nRobots.txt URL examples\nhttps://example.com/robots.txt\nThis is the general case. It's not valid for other subdomains, protocols, or port\nnumbers. It's valid for all files in all subdirectories on the same host, protocol,\nand port number.\nValid for:\nhttps://example.com/\nhttps://example.com/folder/file\nNot valid for:\nhttps://other.example.com/\nhttp://example.com/\nhttps://example.com:8181/\nhttps://www.example.com/robots.txt\nA robots.txt on a subdomain is only valid for that subdomain.\nValid for:\nhttps://www.example.com/\nNot valid for:\nhttps://example.com/\nhttps://shop.www.example.com/\nhttps://www.shop.example.com/\nhttps://example.com/folder/robots.txt\nNot a valid robots.txt file. Crawlers don't check for robots.txt files in subdirectories.\nhttps://www.exämple.com/robots.txt\nIDNs are equivalent to their punycode versions. See also\nRFC 3492\n.\nValid for:\nhttps://www.exämple.com/\nhttps://xn--exmple-cua.com/\nNot valid for:\nhttps://www.example.com/\nftp://example.com/robots.txt\nValid for:\nftp://example.com/\nNot valid for:\nhttps://example.com/\nhttps://212.96.82.21/robots.txt\nA robots.txt with an IP-address as the hostname is only valid for crawling of that", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1937, "chunk_char_end": 3912}
{"id": "9fc870e4-0f5d-493a-b2d4-f7861f061d1f", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "IP address as the hostname. It isn't automatically valid for all websites hosted on that\nIP address (though it's possible that the robots.txt file is shared, in which case it\nwould also be available under the shared hostname).\nValid for:\nhttps://212.96.82.21/\nNot valid for:\nhttps://example.com/\n(even if hosted on\n212.96.82.21\n)\nhttps://example.com:443/robots.txt\nStandard port numbers (\n80\nfor HTTP,\n443\nfor HTTPS,\n21\nfor FTP) are equivalent to their default hostnames.\nValid for:\nhttps://example.com:443/\nhttps://example.com/\nNot valid for:\nhttps://example.com:444/\nhttps://example.com:8181/robots.txt\nRobots.txt files on non-standard port numbers are only valid for content made\navailable through those port numbers.\nValid for:\nhttps://example.com:8181/\nNot valid for:\nhttps://example.com/\nHandling of errors and HTTP status codes\nWhen requesting a robots.txt file, the HTTP status code of the server's response affects how\nthe robots.txt file will be used by Google's crawlers. The following table summarizes how\nGooglebot treats robots.txt files for different HTTP status codes.\nHandling of errors and HTTP status codes\n2xx (success)\nHTTP status codes that signal success prompt Google's crawlers to process the robots.txt\nfile as provided by the server.\n3xx (redirection)\nGoogle follows at least five redirect hops as defined by\nRFC 1945\nand then\nstops and treats it as a\n404\nfor the robots.txt file. This also applies to any\ndisallowed URLs in the redirect chain, since the crawler couldn't fetch rules due to\nthe redirects.\nGoogle doesn't follow logical redirects in robots.txt files (frames, JavaScript, or\nmeta refresh-type redirects).\n4xx (client errors)\nGoogle's crawlers treat all\n4xx\nerrors, except\n429\n, as if a\nvalid robots.txt file didn't exist. This means that Google assumes that there are no crawl\nrestrictions.\n5xx (server errors)\nIf Google finds a robots.txt file but can't fetch it, Google follows this behavior:", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3913, "chunk_char_end": 5849}
{"id": "d5993ab0-cb2a-4530-b45e-bb6611f854ff", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "For the first 12 hours, Google stops crawling the site but keeps trying to fetch the\nrobots.txt file.\nIf Google can't fetch a new version, for the next 30 days Google will use the last good\nversion, while still trying to fetch a new version. A\n503 (service unavailable)\nerror results in fairly frequent retrying. If there's no cached version available,\nGoogle assumes there's no crawl restrictions.\nIf the errors are still not fixed after 30 days:\nIf the site is generally available to Google, Google will behave as if there is no\nrobots.txt file (but still keep checking for a new version).\nIf the site has general availability problems, Google will stop crawling the site,\nwhile still periodically requesting a robots.txt file.\nOther errors\nA robots.txt file which cannot be fetched due to DNS or networking issues, such as\ntimeouts, invalid responses, reset or interrupted connections, and HTTP chunking errors,\nis treated as a\nserver error\n.\nCaching\nGoogle generally caches the contents of robots.txt file for up to 24 hours, but may cache it\nlonger in situations where refreshing the cached version isn't possible (for example, due to\ntimeouts or\n5xx\nerrors). The cached response may be shared by different crawlers.\nGoogle may increase or decrease the cache lifetime based on\nmax-age Cache-Control\nHTTP headers.\nFile format\nThe robots.txt file must be a\nUTF-8\nencoded plain text\nfile and the lines must be separated by\nCR\n,\nCR/LF\n, or\nLF\n.\nGoogle ignores invalid lines in robots.txt files, including the Unicode\nByte Order Mark\n(BOM) at the beginning of the robots.txt file, and use only valid lines. For example, if the\ncontent downloaded is HTML instead of robots.txt rules, Google will try to parse the content\nand extract rules, and ignore everything else.\nSimilarly, if the character encoding of the robots.txt file isn't UTF-8, Google may ignore\ncharacters that are not part of the UTF-8 range, potentially rendering robots.txt rules\ninvalid.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5850, "chunk_char_end": 7804}
{"id": "6a039c4f-476d-4de3-aa0d-da01143c3667", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "Google enforces a robots.txt file size limit of 500\nkibibytes\n(KiB). Content\nwhich is after the maximum file size is ignored. You can reduce the size of the robots.txt\nfile by consolidating rules that would result in an oversized robots.txt file. For\nexample, place excluded material in a separate directory.\nSyntax\nValid robots.txt lines consists of a field, a colon, and a value. Spaces are optional, but\nrecommended to improve readability. Space at the beginning and at the end of the line is\nignored. To include comments, precede your comment with the\n#\ncharacter. Keep in\nmind that everything after the\n#\ncharacter will be ignored. The general format is\n<field>:<value><#optional-comment>\n.\nGoogle supports the following fields (other fields such as\ncrawl-delay\naren't supported):\nuser-agent\n: identifies which crawler the rules apply to.\nallow\n: a URL path that may be crawled.\ndisallow\n: a URL path that may not be crawled.\nsitemap\n: the complete URL of a sitemap.\nThe\nallow\nand\ndisallow\nfields are also called rules (also known as\ndirectives). These rules are always specified in the form of\nrule: [path]\nwhere\n[path]\nis optional. By default, there are no\nrestrictions for crawling for the designated crawlers. Crawlers ignore rules without a\n[path]\n.\nThe\n[path]\nvalue, if specified, is relative to the root of the website from where\nthe robots.txt file was fetched (using the same protocol, port number, host and domain names).\nThe path value must start with\n/\nto designate the root and the value is\ncase-sensitive. Learn more about\nURL matching based on path values\n.\nuser-agent\nThe\nuser-agent\nline identifies which crawler rules apply to. See\nGoogle's crawlers and user-agent strings\nfor a comprehensive list of user-agent strings you can use in your robots.txt file.\nThe value of the\nuser-agent\nline is case-insensitive.\ndisallow\nThe\ndisallow\nrule specifies paths that must not be accessed by the crawlers\nidentified by the\nuser-agent\nline the\ndisallow\nrule is grouped with.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7805, "chunk_char_end": 9791}
{"id": "719dff76-163e-47aa-8878-405e135fd7a1", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "Crawlers ignore the rule without a path.\nGoogle can't index the content of pages which are disallowed for crawling, but it may still\nindex the URL and show it in search results without a snippet. Learn how to\nblock indexing\n.\nThe value of the\ndisallow\nrule is case-sensitive.\nUsage:\ndisallow: [path]\nallow\nThe\nallow\nrule specifies paths that may be accessed by the designated\ncrawlers. When no path is specified, the rule is ignored.\nThe value of the\nallow\nrule is case-sensitive.\nUsage:\nallow: [path]\nsitemap\nGoogle, Bing, and other major search engines support the\nsitemap\nfield in\nrobots.txt, as defined by\nsitemaps.org\n.\nThe value of the\nsitemap\nfield is case-sensitive.\nUsage:\nsitemap: [absoluteURL]\nThe\n[absoluteURL]\nline points to the location of a sitemap or sitemap index file.\nIt must be a fully qualified URL, including the protocol and host, and doesn't have to be\nURL-encoded. The URL doesn't have to be on the same host as the robots.txt file. You can\nspecify multiple\nsitemap\nfields. The sitemap field isn't tied to any specific\nuser agent and may be followed by all crawlers, provided it isn't disallowed for crawling.\nFor example:\nuser-agent: otherbot\ndisallow: /kale\nsitemap: https://example.com/sitemap.xml\nsitemap: https://cdn.example.org/other-sitemap.xml\nsitemap: https://ja.example.org/テスト-サイトマップ.xml\nGrouping of lines and rules\nYou can group together rules that apply to multiple user agents by repeating\nuser-agent\nlines for each crawler.\nFor example:\nuser-agent: a\ndisallow: /c\nuser-agent: b\ndisallow: /d\nuser-agent: e\nuser-agent: f\ndisallow: /g\nuser-agent: h\nIn this example there are four distinct rule groups:\nOne group for user agent \"a\".\nOne group for user agent \"b\".\nOne group for both \"e\" and \"f\" user agents.\nOne group for user agent \"h\".\nFor the technical description of a group, see\nsection 2.1 of the REP\n.\nOrder of precedence for user agents\nOnly one group is valid for a particular crawler. Google's crawlers determine the correct", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9792, "chunk_char_end": 11761}
{"id": "a0226762-c7fb-495e-ac4d-404efce6d669", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "group of rules by finding in the robots.txt file the group with the most specific user agent\nthat matches the crawler's user agent. Other groups are ignored. All non-matching text is\nignored (for example, both\ngooglebot/1.2\nand\ngooglebot*\nare\nequivalent to\ngooglebot\n). The order of the groups within the robots.txt file is\nirrelevant.\nIf there's more than one specific group declared for a user agent, all the rules from the\ngroups applicable to the specific user agent are combined internally into a single group. User\nagent specific groups and global groups (\n*\n) are not combined.\nExamples\nMatching of\nuser-agent\nfields\nuser-agent: googlebot-news\n(group 1)\nuser-agent: *\n(group 2)\nuser-agent: googlebot\n(group 3)\nThis is how the crawlers would choose the relevant group:\nGroup followed per crawler\nGooglebot News\ngooglebot-news\nfollows group 1, because group 1 is the most specific group.\nGooglebot (web)\ngooglebot\nfollows group 3.\nGooglebot Storebot\nStorebot-Google\nfollows group 2, because there is no specific\nStorebot-Google\ngroup.\nGooglebot News (when crawling images)\nWhen crawling images,\ngooglebot-news\nfollows group 1.\ngooglebot-news\ndoesn't crawl the images for Google Images, so it only\nfollows group 1.\nOtherbot (web)\nOther Google crawlers follow group 2.\nOtherbot (news)\nOther Google crawlers that crawl news content, but don't identify as\ngooglebot-news\nfollow group 2. Even if there is an entry for a related\ncrawler, it is only valid if it's specifically matching.\nGrouping of rules\nIf there are multiple groups in a robots.txt file that are relevant to a specific user agent,\nGoogle's crawlers internally merge the groups. For example:\nuser-agent: googlebot-news\ndisallow: /fish\nuser-agent: *\ndisallow: /carrots\nuser-agent: googlebot-news\ndisallow: /shrimp\nThe crawlers internally group the rules based on user agent, for example:\nuser-agent: googlebot-news\ndisallow: /fish\ndisallow: /shrimp\nuser-agent: *\ndisallow: /carrots\nRules other than\nallow\n,\ndisallow\n, and\nuser-agent\nare", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11762, "chunk_char_end": 13762}
{"id": "1efa149c-eec8-4b25-8b0d-15cc10cd0178", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "ignored by the robots.txt parser. This means that the following robots.txt snippet is treated\nas one group, and thus both\nuser-agent\na\nand\nb\nare\naffected by the\ndisallow: /\nrule:\nuser-agent: a\nsitemap: https://example.com/sitemap.xml\nuser-agent: b\ndisallow: /\nWhen the crawlers process the robots.txt rules, they ignore the\nsitemap\nline.\nFor example, this is how the crawlers would understand the previous robots.txt snippet:\nuser-agent: a\nuser-agent: b\ndisallow: /\nURL matching based on path values\nGoogle uses the path value in the\nallow\nand\ndisallow\nrules as a\nbasis to determine whether or not a rule applies to a specific URL on a site. This works by\ncomparing the rule to the path component of the URL that the crawler is trying to fetch.\nNon-7-bit ASCII characters in a path may be included as UTF-8 characters or as percent-escaped\nUTF-8 encoded characters per\nRFC 3986\n.\nGoogle, Bing, and other major search engines support a limited form of\nwildcards\nfor\npath values. These wildcard characters are:\n*\ndesignates 0 or more instances of any valid character.\n$\ndesignates the end of the URL.\nThe following table shows how the different wildcard characters affect parsing:\nExample path matches\n/\nMatches the root and any lower level URL.\n/*\nEquivalent to\n/\n. The trailing wildcard is ignored.\n/$\nMatches only the root. Any lower level URL is allowed for crawling.\n/fish\nMatches any path that starts with\n/fish\n. Note that the matching is case-sensitive.\nMatches:\n/fish\n/fish.html\n/fish/salmon.html\n/fishheads\n/fishheads/yummy.html\n/fish.php?id=anything\nDoesn't match:\n/Fish.asp\n/catfish\n/?id=fish\n/desert/fish\n/fish*\nEquivalent to\n/fish\n. The trailing wildcard is ignored.\nMatches:\n/fish\n/fish.html\n/fish/salmon.html\n/fishheads\n/fishheads/yummy.html\n/fish.php?id=anything\nDoesn't match:\n/Fish.asp\n/catfish\n/?id=fish\n/desert/fish\n/fish/\nMatches anything in the\n/fish/\nfolder.\nMatches:\n/fish/\n/fish/?id=anything\n/fish/salmon.htm\nDoesn't match:\n/fish\n/fish.html\n/animals/fish/\n/Fish/Salmon.asp", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13763, "chunk_char_end": 15759}
{"id": "c3f316d7-1f3c-4112-aae3-398b6d12a000", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": "/*.php\nMatches any path that contains\n.php\n.\nMatches:\n/index.php\n/filename.php\n/folder/filename.php\n/folder/filename.php?parameters\n/folder/any.php.file.html\n/filename.php/\nDoesn't match:\n/\n(even if it maps to /index.php)\n/windows.PHP\n/*.php$\nMatches any path that ends with\n.php\n.\nMatches:\n/filename.php\n/folder/filename.php\nDoesn't match:\n/filename.php?parameters\n/filename.php/\n/filename.php5\n/windows.PHP\n/fish*.php\nMatches any path that contains\n/fish\nand\n.php\n, in that order.\nMatches:\n/fish.php\n/fishheads/catfish.php?parameters\nDoesn't match:\n/Fish.PHP\nOrder of precedence for rules\nWhen matching robots.txt rules to URLs, crawlers use the most specific rule based on the\nlength of the rule path. In case of conflicting rules, including those with wildcards, Google uses\nthe least restrictive rule.\nThe following examples demonstrate which rule Google's crawlers will apply on a given URL.\nSample situations\nhttps://example.com/page\nallow: /p\ndisallow: /\nApplicable rule\n:\nallow: /p\n, because it's more specific.\nhttps://example.com/folder/page\nallow: /folder\ndisallow: /folder\nApplicable rule\n:\nallow: /folder\n, because in case of\nconflicting rules, Google uses the least restrictive rule.\nhttps://example.com/page.htm\nallow: /page\ndisallow: /*.htm\nApplicable rule\n:\ndisallow: /*.htm\n, because the rule path is longer and\nit matches more characters in the URL, so it's more specific.\nhttps://example.com/page.php5\nallow: /page\ndisallow: /*.ph\nApplicable rule\n:\nallow: /page\n, because in case of\nconflicting rules, Google uses the least restrictive rule.\nhttps://example.com/\nallow: /$\ndisallow: /\nApplicable rule\n:\nallow: /$\n, because it's more specific.\nhttps://example.com/page.htm\nallow: /$\ndisallow: /\nApplicable rule\n:\ndisallow: /\n, because the\nallow\nrule only applies on the root URL.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15760, "chunk_char_end": 17749}
{"id": "cc83d070-26ff-4897-8997-33ab54d18efa", "url": "https://developers.google.com/crawling/docs/robots-txt/robots-txt-spec", "source_domain": "developers.google.com", "title": "How Google Interprets the robots.txt Specification | Google Crawling Infrastructure  |  Crawling infrastructure  |  Google for Developers", "section_path": [], "text": ". For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-21 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17750, "chunk_char_end": 17899}
{"id": "728e588a-1658-453b-a26e-e04ac483cdda", "url": "https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data", "source_domain": "developers.google.com", "title": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nIntroduction to structured data markup in Google Search\nGoogle Search works hard to understand the content of a page. You can help us by providing explicit\nclues about the meaning of a page to Google by including structured data on the page.\nStructured data is a standardized format for providing information about a page and classifying\nthe page content; for example, on a recipe page, what are the ingredients, the cooking\ntime and temperature, the calories, and so on.\nWhy add structured data to a page?\nAdding structured data can enable search results that are more engaging to users and might\nencourage them to interact more with your website, which are called\nrich results\n.\nHere are some case studies of websites that have implemented structured data for their site:\nRotten Tomatoes added structured data to 100,000 unique pages and measured a 25% higher click-through rate for pages enhanced with structured data, compared to pages without structured data.\nThe Food Network has converted 80% of their pages to enable search features, and has seen a 35% increase in visits.\nRakuten has found that users spend 1.5x more time on pages that implemented structured data\nthan on non-structured data pages, and have a 3.6x higher interaction rate on AMP pages\nwith search features vs non-feature AMP pages.\nNestlé has measured pages that show as rich results in search have an 82% higher click\nthrough rate than non-rich result pages.\nHow structured data works in Google Search", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1983}
{"id": "bb8fd2f5-e0ab-4a8c-b279-872f340240cb", "url": "https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data", "source_domain": "developers.google.com", "title": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Google uses structured data that it finds on the web to understand the content of the page,\nas well as to gather information about the web and the world in general, such as information\nabout the people, books, or companies that are included in the markup. For example,\nwhen a recipe page has\nJSON-LD\nstructured data\n(describing the title of the recipe, the author of the recipe, and other details), Google Search\ncan use that information to display a rich result for the recipe:\nBecause the structured data labels each individual element of the recipe, users can search\nfor your recipe by ingredient, calorie count, cook time, and so on.\nStructured data is coded using in-page markup on the page that the information applies to.\nThe structured data on the page describes the content of that page. Don't create\nblank or empty pages just to hold structured data, and don't add structured data about\ninformation that is not visible to the user, even if the information is accurate. For more technical\nand quality guidelines, see the\nStructured data\ngeneral guidelines\n.\nThe\nRich Results Test\nis an easy and useful\ntool for validating your structured data, and in some cases, previewing a feature in Google Search. Try it out:\n<html>\n<head>\n<title>Non-Alcoholic Piña Colada</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org/\",\n\"@type\": \"Recipe\",\n\"name\": \"Non-Alcoholic Piña Colada\",\n\"image\": [\n\"https://example.com/photos/1x1/photo.jpg\",\n\"https://example.com/photos/4x3/photo.jpg\",\n\"https://example.com/photos/16x9/photo.jpg\"\n],\n\"author\": {\n\"@type\": \"Person\",\n\"name\": \"Mary Stone\"\n},\n\"datePublished\": \"2024-03-10\",\n\"description\": \"This non-alcoholic pina colada is everyone's favorite!\",\n\"recipeCuisine\": \"American\",\n\"prepTime\": \"PT1M\",\n\"cookTime\": \"PT2M\",\n\"totalTime\": \"PT3M\",\n\"keywords\": \"non-alcoholic\",\n\"recipeYield\": \"4 servings\",\n\"recipeCategory\": \"Drink\",\n\"nutrition\": {\n\"@type\": \"NutritionInformation\",\n\"calories\": \"120 calories\"\n},\n\"aggregateRating\": {", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1984, "chunk_char_end": 3969}
{"id": "03c042e1-3eb3-4c2d-a103-1929e348de44", "url": "https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data", "source_domain": "developers.google.com", "title": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"@type\": \"AggregateRating\",\n\"ratingValue\": 5,\n\"ratingCount\": 18\n},\n\"recipeIngredient\": [\n\"400ml of pineapple juice\",\n\"100ml cream of coconut\",\n\"ice\"\n],\n\"recipeInstructions\": [\n{\n\"@type\": \"HowToStep\",\n\"name\": \"Blend\",\n\"text\": \"Blend 400ml of pineapple juice and 100ml cream of coconut until smooth.\",\n\"url\": \"https://example.com/non-alcoholic-pina-colada#step1\",\n\"image\": \"https://example.com/photos/non-alcoholic-pina-colada/step1.jpg\"\n},\n{\n\"@type\": \"HowToStep\",\n\"name\": \"Fill\",\n\"text\": \"Fill a glass with ice.\",\n\"url\": \"https://example.com/non-alcoholic-pina-colada#step2\",\n\"image\": \"https://example.com/photos/non-alcoholic-pina-colada/step2.jpg\"\n},\n{\n\"@type\": \"HowToStep\",\n\"name\": \"Pour\",\n\"text\": \"Pour the pineapple juice and coconut mixture over ice.\",\n\"url\": \"https://example.com/non-alcoholic-pina-colada#step3\",\n\"image\": \"https://example.com/photos/non-alcoholic-pina-colada/step3.jpg\"\n}\n],\n\"video\": {\n\"@type\": \"VideoObject\",\n\"name\": \"How to Make a Non-Alcoholic Piña Colada\",\n\"description\": \"This is how you make a non-alcoholic piña colada.\",\n\"thumbnailUrl\": [\n\"https://example.com/photos/1x1/photo.jpg\",\n\"https://example.com/photos/4x3/photo.jpg\",\n\"https://example.com/photos/16x9/photo.jpg\"\n],\n\"contentUrl\": \"https://www.example.com/video123.mp4\",\n\"embedUrl\": \"https://www.example.com/videoplayer?video=123\",\n\"uploadDate\": \"2024-02-05T08:00:00+08:00\",\n\"duration\": \"PT1M33S\",\n\"interactionStatistic\": {\n\"@type\": \"InteractionCounter\",\n\"interactionType\": { \"@type\": \"WatchAction\" },\n\"userInteractionCount\": 2347\n},\n\"expires\": \"2024-02-05T08:00:00+08:00\"\n}\n}\n</script>\n</head>\n<body>\n</body>\n</html>\nStructured data vocabulary and format\nThis documentation describes which properties are required, recommended, or optional for\nstructured data with special meaning to Google Search. Most Search structured data uses\nschema.org\nvocabulary, but you should rely\non the Google Search Central documentation as definitive for Google Search behavior, rather", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3970, "chunk_char_end": 5926}
{"id": "faaf77c4-d8fa-40cc-99c3-d4d676021357", "url": "https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data", "source_domain": "developers.google.com", "title": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "than the schema.org documentation. There are more attributes and objects on schema.org that\naren't required by Google Search; they may be useful for other search engines, services, tools, and platforms.\nBe sure to check your structured data using the\nRich Results Test\nduring development, and the\nRich result status reports\nafter deployment, to monitor the validity of your pages, which might break\nafter deployment due to templating or serving issues.\nYou must include all the required properties for an object to be eligible for appearance in\nGoogle Search with enhanced display. In general, defining more recommended features can make\nit more likely that your information can appear in Search results with enhanced display.\nHowever\n, it is more important to supply fewer but complete and accurate\nrecommended properties rather than trying to provide every possible recommended property with\nless complete, badly-formed, or inaccurate data.\nIn addition to the properties and objects documented here, Google can make general use of the\nsameAs\nproperty and other\nschema.org\nstructured data. Some of these elements may be used to enable future Search features, if they\nare deemed useful.\nSupported formats\nGoogle Search supports structured data in the following formats, unless documented otherwise.\nIn general, we recommend using a format that's easiest for you to implement and maintain (in most cases,\nthat's JSON-LD); all 3 formats are equally fine for Google, as long as the markup is valid and\nproperly implemented per the feature's documentation.\nFormats\nJSON-LD\n*\n(Recommended)\nA JavaScript notation embedded in a\n<script>\ntag in the\n<head>\nand\n<body>\nelements of an HTML page. The\nmarkup is not interleaved with the user-visible text, which makes nested data items easier\nto express, such as the\nCountry\nof a\nPostalAddress\nof a\nMusicVenue\nof an\nEvent\n.\nAlso, Google can read JSON-LD data when it is\ndynamically\ninjected into the page's contents", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5927, "chunk_char_end": 7879}
{"id": "9d116fcb-5b76-4998-a9ab-f323624e3f8c", "url": "https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data", "source_domain": "developers.google.com", "title": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": ", such as by JavaScript code or embedded widgets in\nyour content management system.\nMicrodata\nAn open-community HTML specification used to nest structured data within HTML\ncontent. Like RDFa, it uses HTML tag attributes to name the properties you want\nto expose as structured data. It is typically used in the\n<body>\nelement, but can be used in the\n<head>\nelement.\nRDFa\nAn HTML5 extension that supports linked data by introducing\nHTML tag attributes\nthat\ncorrespond to the user-visible content that you want to describe for search engines. RDFa\nis commonly used in both the\n<head>\nand\n<body>\nsections of the HTML page.\nStructured data guidelines\nBe sure to follow the\ngeneral structured data guidelines\n, as well\nas any guidelines specific to your structured data type; otherwise your structured\ndata might be ineligible for rich result display in Google Search.\nGet started with structured data\nIf you're new to structured data, check out\nschema.org\nbeginner's guide to structured data\n. While the guide focuses on Microdata,\nthe basic ideas are relevant for JSON-LD and RDFa.\nOnce you're comfortable with the basics of structured data, explore the\nlist of structured data features in Google Search\nand pick a feature to implement. Each guide goes into detail on how to\nimplement the structured data in a way that makes your site eligible for a rich result\nappearance on Google Search.\nChoose a feature\nMeasuring the effect of structured data\nYou probably want to compare performance of your pages with structured data with those pages that\ndon't have structured data, in order to decide if it's worth your effort. The best way to do that\nis to run a\nbefore and after test on a few pages on your site\n.\nThis can be a little tricky, since page views can vary for a single page for various reasons.\nTake some pages on your site that are not using any structured data, and have several months of\ndata in Search Console. Be sure to choose pages that won't be affected by the time of year or", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7880, "chunk_char_end": 9867}
{"id": "1bd2e814-3821-421e-8635-f76aa5b8aafe", "url": "https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data", "source_domain": "developers.google.com", "title": "Intro to How Structured Data Markup Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "timeliness of the page content; use pages that won't change much, but are still popular enough to\nbe read often enough to generate meaningful data.\nAdd structured data or other features to your pages. Confirm that your markup is valid, and\nthat Google has found your structured data using the\nURL Inspection tool\non your page.\nRecord the performance for a few months in the\nPerformance report\n,\nand filter by URL to compare performance of your page.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-02-04 UTC.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9868, "chunk_char_end": 10657}
{"id": "b73c5813-fc28-4b8b-b962-14e7ee2af50a", "url": "https://developers.google.com/search/docs/appearance/structured-data/search-gallery", "source_domain": "developers.google.com", "title": "Structured Data Markup that Google Search Supports | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Structured Data Markup that Google Search Supports | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nStructured data markup that Google Search supports\nGoogle uses\nstructured data\nto understand the content on the page and show that content in a richer appearance in search\nresults, which is called a\nrich result\n. To make your site eligible for appearance as\none of these rich results, follow the guide to learn how to implement structured data on your site.\nIf you're just getting started, visit\nUnderstand how structured data works\n.\nChoose a category that describes your website\nFilter by...\nEcommerce\nOrganizations\nSports\nJobs\nEntertainment\nNews\nFood and Drink\nEducation and Science\nStructured data features\nFilter column\nArticle\nA news, sports, or blog article displayed in various rich result features, such as\nthe title of the article and larger-than-thumbnail images.\nGet started\nNews, Sports\nBreadcrumb\nNavigation that indicates the page's position in the site hierarchy.\nGet started\nGeneric\nCarousel\nRich results that display in a sequential list or gallery from a single site. This\nfeature must be combined with one of the following features:\nRecipe\n,\nCourse list\n,\nRestaurant\n,\nMovie\n.\nGet started\nFood and Drink, Education and Science, Entertaiment\nCourse list\nA list of educational course from the same course provider. Courses can include\nthe course title, provider, and a short description.\nGet started\nEducation and Science\nDataset\nLarge data sets that appear in Google Dataset Search.\nGet started\nEducation and Science\nDiscussion forum", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1966}
{"id": "7f68190d-509e-4ca5-a82f-34a279146811", "url": "https://developers.google.com/search/docs/appearance/structured-data/search-gallery", "source_domain": "developers.google.com", "title": "Structured Data Markup that Google Search Supports | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "User-generated content (traditionally short-form compared to\nArticle\n),\nfollowed by a threaded or non-threaded discussion about that topic.\nGet started\nEducation Q&A\nEducation-related questions and answers that help students discover flashcards on Google Search.\nGet started\nEducation and Science\nEmployer aggregate rating\nAn evaluation of a hiring organization compiled from many users that's displayed in\nthe job search experience on Google.\nGet started\nJobs\nEvent\nAn interactive rich result that shows a list of organized events, such as concerts\nor art festivals, that people may attend at a particular time and place.\nGet started\nEntertainment\nFAQ\nA Frequently Asked Question (FAQ) page contains a list of questions and answers\npertaining to a particular topic.\nGet started\nImage metadata\nWhen you specify image metadata, Google Images can show more details about the image,\nsuch as who the creator is, how people can use an image, and credit information.\nGet started\nJob posting\nAn interactive rich result that allows job seekers to find a job. The job search\nexperience on Google can feature your logo, reviews, ratings, and job details.\nGet started\nJobs\nLocal business\nBusiness details displayed in the Google knowledge panel, including open hours,\nratings, directions, and actions to book appointments or order items.\nGet started\nOrganizations\nMath solver\nHelp students, teachers, and others with math problems by adding structured data to\nindicate the type of math problems and step-by-step walkthroughs for specific math problems.\nGet started\nEducation and Science\nMovie\nThe movie carousel helps users explore lists of movies on Google Search (for example,\n\"best movies of 2023\"). You can provide details about the movies, such as the title of\neach movie, director information, and images.\nGet started\nEntertainment\nOrganization\nInformation about your organization, such as your logo, legal name of the\norganization, address, contact information, and company identifiers. This information", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1967, "chunk_char_end": 3966}
{"id": "6723c142-037a-401d-b47d-147823985c77", "url": "https://developers.google.com/search/docs/appearance/structured-data/search-gallery", "source_domain": "developers.google.com", "title": "Structured Data Markup that Google Search Supports | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "can show up in knowledge panels and other visual elements (such as\nattribution\n).\nGet started\nOrganizations\nPractice problem\nHelp students, teachers, and parents with education by adding structured data to your practice problems in math and science subjects.\nGet started\nEducation and Science\nProduct\nInformation about a product, including price, availability, and review ratings.\nGet started\nEcommerce\nProfile page\nA page that primarily focuses on information about a single person or organization\nthat is somehow affiliated with the overall website.\nGet started\nQ&A\nQ&A Pages are web pages that contain data in a question and answer format,\nwhich is one question followed by its answers.\nGet started\nRecipe\nRecipes that display as an individual rich result or part of a host carousel.\nGet started\nFood and Drink\nReview snippet\nA short excerpt of a review or a rating from a review website, usually an average\nof the combined rating scores from reviewers. A review snippet can be about\nBook\n,\nRecipe\n,\nMovie\n,\nProduct\n,\nSoftware App\n,\nand\nLocal business\n.\nGet started\nOrganizations, Ecommerce, Food and Drink, Entertainment\nSoftware app\nInformation about a software app, including rating information, a description of\nthe app, and a link to the app.\nGet started\nSpeakable\nAllow search engines and other applications to identify news content to read aloud\non Google Assistant-enabled devices using text-to-speech (TTS).\nGet started\nNews\nSubscription and paywalled content\nIndicate paywalled content on your site to help Google differentiate paywalled\ncontent from the practice of\ncloaking\n, which violates\nour spam policies\n.\nGet started\nNews\nVacation rental\nInformation about a vacation property, such as the name, description,\nimages, location, rating, and reviews.\nGet started\nVideo\nVideo information in search results, with the option to play the video, specify video\nsegments, and live-stream content.\nGet started\nFood and Drink, News, Education and Science, Sports\nSend feedback", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3967, "chunk_char_end": 5951}
{"id": "03989eff-e2e5-4bd2-bbd7-067382d1132e", "url": "https://developers.google.com/search/docs/appearance/structured-data/search-gallery", "source_domain": "developers.google.com", "title": "Structured Data Markup that Google Search Supports | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Except as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-12 UTC.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5952, "chunk_char_end": 6277}
{"id": "e52c2024-015c-4a44-9ac7-d18361645ed6", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nOrganization (\nOrganization\n) structured data\nMerchant knowledge panel in Google Search results\nAdding organization structured data to your home page can help Google better understand your\norganization's administrative details and disambiguate your organization in search results. Some\nproperties are used behind the scenes to disambiguate your organization from other organizations\n(like\niso6523\nand\nnaics\n), while others can influence visual elements in\nSearch results (such as which\nlogo\nis shown in Search results and your\nknowledge panel\n).\nIf you're a merchant, you can influence more details in your\nmerchant knowledge panel\nand\nbrand profile\n,\nsuch as return policy, address, and contact information. There are no required properties;\ninstead, we recommend adding as many properties that are relevant to your organization.\nHow to add structured data\nStructured data is a standardized format for providing information about a page and classifying the page\ncontent. If you're new to structured data, you can learn more about\nhow structured data works\n.\nHere's an overview of how to build, test, and release structured data.\nAdd as many\nrecommended properties\nthat apply\nto your web page. There are no required properties; instead, add the properties that apply to\nyour content. Based on the format you're using, learn where to\ninsert structured data on the page\n.\nFollow the\nguidelines\n.\nValidate your code using the\nRich Results Test", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1930}
{"id": "3dc6a4a6-9f3a-4a69-8564-11beb1396061", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "and fix any critical errors. Consider also fixing any non-critical issues that may be flagged\nin the tool, as they can help improve the quality of your structured data (however, this isn't necessary to be eligible for rich results).\nDeploy a few pages that include your structured data and use the\nURL Inspection tool\nto test how Google sees the page. Be sure that your page is\naccessible to Google and not blocked by a robots.txt file, the\nnoindex\ntag, or\nlogin requirements. If the page looks okay, you can\nask Google to recrawl your URLs\n.\nTo keep Google informed of future changes, we recommend that you\nsubmit a sitemap\n. You can automate this with the\nSearch Console Sitemap API\n.\nExamples\nOrganization\nHere's an example of organization information in JSON-LD code.\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"Organization\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\"https://example.net/profile/example1234\", \"https://example.org/example1234\"],\n\"logo\": \"https://www.example.com/images/logo.png\",\n\"name\": \"Example Corporation\",\n\"description\": \"The example corporation is well-known for producing high-quality widgets\",\n\"email\": \"contact@example.com\",\n\"telephone\": \"+47-99-999-9999\",\n\"address\": {\n\"@type\": \"PostalAddress\",\n\"streetAddress\": \"Rue Improbable 99\",\n\"addressLocality\": \"Paris\",\n\"addressCountry\": \"FR\",\n\"addressRegion\": \"Ile-de-France\",\n\"postalCode\": \"75001\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\"\n}\n</script>\n</head>\n<body>\n</body>\n</html>\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"Organization\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\"https://example.net/profile/example1234\", \"https://example.org/example1234\"],\n\"logo\": \"https://www.example.com/images/logo.png\",\n\"name\": \"Example Corporation\",\n\"description\": \"The example corporation is well-known for producing high-quality widgets\",", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1931, "chunk_char_end": 3930}
{"id": "41f8e688-c0cf-471a-8fad-2b23c45cae9f", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"email\": \"contact@example.com\",\n\"telephone\": \"+47-99-999-9999\",\n\"address\": {\n\"@type\": \"PostalAddress\",\n\"streetAddress\": \"Rue Improbable 99\",\n\"addressLocality\": \"Paris\",\n\"addressCountry\": \"FR\",\n\"addressRegion\": \"Ile-de-France\",\n\"postalCode\": \"75001\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\"\n}\n</script>\n</head>\n<body>\n</body>\n</html>\nOnlineStore\n(subtype of\nOrganization\n) with a shipping policy and return policy\nHere's an example of an online store with both a shipping policy and a return policy in JSON-LD code.\nRefer to the separate\nMerchant return policy markup\ndocumentation for more examples and detailed information for merchant-level standard return policies.\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"OnlineStore\",\n\"name\": \"Example Online Store\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\n\"https://example.net/profile/example12\",\n\"https://example.org/@example34\"\n],\n\"logo\": \"https://www.example.com/assets/images/logo.png\",\n\"contactPoint\": {\n\"contactType\": \"Customer Service\",\n\"email\": \"support@example.com\",\n\"telephone\": \"+47-99-999-9900\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\",\n\"hasShippingService\": [\n{\n\"@type\": \"ShippingService\",\n\"name\": \"shipping to CH and FR\",\n\"description\": \"Shipping to CH 5% of order value, shipping to FR always free\",\n\"fulfillmentType\": \"FulfillmentTypeDelivery\",\n\"shippingConditions\": [\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"CH\"\n},\n\"shippingRate\": {\n\"@type\": \"ShippingRateSettings\",\n\"orderPercentage\": \"0.05\"\n}\n},\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingRate\": {\n\"@type\": \"MonetaryAmount\",\n\"value\": \"0\",\n\"currency\": \"EUR\"\n}\n}\n]", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3931, "chunk_char_end": 5931}
{"id": "0f43e975-3cc6-4da0-9912-330cebcf054e", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "}\n],\n\"hasMerchantReturnPolicy\": {\n\"@type\": \"MerchantReturnPolicy\",\n\"applicableCountry\": [\n\"FR\",\n\"CH\"\n],\n\"returnPolicyCountry\": \"FR\",\n\"returnPolicyCategory\": \"https://schema.org/MerchantReturnFiniteReturnWindow\",\n\"merchantReturnDays\": 60,\n\"returnMethod\": \"https://schema.org/ReturnByMail\",\n\"returnFees\": \"https://schema.org/FreeReturn\",\n\"refundType\": \"https://schema.org/FullRefund\"\n}\n// Other Organization-level properties\n// ...\n}\n</script>\n</head>\n<body>\n</body>\n</html>\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"OnlineStore\",\n\"name\": \"Example Online Store\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\n\"https://example.net/profile/example12\",\n\"https://example.org/@example34\"\n],\n\"logo\": \"https://www.example.com/assets/images/logo.png\",\n\"contactPoint\": {\n\"contactType\": \"Customer Service\",\n\"email\": \"support@example.com\",\n\"telephone\": \"+47-99-999-9900\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\",\n\"hasShippingService\": [\n{\n\"@type\": \"ShippingService\",\n\"name\": \"shipping to CH and FR\",\n\"description\": \"Shipping to CH 5% of order value, shipping to FR always free\",\n\"fulfillmentType\": \"FulfillmentTypeDelivery\",\n\"shippingConditions\": [\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"CH\"\n},\n\"shippingRate\": {\n\"@type\": \"ShippingRateSettings\",\n\"orderPercentage\": \"0.05\"\n}\n},\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingRate\": {\n\"@type\": \"MonetaryAmount\",\n\"value\": \"0\",\n\"currency\": \"EUR\"\n}\n}\n]\n}\n],\n\"hasMerchantReturnPolicy\": {\n\"@type\": \"MerchantReturnPolicy\",\n\"applicableCountry\": [\n\"FR\",\n\"CH\"\n],\n\"returnPolicyCountry\": \"FR\",\n\"returnPolicyCategory\": \"https://schema.org/MerchantReturnFiniteReturnWindow\",", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5932, "chunk_char_end": 7917}
{"id": "c1d67aa8-4b01-4e27-af37-a1f448f1dce1", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"merchantReturnDays\": 60,\n\"returnMethod\": \"https://schema.org/ReturnByMail\",\n\"returnFees\": \"https://schema.org/FreeReturn\",\n\"refundType\": \"https://schema.org/FullRefund\"\n}\n// Other Organization-level properties\n// ...\n}\n</script>\n</head>\n<body>\n</body>\n</html>\nGuidelines\nYou must follow these guidelines to enable structured data to be eligible for inclusion in Google\nSearch results.\nTechnical guidelines\nSearch Essentials\nGeneral structured data guidelines\nTechnical guidelines\nWe recommend placing this information on your home page, or a single page that describes your\norganization, for example the\nabout us\npage. You don't need to include it on every\npage of your site.\nWe recommend using the most specific schema.org subtype of\nOrganization\nthat matches your organization. For example, if you have an ecommerce site, then we recommend using the\nOnlineStore\nsubtype instead of\nOnlineBusiness\n.\nAnd if your site is about a local business, for example a restaurant or a physical store, then we\nrecommend providing your administrative details using the most specific\nsubtype(s)\nof\nLocalBusiness\nand following the required and recommended fields for\nLocal business\nin addition to the fields recommended in this guide.\nStructured data type definitions\nGoogle recognizes the following properties of an\nOrganization\n.\nTo help Google better understand your page, include as many recommended properties that apply\nto your web page. There are no required properties; instead, add the properties that apply to\nyour organization.\nRecommended properties\naddress\nPostalAddress\nThe address (physical or mailing) of your organization, if applicable. Include all properties that apply to your country. The more\nproperties you provide, the higher quality the result is for users.\nYou can provide multiple addresses if you have a location in multiple cities, states, or countries.\nFor example:\n\"address\"\n:\n[{\n\"@type\"\n:\n\"PostalAddress\"\n,\n\"streetAddress\"\n:\n\"999 W Example St Suite 99 Unit 9\"\n,\n\"addressLocality\"\n:", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7918, "chunk_char_end": 9917}
{"id": "6a3b047b-8bee-40be-ab97-530e2b39bef7", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"New York\"\n,\n\"addressRegion\"\n:\n\"NY\"\n,\n\"postalCode\"\n:\n\"10019\"\n,\n\"addressCountry\"\n:\n\"US\"\n},{\n\"streetAddress\"\n:\n\"999 Rue due exemple\"\n,\n\"addressLocality\"\n:\n\"Paris\"\n,\n\"postalCode\"\n:\n\"75001\"\n,\n\"addressCountry\"\n:\n\"FR\"\n}]\naddress.addressCountry\nText\nThe country for your postal address, using the two-letter\nISO 3166-1 alpha-2 country code.\naddress.addressLocality\nText\nThe city of your postal address.\naddress.addressRegion\nText\nThe region of your postal address, if applicable. For example, a state.\naddress.postalCode\nText\nThe postal code for your address.\naddress.streetAddress\nText\nThe full street address of your postal address.\nalternateName\nText\nAnother common name that your organization goes by, if applicable.\ncontactPoint\nContactPoint\nThe best way for a user to contact your business, if applicable. Include all support methods available to your users\nfollowing Google recommended\nbest practices\n. For example:\n\"contactPoint\"\n:\n{\n\"@type\"\n:\n\"ContactPoint\"\n,\n\"telephone\"\n:\n\"+9-999-999-9999\"\n,\n\"email\"\n:\n\"contact@example.com\"\n}\ncontactPoint.email\nText\nThe email address to contact your business, if applicable.\nIf you are using a\nLocalBusiness\ntype, specify a primary email address at\nthe\nLocalBusiness\nlevel before using\ncontactPoint\nto specify\nmultiple ways to reach your organization.\ncontactPoint.telephone\nText\nThe phone number to contact your business, if applicable.\nBe sure to include the country code and area code in the phone number.\nIf you are using a\nLocalBusiness\ntype, specify a primary phone number at\nthe\nLocalBusiness\nlevel before using\ncontactPoint\nto specify\nmultiple ways to reach your organization.\ndescription\nText\nA detailed description of your organization, if applicable.\nduns\nText\nThe Dun & Bradstreet DUNS number for identifying your\nOrganization\n, if\napplicable. We encourage using the\niso6523Code\nfield with prefix\n0060:\ninstead.\nemail\nText\nThe email address to contact your business, if applicable.\nfoundingDate\nDate\nThe date your\nOrganization\nwas founded in", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9918, "chunk_char_end": 11909}
{"id": "b8ff90dc-7c32-438f-b759-0a5ae11a5ee9", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "ISO 8601 date format\n, if applicable.\nglobalLocationNumber\nText\nThe GS1 Global Location Number identifying the location of your\nOrganization\n,\nif applicable.\nhasMerchantReturnPolicy\nRepeated\nMerchantReturnPolicy\nThe return policy of your\nOrganization\n, if applicable. See\nMerchant return policy markup\nfor detailed information on required and optional properties for\nMerchantReturnPolicy\n.\nhasMemberProgram\nRepeated\nMemberProgram\nA member (loyalty) program that you provide, if applicable.\nSee\nMember program markup\nfor detailed information on required and optional properties for\nMemberProgram\n.\nhasShippingService\nRepeated\nShippingService\nThe shipping policy of your\nOrganization\n, if applicable. See\nMerchant shipping policy markup\nfor detailed information on required and optional properties for\nShippingService\n.\niso6523Code\nText\nThe ISO 6523 identifier of your organization, if applicable.\nThe first part of an ISO 6523 identifier is an\nICD\n(International Code Designator)\nwhich defines which identification scheme is used.\nThe second part is the actual identifier. We recommend separating the ICD and the\nidentifier with a colon character (\nU+003A\n). Common ICD values include:\n0060\n: Dun & Bradstreet Data Universal Numbering System (DUNS)\n0088\n: GS1 Global Location Number (GLN)\n0199\n: Legal Entity Identifier (LEI)\nlegalName\nText\nThe registered, legal name of your\nOrganization\n, if applicable and different\nfrom the\nname\nproperty.\nleiCode\nText\nThe identifier for your\nOrganization\nas defined in ISO 17442, if applicable.\nWe encourage using the\niso6523Code\nfield with prefix\n0199:\ninstead.\nlogo\nURL\nor\nImageObject\nA logo that is representative of your organization, if applicable. Adding this property can help Google\nbetter understand which logo you want to show, for example in Search results and knowledge\npanels.\nImage guidelines:\nThe image must be 112x112px, at minimum.\nThe image URL must be crawlable and indexable.\nThe image file format must be\nsupported by Google Images\n.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11910, "chunk_char_end": 13901}
{"id": "920fc654-f4e4-4648-b9ac-e31b4b34872b", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Make sure the image looks how you intend it to look on a purely white background (for\nexample, if the logo is mostly white or gray, it may not look how you want it to look when\ndisplayed on a white background).\nIf you use the\nImageObject\ntype,\nmake sure that it has a valid\ncontentUrl\nproperty or\nurl\nproperty that follows the same guidelines as a\nURL\ntype.\nnaics\nText\nThe\nNorth American Industry Classification System (NAICS) code\nfor your\nOrganization\n, if applicable.\nname\nText\nThe name of your organization. Use the same\nname\nand\nalternateName\nthat you're using for your\nsite name\n.\nnumberOfEmployees\nQuantitativeValue\nThe number of employees in your\nOrganization\n, if applicable.\nExample with a specific number of employees:\n\"numberOfEmployees\"\n:\n{\n\"@type\"\n:\n\"QuantitativeValue\"\n,\n\"value\"\n:\n2056\n}\nExample with the number of employees in a range:\n\"numberOfEmployees\"\n:\n{\n\"@type\"\n:\n\"QuantitativeValue\"\n,\n\"minValue\"\n:\n100\n,\n\"maxValue\"\n:\n999\n}\nsameAs\nURL\nThe URL of a page on another website with additional information about your organization,\nif applicable. For example, a URL to your organization's profile page on a social media or\nreview site. You can provide multiple\nsameAs\nURLs.\ntaxID\nText\nThe tax ID associated with your\nOrganization\n, if applicable. Make sure\ntaxID\nmatches the country that you provided in the\naddress\nfield.\ntelephone\nText\nA business phone number meant to be the primary contact method for customers, if applicable.\nBe sure to include the country code and area code in the phone number.\nurl\nURL\nThe URL of the website of your organization, if applicable. This helps Google uniquely\nidentify your organization.\nvatID\nText\nThe VAT (Value Added Tax) code associated with your\nOrganization\n, if applicable\nto your country and business. This is an important trust signal for users (for example, users\ncan look up your business in public VAT registries).\nTroubleshooting\nIf you're having trouble implementing or debugging structured data, here are some resources that", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13902, "chunk_char_end": 15893}
{"id": "fa54abfd-087b-4339-8982-2470d0981462", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "may help you.\nIf you're using a content management system (CMS) or someone else is taking care of your site,\nask them to help you. Make sure to forward any Search Console message that details the issue to them.\nGoogle does not guarantee that features that consume structured data will show up in search results.\nFor a list of common reasons why Google may not show your content in a rich result, see the\nGeneral Structured Data Guidelines\n.\nYou might have an error in your structured data. Check the\nlist of structured data errors\nand the\nUnparsable structured data report\n.\nIf you received a structured data manual action against your page, the structured data on\nthe page will be ignored (although the page can still appear in Google Search results). To fix\nstructured data issues\n, use the\nManual Actions report\n.\nReview the\nguidelines\nagain to identify if your content isn't compliant\nwith the guidelines. The problem can be caused by either spammy content or spammy markup usage.\nHowever, the issue may not be a syntax issue, and so the Rich Results Test won't be able to\nidentify these issues.\nTroubleshoot missing rich results / drop in total rich results\n.\nAllow time for re-crawling and re-indexing. Remember that it may take several days after\npublishing a page for Google to find and crawl it. For general questions about crawling and indexing, check the\nGoogle Search crawling and indexing FAQ\n.\nPost a question in the\nGoogle Search Central forum\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-12 UTC.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15894, "chunk_char_end": 17694}
{"id": "497cf735-3f6e-4798-8a95-b39354d83c5c", "url": "https://developers.google.com/search/docs/appearance/structured-data/product", "source_domain": "developers.google.com", "title": "Intro to Product Structured Data on Google | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Intro to Product Structured Data on Google | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nIntroduction to\nProduct\nstructured data\nWhen you add structured data to your product pages, your product information can appear in richer\nways in Google Search results (including\nGoogle Images\nand\nGoogle Lens\n). For example, users can see price,\navailability, review ratings, shipping information, and more right in search results.\nDeciding which markup to use\nThere are two main classes of product structured data. Follow the requirements for the type that best\nsuits your use case:\nProduct snippets\n:\nFor product pages where people can't directly purchase the product. This markup has more options\nfor specifying review information, like\npros and cons\non an editorial product review page.\nMerchant listings\n:\nFor pages where customers can purchase products from you. This markup has more options for\nspecifying detailed product information, like\napparel sizing\n,\nshipping details\n,\nand\nreturn policy\ninformation.\nNote that there is some overlap between the two product features. In general, adding the\nrequired product information properties\nfor merchant listings means that your product pages can also be eligible for product snippets.\nBoth features have their own enhancements, so be sure to review both when deciding which markup\nmakes sense in the context of your site (the more properties you can add, the more enhancements\nyour page can be eligible for).\nIn addition to structured data for the individual products that you sell, we also recommend you add", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1968}
{"id": "19e4b816-20ad-4d2f-91db-00985b7ed7ac", "url": "https://developers.google.com/search/docs/appearance/structured-data/product", "source_domain": "developers.google.com", "title": "Intro to Product Structured Data on Google | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "structured data defining the policies of your ecommerce business, nested under\nOrganization\nmarkup:\nMerchant return policy\n:\nSpecify the return policy (or policies) for your business.\nLoyalty Program\n:\nSpecify the loyalty program that you offer.\nHow shopping experiences can appear on Google Search\nHere's how shopping experiences can appear in Google Search results.\nThis list is not exhaustive—Google Search is constantly exploring new and better ways\nto help people find what they're looking for, and the experiences may change over time.\nProduct snippet\nA\ntext result\nthat includes additional product information such as ratings, review information, price,\nand availability\nPopular products\nVisually rich presentation of products for sale\nShopping knowledge panel\nDetailed product information with a list of sellers (using details such as\nproduct identifiers)\nGoogle Images\nAnnotated images of products available for sale\nResult enhancements\nSearch result enhancements are shown at the discretion of each experience, and may\nchange over time. For this reason, it is recommended to provide as much rich product information\nas available, without concern for the exact experiences that will use it.\nHere are some examples of how product rich results may be enhanced:\nRatings\n: Enhance the appearance of your search result by providing\ncustomer reviews and ratings\n.\nPros and Cons\n: Identify\npros and cons\nin your product review description so they can be highlighted in search results.\nShipping\n: Share\nshipping costs\n,\nespecially free shipping, so shoppers understand the total cost.\nAvailability\n: Provide\navailability\ndata to help customers know when you have a product in stock.\nPrice drop\n: Price drops are computed by Google by observing price\nchanges for the product over time. Price drops are not guaranteed to be shown.\nReturns\n: Share\nreturn information\n,\nsuch as your return policy, fees involved in\nreturns, and how many days customers have to return a product.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1969, "chunk_char_end": 3943}
{"id": "6a20453b-580e-4047-9734-83e5aef32e37", "url": "https://developers.google.com/search/docs/appearance/structured-data/product", "source_domain": "developers.google.com", "title": "Intro to Product Structured Data on Google | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Providing product data to Google Search\nTo provide rich product data to Google Search you can add\nProduct\nstructured data to your web pages, upload data feeds with Google Merchant\nCenter and opt into free listings within the Merchant Center console, or both. The Search Central\ndocumentation focuses on structured data on web pages.\nProviding both structured data on web pages and a Merchant Center feed maximizes\nyour eligibility to experiences and helps Google correctly understand and verify your data.\nSome experiences combine data from structured data and Google Merchant Center feeds if\nboth are available. For example, product snippets may use pricing data from your\nmerchant feed if it's not present in the structured data on the page. The\nGoogle Merchant Center feed documentation\nincludes additional recommendations and requirements for feed attributes.\nIn addition to Google Search, learn more about eligibility to the\nGoogle Shopping tab\nby reading the\ndata and eligibility requirements in Google Merchant Center\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-10 UTC.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3944, "chunk_char_end": 5310}
{"id": "3b1f9e9e-231a-4dc4-8c90-a8d46f78d251", "url": "https://schema.org/docs/documents.html", "source_domain": "schema.org", "title": "Documentation - schema.org", "section_path": [], "text": "Documentation - schema.org\nNote\n: You are viewing the development\nversion of\nSchema.org\n.\nSee\nhow we work\nfor more details.\nSchema.org\nDocs\nSchemas\nValidate\nAbout\nDocumentation\nSite information\nGetting started\nA simple introduction to microdata and using schema.org for marking up your site.\nFrequently asked questions\nHow we work\nReleases\nFull history of recent releases.\nSchema information\nSchemas\nThe actual schemas, arranged in a hierarchy, with a page for each item in the schema.\nThe full type hierarchy\nThe full type hierarchy, in a single file.\nData model\nA brief note on the data model used, etc.\nStyle guide\nNaming conventions and related patterns for schema authoring.\nDevelopers\nDeveloper-oriented information about schema.org.\nVocabulary definition download\nDownload definition files for core vocabulary and extensions.\nExtension mechanism\nThe extension mechanism that can be used to extend the schemas.\nSchema.org COVID-19 response\nUS CDC Data Table fields.\nOverview of dataset-related vocabulary\nExternal enumerations (blog post)\nHow to use external controlled lists to add detail.\nExtensions\nHealth and medical types\nNotes on the health and medical types under\nMedicalEntity\n.\nHotels\nBackground information and examples on the types and properties for marking up hotels\nand other accommodations using e.g.\nHotel\nand\nHotelRoom\n.\nAutos\nBackground information on marking up automobiles.\nBanks and financial institutions\nBackground information and examples on the types and properties for marking up banks and their products e.g.\nMortgageLoan\nand\nExchangeRateSpecification\n.\nAdditional Resources\nSchema.org blog\nSchema.org Community Group\nSince April 2015, the W3C\nSchema.org Community Group\nis the main forum for schema collaboration, and provides the\npublic-schemaorg@w3.org\nmailing list for discussions. Schema.org issues are tracked\non GitHub\nand coordinated via the\ngroup\n.\nEarlier discussions can be found in W3C's\npublic-vocabs\ngroup. There was also a\ngoogle group", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1983}
{"id": "9ca463a0-8091-42bc-9e50-3d24af5b70f9", "url": "https://schema.org/docs/documents.html", "source_domain": "schema.org", "title": "Documentation - schema.org", "section_path": [], "text": "used during 2011.\nPrior to 2014, the\nW3C Web Schemas group\nhosted a collection of\nproposals\nfor additions and improvements to schema.org.\nSearch\nA\ncustom search\nacross all these discussions and sites is also available.\nGeneral feedback form\nPlease give us feedback, report bugs, etc. For technical feedback,\nGitHub\nis preferred.\nTerms and conditions\nâ¢\nSchema.org\nâ¢\nV29.3\n|\n2025-09-04", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1984, "chunk_char_end": 2371}
{"id": "fabfd70c-d9cc-469e-a255-f75f872cb12f", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "Full schema hierarchy - Schema.org\nNote\n: You are viewing the development\nversion of\nSchema.org\n.\nSee\nhow we work\nfor more details.\nSchema.org\nDocs\nSchemas\nValidate\nAbout\nFull schema hierarchy\nSchema.org is defined as two hierarchies: one for textual property values, and one for the things that they describe.\nThis is the main schema.org hierarchy: a collection of types (or \"classes\"), each of which has one or more parent types. Although a type may have more than one super-type, here we show each type in one branch of the tree only. There is also a parallel hierarchy for\ndata types\n.\nTypes:\nClose hierarchy\n/\nOpen hierarchy\nThing\nAction\nAchieveAction\nLoseAction\nTieAction\nWinAction\nAssessAction\nChooseAction\nVoteAction\nIgnoreAction\nReactAction\nAgreeAction\nDisagreeAction\nDislikeAction\nEndorseAction\nLikeAction\nWantAction\nReviewAction\nConsumeAction\nDrinkAction\nEatAction\nInstallAction\nListenAction\nPlayGameAction\nReadAction\nUseAction\nWearAction\nViewAction\nWatchAction\nControlAction\nActivateAction\nDeactivateAction\nResumeAction\nSuspendAction\nCreateAction\nCookAction\nDrawAction\nFilmAction\nPaintAction\nPhotographAction\nWriteAction\nFindAction\nCheckAction\nDiscoverAction\nTrackAction\nInteractAction\nBefriendAction\nCommunicateAction\nAskAction\nCheckInAction\nCheckOutAction\nCommentAction\nInformAction\nConfirmAction\nRsvpAction\nInviteAction\nReplyAction\nShareAction\nFollowAction\nJoinAction\nLeaveAction\nMarryAction\nRegisterAction\nSubscribeAction\nUnRegisterAction\nMoveAction\nArriveAction\nDepartAction\nTravelAction\nOrganizeAction\nAllocateAction\nAcceptAction\nAssignAction\nAuthorizeAction\nRejectAction\nApplyAction\nBookmarkAction\nPlanAction\nCancelAction\nReserveAction\nScheduleAction\nPlayAction\nExerciseAction\nPerformAction\nSearchAction\nSeekToAction\nSolveMathAction\nTradeAction\nBuyAction\nOrderAction\nPayAction\nPreOrderAction\nQuoteAction\nRentAction\nSellAction\nTipAction\nTransferAction\nBorrowAction\nDonateAction\nDownloadAction\nGiveAction\nLendAction\nMoneyTransfer\nReceiveAction\nReturnAction\nSendAction\nTakeAction", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1995}
{"id": "aac44bfb-622d-4393-8115-8f533aa17493", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "UpdateAction\nAddAction\nInsertAction\nAppendAction\nPrependAction\nDeleteAction\nReplaceAction\nBioChemEntity\nChemicalSubstance\nGene\nMolecularEntity\nProtein\nCreativeWork\nAmpStory\nArchiveComponent\nArticle\nAdvertiserContentArticle\nNewsArticle\nAnalysisNewsArticle\nAskPublicNewsArticle\nBackgroundNewsArticle\nOpinionNewsArticle\nReportageNewsArticle\nReviewNewsArticle\nReport\nSatiricalArticle\nScholarlyArticle\nMedicalScholarlyArticle\nSocialMediaPosting\nBlogPosting\nLiveBlogPosting\nDiscussionForumPosting\nTechArticle\nAPIReference\nAtlas\nBlog\nBook\nAudiobook\nCertification\nChapter\nClaim\nClip\nMovieClip\nRadioClip\nTVClip\nVideoGameClip\nCode\nCollection\nProductCollection\nComicStory\nComicCoverArt\nComment\nAnswer\nCorrectionComment\nQuestion\nConversation\nCourse\nCreativeWorkSeason\nPodcastSeason\nRadioSeason\nTVSeason\nCreativeWorkSeries\nBookSeries\nMovieSeries\nPeriodical\nComicSeries\nNewspaper\nPodcastSeries\nRadioSeries\nTVSeries\nVideoGameSeries\nDataCatalog\nDataset\nDataFeed\nCompleteDataFeed\nDefinedTermSet\nCategoryCodeSet\nDiet\nDigitalDocument\nNoteDigitalDocument\nPresentationDigitalDocument\nSpreadsheetDigitalDocument\nTextDigitalDocument\nDrawing\nEducationalOccupationalCredential\nEpisode\nPodcastEpisode\nRadioEpisode\nTVEpisode\nExercisePlan\nGame\nVideoGame\nGuide\nHowTo\nRecipe\nHowToDirection\nHowToSection\nHowToStep\nHowToTip\nHyperToc\nHyperTocEntry\nLearningResource\nCourse\nQuiz\nSyllabus\nLegislation\nLegislationObject\nManuscript\nMap\nMathSolver\nMediaObject\n3DModel\nAmpStory\nAudioObject\nAudioObjectSnapshot\nAudiobook\nDataDownload\nImageObject\nBarcode\nImageObjectSnapshot\nLegislationObject\nMusicVideoObject\nTextObject\nVideoObject\nVideoObjectSnapshot\nMediaReviewItem\nMenu\nMenuSection\nMessage\nEmailMessage\nMovie\nMusicComposition\nMusicPlaylist\nMusicAlbum\nMusicRelease\nMusicRecording\nPainting\nPhotograph\nPlay\nPoster\nPublicationIssue\nComicIssue\nPublicationVolume\nQuotation\nReview\nClaimReview\nCriticReview\nReviewNewsArticle\nEmployerReview\nMediaReview\nRecommendation\nUserReview\nSculpture\nSeason\nSheetMusic\nShortStory\nSoftwareApplication", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1996, "chunk_char_end": 3986}
{"id": "690b0bc0-7cca-4b78-a3a3-359147b67f23", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "MobileApplication\nVideoGame\nWebApplication\nSoftwareSourceCode\nSpecialAnnouncement\nStatement\nTVSeason\nTVSeries\nThesis\nVisualArtwork\nCoverArt\nComicCoverArt\nWebContent\nHealthTopicContent\nWebPage\nAboutPage\nCheckoutPage\nCollectionPage\nMediaGallery\nImageGallery\nVideoGallery\nContactPage\nFAQPage\nItemPage\nMedicalWebPage\nProfilePage\nQAPage\nRealEstateListing\nSearchResultsPage\nWebPageElement\nSiteNavigationElement\nTable\nWPAdBlock\nWPFooter\nWPHeader\nWPSideBar\nWebSite\nEvent\nBusinessEvent\nChildrensEvent\nComedyEvent\nCourseInstance\nDanceEvent\nDeliveryEvent\nEducationEvent\nEventSeries\nExhibitionEvent\nFestival\nFoodEvent\nHackathon\nLiteraryEvent\nMusicEvent\nPublicationEvent\nBroadcastEvent\nOnDemandEvent\nSaleEvent\nScreeningEvent\nSocialEvent\nSportsEvent\nTheaterEvent\nUserInteraction\nUserBlocks\nUserCheckins\nUserComments\nUserDownloads\nUserLikes\nUserPageVisits\nUserPlays\nUserPlusOnes\nUserTweets\nVisualArtsEvent\nIntangible\nActionAccessSpecification\nAlignmentObject\nAudience\nBusinessAudience\nEducationalAudience\nMedicalAudience\nPatient\nPeopleAudience\nMedicalAudience\nParentAudience\nResearcher\nBedDetails\nBrand\nBroadcastChannel\nRadioChannel\nAMRadioChannel\nFMRadioChannel\nTelevisionChannel\nBroadcastFrequencySpecification\nClass\nComputerLanguage\nConstraintNode\nStatisticalVariable\nDataFeedItem\nDefinedTerm\nCategoryCode\nMedicalCode\nDemand\nDigitalDocumentPermission\nEducationalOccupationalProgram\nWorkBasedProgram\nEnergyConsumptionDetails\nEntryPoint\nEnumeration\nAdultOrientedEnumeration\n::\nAlcoholConsideration\n::\nDangerousGoodConsideration\n::\nHealthcareConsideration\n::\nNarcoticConsideration\n::\nReducedRelevanceForChildrenConsideration\n::\nSexualContentConsideration\n::\nTobaccoNicotineConsideration\n::\nUnclassifiedAdultConsideration\n::\nViolenceConsideration\n::\nWeaponConsideration\nBoardingPolicyType\n::\nGroupBoardingPolicy\n::\nZoneBoardingPolicy\nBookFormatType\n::\nAudiobookFormat\n::\nEBook\n::\nGraphicNovel\n::\nHardcover\n::\nPaperback\nBusinessEntityType\nBusinessFunction\nCarUsageType\n::\nDrivingSchoolVehicleUsage\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3987, "chunk_char_end": 5970}
{"id": "2de6928b-1908-4002-ad8d-61af8c9477f8", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "RentalVehicleUsage\n::\nTaxiVehicleUsage\nCertificationStatusEnumeration\n::\nCertificationActive\n::\nCertificationInactive\nContactPointOption\n::\nHearingImpairedSupported\n::\nTollFree\nDayOfWeek\n::\nFriday\n::\nMonday\n::\nPublicHolidays\n::\nSaturday\n::\nSunday\n::\nThursday\n::\nTuesday\n::\nWednesday\nDeliveryMethod\n::\nLockerDelivery\n::\nOnSitePickup\n::\nParcelService\nDigitalDocumentPermissionType\n::\nCommentPermission\n::\nReadPermission\n::\nWritePermission\nDigitalPlatformEnumeration\n::\nAndroidPlatform\n::\nDesktopWebPlatform\n::\nGenericWebPlatform\n::\nIOSPlatform\n::\nMobileWebPlatform\nEnergyEfficiencyEnumeration\nEUEnergyEfficiencyEnumeration\n::\nEUEnergyEfficiencyCategoryA\n::\nEUEnergyEfficiencyCategoryA1Plus\n::\nEUEnergyEfficiencyCategoryA2Plus\n::\nEUEnergyEfficiencyCategoryA3Plus\n::\nEUEnergyEfficiencyCategoryB\n::\nEUEnergyEfficiencyCategoryC\n::\nEUEnergyEfficiencyCategoryD\n::\nEUEnergyEfficiencyCategoryE\n::\nEUEnergyEfficiencyCategoryF\n::\nEUEnergyEfficiencyCategoryG\nEnergyStarEnergyEfficiencyEnumeration\n::\nEnergyStarCertified\nEventAttendanceModeEnumeration\n::\nMixedEventAttendanceMode\n::\nOfflineEventAttendanceMode\n::\nOnlineEventAttendanceMode\nFulfillmentTypeEnumeration\n::\nFulfillmentTypeCollectionPoint\n::\nFulfillmentTypeDelivery\n::\nFulfillmentTypePickupDropoff\n::\nFulfillmentTypePickupInStore\n::\nFulfillmentTypeScheduledDelivery\nGameAvailabilityEnumeration\n::\nDemoGameAvailability\n::\nFullGameAvailability\nGamePlayMode\n::\nCoOp\n::\nMultiPlayer\n::\nSinglePlayer\nGenderType\n::\nFemale\n::\nMale\nGovernmentBenefitsType\n::\nBasicIncome\n::\nBusinessSupport\n::\nDisabilitySupport\n::\nHealthCare\n::\nOneTimePayments\n::\nPaidLeave\n::\nParentalSupport\n::\nUnemploymentSupport\nHealthAspectEnumeration\n::\nAllergiesHealthAspect\n::\nBenefitsHealthAspect\n::\nCausesHealthAspect\n::\nContagiousnessHealthAspect\n::\nEffectivenessHealthAspect\n::\nGettingAccessHealthAspect\n::\nHowItWorksHealthAspect\n::\nHowOrWhereHealthAspect\n::\nIngredientsHealthAspect\n::\nLivingWithHealthAspect\n::\nMayTreatHealthAspect\n::\nMisconceptionsHealthAspect\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5971, "chunk_char_end": 7951}
{"id": "016a8721-34a1-4a13-97fa-db2c3c5cceb2", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "OverviewHealthAspect\n::\nPatientExperienceHealthAspect\n::\nPregnancyHealthAspect\n::\nPreventionHealthAspect\n::\nPrognosisHealthAspect\n::\nRelatedTopicsHealthAspect\n::\nRisksOrComplicationsHealthAspect\n::\nSafetyHealthAspect\n::\nScreeningHealthAspect\n::\nSeeDoctorHealthAspect\n::\nSelfCareHealthAspect\n::\nSideEffectsHealthAspect\n::\nStagesHealthAspect\n::\nSymptomsHealthAspect\n::\nTreatmentsHealthAspect\n::\nTypesHealthAspect\n::\nUsageOrScheduleHealthAspect\nIncentiveQualifiedExpenseType\n::\nIncentiveQualifiedExpenseTypeGoodsOnly\n::\nIncentiveQualifiedExpenseTypeGoodsOrServices\n::\nIncentiveQualifiedExpenseTypeServicesOnly\n::\nIncentiveQualifiedExpenseTypeUtilityBill\nIncentiveStatus\n::\nIncentiveStatusActive\n::\nIncentiveStatusInDevelopment\n::\nIncentiveStatusOnHold\n::\nIncentiveStatusRetired\nIncentiveType\n::\nIncentiveTypeLoan\n::\nIncentiveTypeRebateOrSubsidy\n::\nIncentiveTypeTaxCredit\n::\nIncentiveTypeTaxDeduction\n::\nIncentiveTypeTaxWaiver\nItemAvailability\n::\nBackOrder\n::\nDiscontinued\n::\nInStock\n::\nInStoreOnly\n::\nLimitedAvailability\n::\nMadeToOrder\n::\nOnlineOnly\n::\nOutOfStock\n::\nPreOrder\n::\nPreSale\n::\nReserved\n::\nSoldOut\nItemListOrderType\n::\nItemListOrderAscending\n::\nItemListOrderDescending\n::\nItemListUnordered\nLegalValueLevel\n::\nAuthoritativeLegalValue\n::\nDefinitiveLegalValue\n::\nOfficialLegalValue\n::\nUnofficialLegalValue\nMapCategoryType\n::\nParkingMap\n::\nSeatingMap\n::\nTransitMap\n::\nVenueMap\nMeasurementMethodEnum\n::\nExampleMeasurementMethodEnum\nMeasurementTypeEnumeration\nBodyMeasurementTypeEnumeration\n::\nBodyMeasurementArm\n::\nBodyMeasurementBust\n::\nBodyMeasurementChest\n::\nBodyMeasurementFoot\n::\nBodyMeasurementHand\n::\nBodyMeasurementHead\n::\nBodyMeasurementHeight\n::\nBodyMeasurementHips\n::\nBodyMeasurementInsideLeg\n::\nBodyMeasurementNeck\n::\nBodyMeasurementUnderbust\n::\nBodyMeasurementWaist\n::\nBodyMeasurementWeight\nWearableMeasurementTypeEnumeration\n::\nWearableMeasurementBack\n::\nWearableMeasurementChestOrBust\n::\nWearableMeasurementCollar\n::\nWearableMeasurementCup\n::\nWearableMeasurementHeight\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7952, "chunk_char_end": 9942}
{"id": "e7aeef4d-8e53-4e47-879c-88ed7ccb8b89", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "WearableMeasurementHips\n::\nWearableMeasurementInseam\n::\nWearableMeasurementLength\n::\nWearableMeasurementOutsideLeg\n::\nWearableMeasurementSleeve\n::\nWearableMeasurementWaist\n::\nWearableMeasurementWidth\nMediaEnumeration\nIPTCDigitalSourceEnumeration\n::\nAlgorithmicMediaDigitalSource\n::\nAlgorithmicallyEnhancedDigitalSource\n::\nCompositeCaptureDigitalSource\n::\nCompositeDigitalSource\n::\nCompositeSyntheticDigitalSource\n::\nCompositeWithTrainedAlgorithmicMediaDigitalSource\n::\nDataDrivenMediaDigitalSource\n::\nDigitalArtDigitalSource\n::\nDigitalCaptureDigitalSource\n::\nMinorHumanEditsDigitalSource\n::\nMultiFrameComputationalCaptureDigitalSource\n::\nNegativeFilmDigitalSource\n::\nPositiveFilmDigitalSource\n::\nPrintDigitalSource\n::\nScreenCaptureDigitalSource\n::\nTrainedAlgorithmicMediaDigitalSource\n::\nVirtualRecordingDigitalSource\nMediaManipulationRatingEnumeration\n::\nDecontextualizedContent\n::\nEditedOrCroppedContent\n::\nOriginalMediaContent\n::\nSatireOrParodyContent\n::\nStagedContent\n::\nTransformedContent\nMedicalEnumeration\nDrugCostCategory\n::\nReimbursementCap\n::\nRetail\n::\nWholesale\nDrugPregnancyCategory\n::\nFDAcategoryA\n::\nFDAcategoryB\n::\nFDAcategoryC\n::\nFDAcategoryD\n::\nFDAcategoryX\n::\nFDAnotEvaluated\nDrugPrescriptionStatus\n::\nOTC\n::\nPrescriptionOnly\nInfectiousAgentClass\n::\nBacteria\n::\nFungus\n::\nMulticellularParasite\n::\nPrion\n::\nProtozoa\n::\nVirus\nMedicalAudienceType\n::\nClinician\n::\nMedicalResearcher\nMedicalDevicePurpose\n::\nDiagnostic\n::\nTherapeutic\nMedicalEvidenceLevel\n::\nEvidenceLevelA\n::\nEvidenceLevelB\n::\nEvidenceLevelC\nMedicalImagingTechnique\n::\nCT\n::\nMRI\n::\nPET\n::\nRadiography\n::\nUltrasound\n::\nXRay\nMedicalObservationalStudyDesign\n::\nCaseSeries\n::\nCohortStudy\n::\nCrossSectional\n::\nLongitudinal\n::\nObservational\n::\nRegistry\nMedicalProcedureType\n::\nNoninvasiveProcedure\n::\nPercutaneousProcedure\nMedicalSpecialty\n::\nAnesthesia\n::\nCardiovascular\n::\nCommunityHealth\n::\nDentistry\n::\nDermatologic\n::\nDermatology\n::\nDietNutrition\n::\nEmergency\n::\nEndocrine\n::\nGastroenterologic\n::\nGenetic\n::\nGeriatric\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9943, "chunk_char_end": 11941}
{"id": "f819c089-3b58-40f5-b3f8-3ee8fcf70ecc", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "Gynecologic\n::\nHematologic\n::\nInfectious\n::\nLaboratoryScience\n::\nMidwifery\n::\nMusculoskeletal\n::\nNeurologic\n::\nNursing\n::\nObstetric\n::\nOncologic\n::\nOptometric\n::\nOtolaryngologic\n::\nPathology\n::\nPediatric\n::\nPharmacySpecialty\n::\nPhysiotherapy\n::\nPlasticSurgery\n::\nPodiatric\n::\nPrimaryCare\n::\nPsychiatric\n::\nPublicHealth\n::\nPulmonary\n::\nRadiography\n::\nRenal\n::\nRespiratoryTherapy\n::\nRheumatologic\n::\nSpeechPathology\n::\nSurgical\n::\nToxicologic\n::\nUrologic\nMedicalStudyStatus\n::\nActiveNotRecruiting\n::\nCompleted\n::\nEnrollingByInvitation\n::\nNotYetRecruiting\n::\nRecruiting\n::\nResultsAvailable\n::\nResultsNotAvailable\n::\nSuspended\n::\nTerminated\n::\nWithdrawn\nMedicalTrialDesign\n::\nDoubleBlindedTrial\n::\nInternationalTrial\n::\nMultiCenterTrial\n::\nOpenTrial\n::\nPlaceboControlledTrial\n::\nRandomizedTrial\n::\nSingleBlindedTrial\n::\nSingleCenterTrial\n::\nTripleBlindedTrial\nMedicineSystem\n::\nAyurvedic\n::\nChiropractic\n::\nHomeopathic\n::\nOsteopathic\n::\nTraditionalChinese\n::\nWesternConventional\nPhysicalExam\n::\nAbdomen\n::\nAppearance\n::\nCardiovascularExam\n::\nEar\n::\nEye\n::\nGenitourinary\n::\nHead\n::\nLung\n::\nMusculoskeletalExam\n::\nNeck\n::\nNeuro\n::\nNose\n::\nSkin\n::\nThroat\nMerchantReturnEnumeration\n::\nMerchantReturnFiniteReturnWindow\n::\nMerchantReturnNotPermitted\n::\nMerchantReturnUnlimitedWindow\n::\nMerchantReturnUnspecified\nMusicAlbumProductionType\n::\nCompilationAlbum\n::\nDJMixAlbum\n::\nDemoAlbum\n::\nLiveAlbum\n::\nMixtapeAlbum\n::\nRemixAlbum\n::\nSoundtrackAlbum\n::\nSpokenWordAlbum\n::\nStudioAlbum\nMusicAlbumReleaseType\n::\nAlbumRelease\n::\nBroadcastRelease\n::\nEPRelease\n::\nSingleRelease\nMusicReleaseFormatType\n::\nCDFormat\n::\nCassetteFormat\n::\nDVDFormat\n::\nDigitalAudioTapeFormat\n::\nDigitalFormat\n::\nLaserDiscFormat\n::\nVinylFormat\nNonprofitType\nNLNonprofitType\n::\nNonprofitANBI\n::\nNonprofitSBBI\nUKNonprofitType\n::\nCharitableIncorporatedOrganization\n::\nLimitedByGuaranteeCharity\n::\nUKTrust\n::\nUnincorporatedAssociationCharity\nUSNonprofitType\n::\nNonprofit501a\n::\nNonprofit501c1\n::\nNonprofit501c10\n::\nNonprofit501c11\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11942, "chunk_char_end": 13928}
{"id": "056323af-614f-46e6-be75-2642e2011e41", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "Nonprofit501c12\n::\nNonprofit501c13\n::\nNonprofit501c14\n::\nNonprofit501c15\n::\nNonprofit501c16\n::\nNonprofit501c17\n::\nNonprofit501c18\n::\nNonprofit501c19\n::\nNonprofit501c2\n::\nNonprofit501c20\n::\nNonprofit501c21\n::\nNonprofit501c22\n::\nNonprofit501c23\n::\nNonprofit501c24\n::\nNonprofit501c25\n::\nNonprofit501c26\n::\nNonprofit501c27\n::\nNonprofit501c28\n::\nNonprofit501c3\n::\nNonprofit501c4\n::\nNonprofit501c5\n::\nNonprofit501c6\n::\nNonprofit501c7\n::\nNonprofit501c8\n::\nNonprofit501c9\n::\nNonprofit501d\n::\nNonprofit501e\n::\nNonprofit501f\n::\nNonprofit501k\n::\nNonprofit501n\n::\nNonprofit501q\n::\nNonprofit527\nOfferItemCondition\n::\nDamagedCondition\n::\nNewCondition\n::\nRefurbishedCondition\n::\nUsedCondition\nPaymentMethodType\n::\nByBankTransferInAdvance\n::\nByInvoice\n::\nCOD\n::\nCash\n::\nCheckInAdvance\n::\nDirectDebit\n::\nInStorePrepay\n::\nPhoneCarrierPayment\nPhysicalActivityCategory\n::\nAerobicActivity\n::\nAnaerobicActivity\n::\nBalance\n::\nFlexibility\n::\nLeisureTimeActivity\n::\nOccupationalActivity\n::\nStrengthTraining\nPriceComponentTypeEnumeration\n::\nActivationFee\n::\nCleaningFee\n::\nDistanceFee\n::\nDownpayment\n::\nInstallment\n::\nSubscription\nPriceTypeEnumeration\n::\nInvoicePrice\n::\nListPrice\n::\nMSRP\n::\nMinimumAdvertisedPrice\n::\nRegularPrice\n::\nSRP\n::\nSalePrice\n::\nStrikethroughPrice\nPurchaseType\n::\nPurchaseTypeLease\n::\nPurchaseTypeNewPurchase\n::\nPurchaseTypeTradeIn\n::\nPurchaseTypeUsedPurchase\nQualitativeValue\nBedType\nDriveWheelConfigurationValue\n::\nAllWheelDriveConfiguration\n::\nFourWheelDriveConfiguration\n::\nFrontWheelDriveConfiguration\n::\nRearWheelDriveConfiguration\nSizeSpecification\nSteeringPositionValue\n::\nLeftHandDriving\n::\nRightHandDriving\nRefundTypeEnumeration\n::\nExchangeRefund\n::\nFullRefund\n::\nStoreCreditRefund\nRestrictedDiet\n::\nDiabeticDiet\n::\nGlutenFreeDiet\n::\nHalalDiet\n::\nHinduDiet\n::\nKosherDiet\n::\nLowCalorieDiet\n::\nLowFatDiet\n::\nLowLactoseDiet\n::\nLowSaltDiet\n::\nVeganDiet\n::\nVegetarianDiet\nReturnFeesEnumeration\n::\nFreeReturn\n::\nOriginalShippingFees\n::\nRestockingFees\n::\nReturnFeesCustomerResponsibility\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13929, "chunk_char_end": 15921}
{"id": "f62174a1-c09a-44bb-88f0-670b592eca00", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "ReturnShippingFees\nReturnLabelSourceEnumeration\n::\nReturnLabelCustomerResponsibility\n::\nReturnLabelDownloadAndPrint\n::\nReturnLabelInBox\nReturnMethodEnumeration\n::\nKeepProduct\n::\nReturnAtKiosk\n::\nReturnByMail\n::\nReturnInStore\nRsvpResponseType\n::\nRsvpResponseMaybe\n::\nRsvpResponseNo\n::\nRsvpResponseYes\nSizeGroupEnumeration\nWearableSizeGroupEnumeration\n::\nWearableSizeGroupBig\n::\nWearableSizeGroupBoys\n::\nWearableSizeGroupExtraShort\n::\nWearableSizeGroupExtraTall\n::\nWearableSizeGroupGirls\n::\nWearableSizeGroupHusky\n::\nWearableSizeGroupInfants\n::\nWearableSizeGroupJuniors\n::\nWearableSizeGroupMaternity\n::\nWearableSizeGroupMens\n::\nWearableSizeGroupMisses\n::\nWearableSizeGroupPetite\n::\nWearableSizeGroupPlus\n::\nWearableSizeGroupRegular\n::\nWearableSizeGroupShort\n::\nWearableSizeGroupTall\n::\nWearableSizeGroupWomens\nSizeSystemEnumeration\n::\nSizeSystemImperial\n::\nSizeSystemMetric\nWearableSizeSystemEnumeration\n::\nWearableSizeSystemAU\n::\nWearableSizeSystemBR\n::\nWearableSizeSystemCN\n::\nWearableSizeSystemContinental\n::\nWearableSizeSystemDE\n::\nWearableSizeSystemEN13402\n::\nWearableSizeSystemEurope\n::\nWearableSizeSystemFR\n::\nWearableSizeSystemGS1\n::\nWearableSizeSystemIT\n::\nWearableSizeSystemJP\n::\nWearableSizeSystemMX\n::\nWearableSizeSystemUK\n::\nWearableSizeSystemUS\nSpecialty\nMedicalSpecialty\nStatusEnumeration\nActionStatusType\n::\nActiveActionStatus\n::\nCompletedActionStatus\n::\nFailedActionStatus\n::\nPotentialActionStatus\nEventStatusType\n::\nEventCancelled\n::\nEventMovedOnline\n::\nEventPostponed\n::\nEventRescheduled\n::\nEventScheduled\nGameServerStatus\n::\nOfflinePermanently\n::\nOfflineTemporarily\n::\nOnline\n::\nOnlineFull\nLegalForceStatus\n::\nInForce\n::\nNotInForce\n::\nPartiallyInForce\nOrderStatus\n::\nOrderCancelled\n::\nOrderDelivered\n::\nOrderInTransit\n::\nOrderPaymentDue\n::\nOrderPickupAvailable\n::\nOrderProblem\n::\nOrderProcessing\n::\nOrderReturned\nPaymentStatusType\n::\nPaymentAutomaticallyApplied\n::\nPaymentComplete\n::\nPaymentDeclined\n::\nPaymentDue\n::\nPaymentPastDue\nReservationStatusType\n::\nReservationCancelled\n::", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15922, "chunk_char_end": 17920}
{"id": "150073e0-26a8-4ec8-9d3c-f607fe3b2a07", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "ReservationConfirmed\n::\nReservationHold\n::\nReservationPending\nTierBenefitEnumeration\n::\nTierBenefitLoyaltyPoints\n::\nTierBenefitLoyaltyPrice\n::\nTierBenefitLoyaltyReturns\n::\nTierBenefitLoyaltyShipping\nWarrantyScope\nFinancialIncentive\nFloorPlan\nGameServer\nGeospatialGeometry\nGrant\nMonetaryGrant\nHealthInsurancePlan\nHealthPlanCostSharingSpecification\nHealthPlanFormulary\nHealthPlanNetwork\nInvoice\nItemList\nBreadcrumbList\nHowToSection\nHowToStep\nOfferCatalog\nJobPosting\nLanguage\nListItem\nHowToDirection\nHowToItem\nHowToSupply\nHowToTool\nHowToSection\nHowToStep\nHowToTip\nMediaSubscription\nMemberProgram\nMemberProgramTier\nMenuItem\nMerchantReturnPolicy\nMerchantReturnPolicySeasonalOverride\nObservation\nOccupation\nOccupationalExperienceRequirements\nOffer\nAggregateOffer\nOfferForLease\nOfferForPurchase\nOrder\nOrderItem\nParcelDelivery\nPaymentMethod\nPaymentCard\nCreditCard\nPaymentService\nPermit\nGovernmentPermit\nProgramMembership\nProperty\nPropertyValueSpecification\nQuantity\nDistance\nDuration\nEnergy\nMass\nRating\nAggregateRating\nEmployerAggregateRating\nEndorsementRating\nReservation\nBoatReservation\nBusReservation\nEventReservation\nFlightReservation\nFoodEstablishmentReservation\nLodgingReservation\nRentalCarReservation\nReservationPackage\nTaxiReservation\nTrainReservation\nRole\nLinkRole\nOrganizationRole\nEmployeeRole\nPerformanceRole\nSchedule\nSeat\nSeries\nCreativeWorkSeries\nEventSeries\nService\nBroadcastService\nRadioBroadcastService\nCableOrSatelliteService\nFinancialProduct\nBankAccount\nDepositAccount\nCurrencyConversionService\nInvestmentOrDeposit\nBrokerageAccount\nDepositAccount\nInvestmentFund\nLoanOrCredit\nCreditCard\nMortgageLoan\nPaymentCard\nPaymentService\nFoodService\nGovernmentService\nTaxi\nTaxiService\nWebAPI\nServiceChannel\nSpeakableSpecification\nStatisticalPopulation\nStructuredValue\nCDCPMDRecord\nContactPoint\nPostalAddress\nDatedMoneySpecification\nDefinedRegion\nEngineSpecification\nExchangeRateSpecification\nGeoCoordinates\nGeoShape\nGeoCircle\nInteractionCounter\nMonetaryAmount\nNutritionInformation\nOfferShippingDetails", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17921, "chunk_char_end": 19920}
{"id": "754be986-6afc-4a65-bf74-af1412b5f0ab", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "OpeningHoursSpecification\nOwnershipInfo\nPostalCodeRangeSpecification\nPriceSpecification\nCompoundPriceSpecification\nDeliveryChargeSpecification\nPaymentChargeSpecification\nUnitPriceSpecification\nPropertyValue\nLocationFeatureSpecification\nQuantitativeValue\nObservation\nQuantitativeValueDistribution\nMonetaryAmountDistribution\nRepaymentSpecification\nServicePeriod\nShippingConditions\nShippingDeliveryTime\nShippingRateSettings\nShippingService\nTypeAndQuantityNode\nWarrantyPromise\nTicket\nTrip\nBoatTrip\nBusTrip\nFlight\nTouristTrip\nTrainTrip\nVirtualLocation\nMedicalEntity\nAnatomicalStructure\nBone\nBrainStructure\nJoint\nLigament\nMuscle\nNerve\nVessel\nArtery\nLymphaticVessel\nVein\nAnatomicalSystem\nDrugClass\nDrugCost\nLifestyleModification\nDiet\nPhysicalActivity\nExercisePlan\nMedicalCause\nMedicalCondition\nInfectiousDisease\nMedicalSignOrSymptom\nMedicalSign\nVitalSign\nMedicalSymptom\nMedicalContraindication\nMedicalDevice\nMedicalGuideline\nMedicalGuidelineContraindication\nMedicalGuidelineRecommendation\nMedicalIndication\nApprovedIndication\nPreventionIndication\nTreatmentIndication\nMedicalIntangible\nDDxElement\nDoseSchedule\nMaximumDoseSchedule\nRecommendedDoseSchedule\nReportedDoseSchedule\nDrugLegalStatus\nDrugStrength\nMedicalCode\nMedicalConditionStage\nMedicalProcedure\nDiagnosticProcedure\nPalliativeProcedure\nPhysicalExam\nSurgicalProcedure\nTherapeuticProcedure\nMedicalTherapy\nOccupationalTherapy\nPalliativeProcedure\nPhysicalTherapy\nRadiationTherapy\n::\nRespiratoryTherapy\nPsychologicalTreatment\nMedicalRiskEstimator\nMedicalRiskCalculator\nMedicalRiskScore\nMedicalRiskFactor\nMedicalStudy\nMedicalObservationalStudy\nMedicalTrial\nMedicalTest\nBloodTest\nImagingTest\nMedicalTestPanel\nPathologyTest\nSubstance\nDietarySupplement\nDrug\nSuperficialAnatomy\nOrganization\nAirline\nConsortium\nCooperative\nCorporation\nEducationalOrganization\nCollegeOrUniversity\nElementarySchool\nHighSchool\nMiddleSchool\nPreschool\nSchool\nFundingScheme\nGovernmentOrganization\nLibrarySystem\nLocalBusiness\nAnimalShelter\nArchiveOrganization\nAutomotiveBusiness", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19921, "chunk_char_end": 21915}
{"id": "6473aae1-38e1-4979-bc83-46e5039933d1", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "AutoBodyShop\nAutoDealer\nAutoPartsStore\nAutoRental\nAutoRepair\nAutoWash\nGasStation\nMotorcycleDealer\nMotorcycleRepair\nChildCare\nDentist\nDryCleaningOrLaundry\nEmergencyService\nFireStation\nHospital\nPoliceStation\nEmploymentAgency\nEntertainmentBusiness\nAdultEntertainment\nAmusementPark\nArtGallery\nCasino\nComedyClub\nMovieTheater\nNightClub\nFinancialService\nAccountingService\nAutomatedTeller\nBankOrCreditUnion\nInsuranceAgency\nFoodEstablishment\nBakery\nBarOrPub\nBrewery\nCafeOrCoffeeShop\nDistillery\nFastFoodRestaurant\nIceCreamShop\nRestaurant\nWinery\nGovernmentOffice\nPostOffice\nHealthAndBeautyBusiness\nBeautySalon\nDaySpa\nHairSalon\nHealthClub\nNailSalon\nTattooParlor\nHomeAndConstructionBusiness\nElectrician\nGeneralContractor\nHVACBusiness\nHousePainter\nLocksmith\nMovingCompany\nPlumber\nRoofingContractor\nInternetCafe\nLegalService\nAttorney\nNotary\nLibrary\nLodgingBusiness\nBedAndBreakfast\nCampground\nHostel\nHotel\nMotel\nResort\nSkiResort\nVacationRental\nMedicalBusiness\n::\nCommunityHealth\nDentist\n::\nDermatology\n::\nDietNutrition\n::\nEmergency\n::\nGeriatric\n::\nGynecologic\nMedicalClinic\nCovidTestingFacility\n::\nMidwifery\n::\nNursing\n::\nObstetric\n::\nOncologic\nOptician\n::\nOptometric\n::\nOtolaryngologic\n::\nPediatric\nPharmacy\nPhysician\nIndividualPhysician\nPhysiciansOffice\n::\nPhysiotherapy\n::\nPlasticSurgery\n::\nPodiatric\n::\nPrimaryCare\n::\nPsychiatric\n::\nPublicHealth\nProfessionalService\nRadioStation\nRealEstateAgent\nRecyclingCenter\nSelfStorage\nShoppingCenter\nSportsActivityLocation\nBowlingAlley\nExerciseGym\nGolfCourse\nHealthClub\nPublicSwimmingPool\nSkiResort\nSportsClub\nStadiumOrArena\nTennisComplex\nStore\nAutoPartsStore\nBikeStore\nBookStore\nClothingStore\nComputerStore\nConvenienceStore\nDepartmentStore\nElectronicsStore\nFlorist\nFurnitureStore\nGardenStore\nGroceryStore\nHardwareStore\nHobbyShop\nHomeGoodsStore\nJewelryStore\nLiquorStore\nMensClothingStore\nMobilePhoneStore\nMovieRentalStore\nMusicStore\nOfficeEquipmentStore\nOutletStore\nPawnShop\nPetStore\nShoeStore\nSportingGoodsStore\nTireShop\nToyStore\nWholesaleStore\nTelevisionStation", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 11, "chunk_char_start": 21916, "chunk_char_end": 23905}
{"id": "0d639ca4-01ed-4b3f-91f1-4cfe85dfcabc", "url": "https://schema.org/docs/full.html", "source_domain": "schema.org", "title": "Full schema hierarchy - Schema.org", "section_path": [], "text": "TouristInformationCenter\nTravelAgency\nMedicalOrganization\nDentist\nDiagnosticLab\nHospital\nMedicalClinic\nPharmacy\nPhysician\nVeterinaryCare\nNGO\nNewsMediaOrganization\nOnlineBusiness\nOnlineStore\nOnlineMarketplace\nPerformingGroup\nDanceGroup\nMusicGroup\nTheaterGroup\nPoliticalParty\nProject\nFundingAgency\nResearchProject\nResearchOrganization\nSearchRescueOrganization\nSportsOrganization\nSportsTeam\nWorkersUnion\nPerson\nPatient\nPlace\nAccommodation\nApartment\nCampingPitch\nHouse\nSingleFamilyResidence\nRoom\nHotelRoom\nMeetingRoom\nSuite\nAdministrativeArea\nCity\nCountry\nSchoolDistrict\nState\nCivicStructure\nAirport\nAquarium\nBeach\nBoatTerminal\nBridge\nBusStation\nBusStop\nCampground\nCemetery\nCrematorium\nEducationalOrganization\nEventVenue\nFireStation\nGovernmentBuilding\nCityHall\nCourthouse\nDefenceEstablishment\nEmbassy\nLegislativeBuilding\nHospital\nMovieTheater\nMuseum\nMusicVenue\nPark\nParkingFacility\nPerformingArtsTheater\nPlaceOfWorship\nBuddhistTemple\nChurch\nCatholicChurch\nHinduTemple\nMosque\nSynagogue\nPlayground\nPoliceStation\nPublicToilet\nRVPark\nStadiumOrArena\nSubwayStation\nTaxiStand\nTrainStation\nZoo\nLandform\nBodyOfWater\nCanal\nLakeBodyOfWater\nOceanBodyOfWater\nPond\nReservoir\nRiverBodyOfWater\nSeaBodyOfWater\nWaterfall\nContinent\nMountain\nVolcano\nLandmarksOrHistoricalBuildings\nLocalBusiness\nResidence\nApartmentComplex\nGatedResidenceCommunity\nTouristAttraction\nTouristDestination\nProduct\nDietarySupplement\nDrug\nIndividualProduct\nProductCollection\nProductGroup\nProductModel\nSomeProducts\nVehicle\nBusOrCoach\nCar\nMotorcycle\nMotorizedBicycle\nTaxon\nDataTypes:\nClose hierarchy\n/\nOpen hierarchy\nDataType\nBoolean\nFalse\nTrue\nDate\nDateTime\nNumber\nFloat\nInteger\nText\nCssSelectorType\nPronounceableText\nURL\nXPathType\nTime\nTerms and conditions\nâ¢\nSchema.org\nâ¢\nV29.3\n|\n2025-09-04", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 12, "chunk_char_start": 23906, "chunk_char_end": 25651}
{"id": "d6306a72-d0ef-4d16-8655-ba0bee383b1a", "url": "https://developers.google.com/search/docs/appearance/page-experience", "source_domain": "developers.google.com", "title": "Understanding Google Page Experience | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Understanding Google Page Experience | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nUnderstanding page experience in Google Search results\nGoogle's core ranking systems look to reward content that provides a good page experience. Site owners seeking to be\nsuccessful with our systems should not focus on only one or two aspects of page experience. Instead, check if you're\nproviding an overall great page experience across many aspects.\nSelf-assess your content's page experience\nAnswering yes to the following questions means you're probably on track in providing a good page experience:\nDo your pages have good Core Web Vitals?\nAre your pages served in a secure fashion?\nDoes your content display well on mobile devices?\nDoes your content avoid using an excessive amount of ads that distract from or interfere with the\nmain content?\nDo your pages avoid using intrusive interstitials?\nIs your page designed so visitors can easily distinguish the main content from other\ncontent on your page?\nThese questions don't encompass all page experience aspects to consider.\nHowever, questions like these, and consulting the following resources, may help you align\nwith providing an overall good page experience.\nPage experience resources\nHere are some resources that can help you measure, monitor, and optimize your page experience:\nUnderstanding Core Web Vitals and Google Search results\n:\nLearn more about Core Web Vitals and how they work in Google Search results.\nSearch Console's HTTPS report\n:\nCheck if you're serving secure HTTPS pages and what to fix, if you're not.\nCheck if a site's", "engine": "google", "topic": "page_experience", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 2000}
{"id": "1848fa79-4851-4dab-b512-43bb9b1e6877", "url": "https://developers.google.com/search/docs/appearance/page-experience", "source_domain": "developers.google.com", "title": "Understanding Google Page Experience | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "connection is secure\n: Learn how to\ncheck if your site's connection is secure, as reported by Chrome.\nIf the page isn't served over HTTPS, learn how to\nsecure your site with HTTPS\n.\nAvoid intrusive\ninterstitials and dialog\n: Learn how to avoid interstitials that can make content less accessible.\nChrome Lighthouse:\nThis toolset from Chrome can\nhelp you identify a range of improvements to make related to page experience, including mobile usability.\nFAQ\nIs there a single \"page experience signal\" that Google Search uses for ranking?\nThere is no single signal. Our core ranking systems look at a variety of signals that align with overall page\nexperience.\nWhat aspects of page experience are used in ranking?\nCore Web Vitals are used by our ranking systems.\nWe recommend site owners achieve good Core Web Vitals for success with Search and to ensure a great user experience generally.\nKeep in mind that getting good results in reports like Search Console's\nCore Web Vitals report\nor third-party tools doesn't guarantee that your pages will rank at the top of Google Search results;\nthere's more to great page experience than Core Web Vitals scores alone.\nThese scores are meant to help you to improve your site for your users overall,\nand trying to get a perfect score just for SEO reasons may not be the best use of your time.\nBeyond Core Web Vitals, other page experience aspects don't directly help your website rank higher in search results.\nHowever, they can make your website more satisfying to use, which is generally aligned with what our ranking systems seek to reward.\nTherefore it's still worth working to improve page experience overall.\nIs page experience evaluated on a site-wide or page-specific basis?\nOur core ranking systems generally evaluate content on a page-specific basis, including when\nunderstanding aspects related to page experience. However, we do have some site-wide assessments.\nHow important is page experience to ranking success?", "engine": "google", "topic": "page_experience", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 2001, "chunk_char_end": 3963}
{"id": "ccee8871-19f6-4cba-b438-e8b67a37d61c", "url": "https://developers.google.com/search/docs/appearance/page-experience", "source_domain": "developers.google.com", "title": "Understanding Google Page Experience | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Google Search always seeks to show the most relevant content, even if the page experience is sub-par. But for many\nqueries, there is lots of helpful content available. Having a great page experience can contribute to success in\nSearch, in such cases.\nRecent updates on our blog\nHere's everything we've announced about page experience on the\nGoogle Search Central blog\n:\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-02-04 UTC.", "engine": "google", "topic": "page_experience", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3964, "chunk_char_end": 4673}
{"id": "c077bf95-c188-47c7-8bf9-f1172503dec5", "url": "https://developers.google.com/search/docs/appearance/core-web-vitals", "source_domain": "developers.google.com", "title": "Understanding Core Web Vitals and Google search results | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Understanding Core Web Vitals and Google search results | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nUnderstanding Core Web Vitals and Google search results\nCore Web Vitals\nis a set of metrics that measure real-world\nuser experience for loading performance, interactivity, and visual stability of the page. We highly recommend site\nowners achieve good Core Web Vitals for success with Search and to ensure a great user experience generally. This,\nalong with other page experience aspects, aligns with what our core ranking systems seek to reward. Learn more in\nUnderstanding page\nexperience in Google Search results\n.\nCore Web Vitals metrics\nLargest Contentful Paint (LCP)\n: Measures\nloading performance. To provide a good user experience, strive to have\nLCP occur within\nthe first 2.5 seconds\nof the page starting to load.\nInteraction To Next Paint (INP)\n:\nMeasures responsiveness. To provide a good user experience, strive to have\nan\nINP of less than 200 milliseconds\n.\nCumulative Layout Shift (CLS)\n: Measures\nvisual stability. To provide a good user experience, strive to have a\nCLS score of\nless than 0.1\n.\nOptimizing your Core Web Vitals\nHere are some resources that can help you measure, monitor, and optimize your Core Web Vitals:\nCheck the\nCore Web\nVitals report in Search Console\n. This shows how your pages perform.\nLearn more about\nCore\nWeb Vitals\n, a guide about Core Web Vitals, including\nhow to measure, debug, improve and best practices.\nLearn about the different tools that can help you\nmeasure\nand report Core Web Vitals\n. These tools measure LCP, INP, and CLS.", "engine": "google", "topic": "page_experience", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1997}
{"id": "cc2f8574-6d0a-41ed-abc7-4dc6b2ce9681", "url": "https://developers.google.com/search/docs/appearance/core-web-vitals", "source_domain": "developers.google.com", "title": "Understanding Core Web Vitals and Google search results | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Recent updates on our blog\nHere's everything we've announced about Core Web Vitals on the\nGoogle Search Central blog\n:\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-02-04 UTC.", "engine": "google", "topic": "page_experience", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1998, "chunk_char_end": 2456}
{"id": "ef4fe41e-2989-4627-81c0-37e51343f7c4", "url": "https://web.dev/explore/learn-core-web-vitals", "source_domain": "web.dev", "title": "Core Web Vitals  |  web.dev", "section_path": [], "text": "Core Web Vitals  |  web.dev\nSkip to main content\n/\nEnglish\nDeutsch\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nעברית\nالعربيّة\nفارسی\nहिंदी\nবাংলা\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nweb.dev\nStay organized with collections\nSave and categorize content based on your preferences.\nCore Web Vitals\nEssential metrics for a healthy site.\nOverview\nWeb Vitals\nThe business impact of Core Web Vitals\nOptimizing Core Web Vitals for business decision makers\nCore Web Vitals Metrics\nLargest Contentful Paint (LCP)\nCumulative Layout Shift (CLS)\nInteraction to Next Paint (INP)\nDefining the Core Web Vitals metrics thresholds\nMeasure Core Web Vitals\nGetting started with measuring Web Vitals\nCore Web Vitals workflows with Google tools\nBest practices for measuring Web Vitals in the field\nWhy is CrUX data different from my RUM data?\nDebug Core Web Vitals\nWhy lab and field data can be different (and what to do about it)\nDebug layout shifts\nDebug performance in the field\nImprove Core Web Vitals\nOptimize Largest Contentful Paint\nOptimize Cumulative Layout Shift\nOptimize Interaction to Next Paint\nThe most effective ways to improve Core Web Vitals\nBest practices\nBest practices for carousels\nBest practices for cookie notices\nBest practices for fonts\nBest practices for tags and tag managers\nBest practices for using third-party embeds\nInfinite scroll without layout shifts\nCSS for Web Vitals", "engine": "generic", "topic": "page_experience", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1444}
{"id": "17511885-056a-4953-8960-a0df91c10cfb", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "ï»¿\nsitemaps.org - Protocol\nsitemaps.org\nFAQ\nProtocol\nHome\nSitemaps XML format\nJump to:\nXML tag definitions\nEntity escaping\nUsing Sitemap index files\nOther Sitemap formats\nSitemap file location\nValidating your Sitemap\nExtending the Sitemaps protocol\nInforming search engine crawlers\nThis document describes the XML schema for the Sitemap protocol.\nThe Sitemap protocol format consists of XML tags. All data values in a Sitemap must\nbe\nentity-escaped\n. The file itself must be UTF-8 encoded.\nThe Sitemap must:\nBegin with an opening\n<\nurlset\n>\ntag and\nend with a closing\n</urlset>\ntag.\nSpecify the namespace (protocol standard) within the\n<urlset>\ntag.\nInclude a\n<\nurl\n>\nentry for each URL, as\na parent XML tag.\nInclude a\n<\nloc\n>\nchild entry for each\n<url>\nparent tag.\nAll other tags are optional. Support for these optional tags may vary among search\nengines. Refer to each search engine's documentation for details.\nAlso, all URLs in a Sitemap must be from a single host, such as www.example.com\nor store.example.com. For further details, refer the\nSitemap file\nlocation\nSample XML Sitemap\nThe following example shows a Sitemap that contains just one URL and uses all optional\ntags. The optional tags are in italics.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<\nurlset\nxmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<\nurl\n>\n<\nloc\n>http://www.example.com/</loc>\n<\nlastmod\n>2005-01-01</lastmod>\n<\nchangefreq\n>monthly</changefreq>\n<\npriority\n>0.8</priority>\n</url>\n</urlset>\nAlso see our example with\nmultiple URLs\n.\nXML tag definitions\nThe available XML tags are described below.\nAttribute\nDescription\n<urlset>\nrequired\nEncapsulates the file and references the current protocol standard.\n<url>\nrequired\nParent tag for each URL entry. The remaining tags are children of this tag.\n<loc>\nrequired\nURL of the page. This URL must begin with the protocol (such as http) and end with\na trailing slash, if your web server requires it. This value must be less than 2,048\ncharacters.\n<lastmod>\noptional", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1992}
{"id": "d9838ff8-1f58-4ee3-b7dc-778f74bd8e3d", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "The date of last modification of the page. This date should be in\nW3C Datetime\nformat.\nThis format allows you to omit the time portion, if desired, and use YYYY-MM-DD.\nNote that the date must be set to the date the linked page was last modified, not when the sitemap is generated.\nNote also that this tag is separate from the If-Modified-Since (304) header the server can return, and search engines may use the information from both sources differently.\n<changefreq>\noptional\nHow frequently the page is likely to change. This value provides general information\nto search engines and may not correlate exactly to how often they crawl the page.\nValid values are:\nalways\nhourly\ndaily\nweekly\nmonthly\nyearly\nnever\nThe value \"always\" should be used to describe documents that change each time they\nare accessed. The value \"never\" should be used to describe archived URLs.\nPlease note that the value of this tag is considered a\nhint\nand not a command.\nEven though search engine crawlers may consider this information when making decisions,\nthey may crawl pages marked \"hourly\" less frequently than that, and they may crawl\npages marked \"yearly\" more frequently than that. Crawlers may periodically crawl\npages marked \"never\" so that they can handle unexpected changes to those pages.\n<priority>\noptional\nThe priority of this URL relative to other URLs on your site. Valid values range\nfrom 0.0 to 1.0. This value does not affect how your pages are compared to pages\non other sites—it only lets the search engines know which pages you deem most\nimportant for the crawlers.\nThe default priority of a page is 0.5.\nPlease note that the priority you assign to a page is not likely to influence the\nposition of your URLs in a search engine's result pages. Search engines may use\nthis information when selecting between URLs on the same site, so you can use this\ntag to increase the likelihood that your most important pages are present in a search\nindex.", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1993, "chunk_char_end": 3934}
{"id": "e2c79877-0a6f-4aca-9672-fde0165bd75a", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "Also, please note that assigning a high priority to all of the URLs on your site\nis not likely to help you. Since the priority is relative, it is only used to select\nbetween URLs on your site.\nBack to top\nEntity escaping\nYour Sitemap file must be UTF-8 encoded (you can generally do this when you save\nthe file). As with all XML files, any data values (including URLs) must use entity\nescape codes for the characters listed in the table below.\nCharacter\nEscape Code\nAmpersand\n&\n&amp;\nSingle Quote\n'\n&apos;\nDouble Quote\n\"\n&quot;\nGreater Than\n>\n&gt;\nLess Than\n<\n&lt;\nIn addition, all URLs (including the URL of your Sitemap) must be URL-escaped and\nencoded for readability by the web server on which they are located. However, if\nyou are using any sort of script, tool, or log file to generate your URLs (anything\nexcept typing them in by hand), this is usually already done for you. Please check\nto make sure that your URLs follow the\nRFC-3986\nstandard for URIs, the\nRFC-3987\nstandard for IRIs, and the\nXML standard\n.\nBelow is an example of a URL that uses a non-ASCII character (\nü\n),\nas well as a character that requires entity escaping (\n&\n):\nhttp://www.example.com/ümlat.php&q=name\nBelow is that same URL, ISO-8859-1 encoded (for hosting on a server that uses that\nencoding) and URL escaped:\nhttp://www.example.com/%FCmlat.php&q=name\nBelow is that same URL, UTF-8 encoded (for hosting on a server that uses that encoding)\nand URL escaped:\nhttp://www.example.com/%C3%BCmlat.php&q=name\nBelow is that same URL, but also entity escaped:\nhttp://www.example.com/%C3%BCmlat.php&amp;q=name\nSample XML Sitemap\nThe following example shows a Sitemap in XML format. The Sitemap in the example\ncontains a small number of URLs, each using a different set of optional parameters.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<\nurlset\nxmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<\nurl\n>\n<\nloc\n>http://www.example.com/</loc>\n<\nlastmod\n>2005-01-01</lastmod>\n<\nchangefreq\n>monthly</changefreq>\n<\npriority", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3935, "chunk_char_end": 5926}
{"id": "870cc841-1db9-45b0-8b91-ae467711764e", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": ">0.8</priority>\n</url>\n<\nurl\n>\n<\nloc\n>http://www.example.com/catalog?item=12&amp;desc=vacation_hawaii</loc>\n<\nchangefreq\n>weekly</changefreq>\n</url>\n<\nurl\n>\n<\nloc\n>http://www.example.com/catalog?item=73&amp;desc=vacation_new_zealand</loc>\n<\nlastmod\n>2004-12-23</lastmod>\n<\nchangefreq\n>weekly</changefreq>\n</url>\n<\nurl\n>\n<\nloc\n>http://www.example.com/catalog?item=74&amp;desc=vacation_newfoundland</loc>\n<\nlastmod\n>2004-12-23T18:00:15+00:00</lastmod>\n<\npriority\n>0.3</priority>\n</url>\n<\nurl\n>\n<\nloc\n>http://www.example.com/catalog?item=83&amp;desc=vacation_usa</loc>\n<\nlastmod\n>2004-11-23</lastmod>\n</url>\n</urlset>\nBack to top\nUsing Sitemap index files (to group multiple sitemap\nfiles)\nYou can provide multiple Sitemap files, but each Sitemap file that you provide must\nhave no more than 50,000 URLs and must be no larger than 50MB (52,428,800 bytes).\nIf you would like, you may compress your Sitemap files using gzip to reduce your\nbandwidth requirement; however the sitemap file once uncompressed must be no larger\nthan 50MB. If you want to list more than 50,000 URLs, you must create multiple Sitemap\nfiles.\nIf you do provide multiple Sitemaps, you should then list each Sitemap file in a\nSitemap index file. Sitemap index files may not list more than 50,000 Sitemaps and\nmust be no larger than 50MB (52,428,800 bytes) and can be compressed. You can have\nmore than one Sitemap index file. The XML format of a Sitemap index file is very\nsimilar to the XML format of a Sitemap file.\nThe Sitemap index file must:\nBegin with an opening\n<\nsitemapindex\n>\ntag and end with a closing\n</sitemapindex>\ntag.\nInclude a\n<\nsitemap\n>\nentry\nfor each Sitemap as a parent XML tag.\nInclude a\n<\nloc\n>\nchild entry for\neach\n<sitemap>\nparent tag.\nThe optional\n<\nlastmod\n>\ntag\nis also available for Sitemap index files.\nNote:\nA Sitemap index file can only specify Sitemaps that are found\non the same site as the Sitemap index file. For example, http://www.yoursite.com/sitemap_index.xml", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5927, "chunk_char_end": 7893}
{"id": "be9d60ee-6991-48f2-8961-fe95c4cd53a6", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "can include Sitemaps on http://www.yoursite.com but not on http://www.example.com\nor http://yourhost.yoursite.com. As with Sitemaps, your Sitemap index file must\nbe UTF-8 encoded.\nSample XML Sitemap\nIndex\nThe following example shows a Sitemap index that lists two Sitemaps:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<\nsitemapindex\nxmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<\nsitemap\n>\n<\nloc\n>http://www.example.com/sitemap1.xml.gz</loc>\n<\nlastmod\n>2004-10-01T18:23:17+00:00</lastmod>\n</sitemap>\n<\nsitemap\n>\n<\nloc\n>http://www.example.com/sitemap2.xml.gz</loc>\n<\nlastmod\n>2005-01-01</lastmod>\n</sitemap>\n</sitemapindex>\nNote:\nSitemap URLs, like all values in your XML files, must be\nentity escaped\n.\nSitemap\nIndex XML Tag Definitions\nAttribute\nDescription\n<sitemapindex>\nrequired\nEncapsulates information about all of the Sitemaps in the file.\n<sitemap>\nrequired\nEncapsulates information about an individual Sitemap.\n<loc>\nrequired\nIdentifies the location of the Sitemap.\nThis location can be a Sitemap, an Atom file, RSS file or a simple text file.\n<lastmod>\noptional\nIdentifies the time that the corresponding Sitemap file was modified. It does not\ncorrespond to the time that any of the pages listed in that Sitemap were changed.\nThe value for the lastmod tag should be in\nW3C Datetime\nformat.\nBy providing the last modification timestamp, you enable search engine crawlers\nto retrieve only a subset of the Sitemaps in the index i.e. a crawler may only retrieve\nSitemaps that were modified since a certain date. This incremental Sitemap fetching\nmechanism allows for the rapid discovery of new URLs on very large sites.\nBack to top\nOther Sitemap formats\nThe Sitemap protocol enables you to provide details about your pages to search engines,\nand we encourage its use since you can provide additional information about site\npages beyond just the URLs. However, in addition to the XML protocol, we support\nRSS feeds and text files, which provide more limited information.\nSyndication feed", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7894, "chunk_char_end": 9889}
{"id": "0632f20c-f70d-4e63-bc98-daa8115e1714", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "You can provide an RSS (Real Simple Syndication) 2.0 or Atom 0.3 or 1.0 feed. Generally,\nyou would use this format only if your site already has a syndication feed. Note\nthat this method may not let search engines know about all the URLs in your site,\nsince the feed may only provide information on recent URLs, although search engines\ncan still use that information to find out about other pages on your site during\ntheir normal crawling processes by following links inside pages in the feed. Make\nsure that the feed is located in the highest-level directory you want search engines\nto crawl. Search engines extract the information from the feed as follows:\n<link> field\n- indicates the URL\nmodified date field (the <pubDate> field for RSS feeds and the <updated>\ndate for Atom feeds)\n- indicates when each URL was last modified. Use of\nthe modified date field is optional.\nText file\nYou can provide a simple text file that contains one URL per line. The text file\nmust follow these guidelines:\nThe text file must have one URL per line. The URLs cannot contain embedded new lines.\nYou must fully specify URLs, including the http.\nEach text file can contain a maximum of 50,000 URLs and must be no larger than 50MB\n(52,428,800 bytes). If you site includes more than 50,000 URLs, you can separate\nthe list into multiple text files and add each one separately.\nThe text file must use UTF-8 encoding. You can specify this when you save the file\n(for instance, in Notepad, this is listed in the Encoding menu of the Save As dialog\nbox).\nThe text file should contain no information other than the list of URLs.\nThe text file should contain no header or footer information.\nIf you would like, you may compress your Sitemap text file using gzip to reduce\nyour bandwidth requirement.\nYou can name the text file anything you wish. Please check to make sure that your\nURLs follow the\nRFC-3986\nstandard\nfor URIs, the\nRFC-3987\nstandard\nfor IRIs", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9890, "chunk_char_end": 11822}
{"id": "f555534e-525f-489d-8b33-241ff04f1ec2", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "You should upload the text file to the highest-level directory you want search engines\nto crawl and make sure that you don't list URLs in the text file that are located\nin a higher-level directory.\nSample text file entries are shown below.\nhttp://www.example.com/catalog?item=1\nhttp://www.example.com/catalog?item=11\nBack to top\nSitemap file location\nThe location of a Sitemap file determines the set of URLs that can be included in\nthat Sitemap. A Sitemap file located at http://example.com/catalog/sitemap.xml can\ninclude any URLs starting with http://example.com/catalog/ but can not include URLs\nstarting with http://example.com/images/.\nIf you have the permission to change http://example.org/path/sitemap.xml, it is\nassumed that you also have permission to provide information for URLs with the prefix\nhttp://example.org/path/. Examples of URLs considered valid in http://example.com/catalog/sitemap.xml\ninclude:\nhttp://example.com/catalog/show?item=23\nhttp://example.com/catalog/show?item=233&user=3453\nURLs not considered valid in http://example.com/catalog/sitemap.xml include:\nhttp://example.com/image/show?item=23\nhttp://example.com/image/show?item=233&user=3453\nhttps://example.com/catalog/page1.php\nNote that this means that all URLs listed in the Sitemap must use the same protocol\n(http, in this example) and reside on the same host as the Sitemap. For instance,\nif the Sitemap is located at http://www.example.com/sitemap.xml, it can't include\nURLs from http://subdomain.example.com.\nURLs that are not considered valid are dropped from further consideration. It is\nstrongly recommended that you place your Sitemap at the root directory of your web\nserver. For example, if your web server is at example.com, then your Sitemap index\nfile would be at http://example.com/sitemap.xml. In certain cases, you may need\nto produce different Sitemaps for different paths (e.g., if security permissions\nin your organization compartmentalize write access to different directories).", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11823, "chunk_char_end": 13808}
{"id": "348e1b3e-8361-487b-bd49-ed7e392337f1", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "If you submit a Sitemap using a path with a port number, you must include that port\nnumber as part of the path in each URL listed in the Sitemap file. For instance,\nif your Sitemap is located at http://www.example.com:100/sitemap.xml, then each\nURL listed in the Sitemap must begin with http://www.example.com:100.\nSitemaps & Cross\nSubmits\nTo submit Sitemaps for multiple hosts from a single host, you need to \"prove\" ownership\nof the host(s) for which URLs are being submitted in a Sitemap. Here's an example.\nLet's say that you want to submit Sitemaps for 3 hosts:\nwww.host1.com with Sitemap file sitemap-host1.xml\nwww.host2.com with Sitemap file sitemap-host2.xml\nwww.host3.com with Sitemap file sitemap-host3.xml\nMoreover, you want to place all three Sitemaps on a single host: www.sitemaphost.com.\nSo the Sitemap URLs will be:\nhttp://www.sitemaphost.com/sitemap-host1.xml\nhttp://www.sitemaphost.com/sitemap-host2.xml\nhttp://www.sitemaphost.com/sitemap-host3.xml\nBy default, this will result in a \"cross submission\" error since you are trying\nto submit URLs for www.host1.com through a Sitemap that is hosted on www.sitemaphost.com\n(and same for the other two hosts). One way to avoid the error is to prove that\nyou own (i.e. have the authority to modify files) www.host1.com. You can do this\nby modifying the robots.txt file on www.host1.com to point to the Sitemap on www.sitemaphost.com.\nIn this example, the robots.txt file at http://www.host1.com/robots.txt would contain\nthe line \"Sitemap: http://www.sitemaphost.com/sitemap-host1.xml\". By modifying the\nrobots.txt file on www.host1.com and having it point to the Sitemap on www.sitemaphost.com,\nyou have implicitly proven that you own www.host1.com. In other words, whoever controls\nthe robots.txt file on www.host1.com trusts the Sitemap at http://www.sitemaphost.com/sitemap-host1.xml\nto contain URLs for www.host1.com. The same process can be repeated for the other\ntwo hosts.\nNow you can submit the Sitemaps on www.sitemaphost.com.", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13809, "chunk_char_end": 15805}
{"id": "4ceaba67-3b7c-464d-8b5b-7c59c9c5f967", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "When a particular host's robots.txt, say http://www.host1.com/robots.txt, points\nto a Sitemap or a Sitemap index on another host; it is expected that for each of\nthe target Sitemaps, such as http://www.sitemaphost.com/sitemap-host1.xml, all the\nURLs belong to the host pointing to it. This is because, as noted earlier, a Sitemap\nis expected to have URLs from a single host only.\nBack to top\nValidating your Sitemap\nThe following XML schemas define the elements and attributes that can appear in\nyour Sitemap file. You can download this schema from the links below:\nFor Sitemaps:\nhttp://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\nFor Sitemap index files:\nhttp://www.sitemaps.org/schemas/sitemap/0.9/siteindex.xsd\nThere are a number of tools available to help you validate the structure of your\nSitemap based on this schema. You can find a list of XML-related tools at each of\nthe following locations:\nhttp://www.w3.org/XML/Schema#Tools\nhttp://www.xml.com/pub/a/2000/12/13/schematools.html\nIn order to validate your Sitemap or Sitemap index file against a schema, the XML\nfile will need additional headers as shown below.\nSitemap:\n<?xml version='1.0' encoding='UTF-8'?>\n<urlset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\"\nxmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<url>\n...\n</url>\n</urlset>\nSitemap index file:\n<?xml version='1.0' encoding='UTF-8'?>\n<sitemapindex xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/siteindex.xsd\"\nxmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<sitemap>\n...\n</sitemap>\n</sitemapindex>\nBack to top\nExtending the Sitemaps protocol\nYou can extend the Sitemaps protocol using your own namespace. Simply specify this\nnamespace in the root element. For example:\n<?xml version='1.0' encoding='UTF-8'?>", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15806, "chunk_char_end": 17797}
{"id": "987a4004-7557-4f27-97da-4ec198e9aa93", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "<urlset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\"\nxmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\nxmlns:example=\"http://www.example.com/schemas/example_schema\"> <!-- namespace extension -->\n<url>\n<example:example_tag>\n...\n</example:example_tag>\n...\n</url>\n</urlset>\nBack to top\nInforming search engine crawlers\nOnce you have created the Sitemap file and placed it on your webserver, you need\nto inform the search engines that support this protocol of its location. You can\ndo this by:\nsubmitting it to them via the search engine's submission\ninterface\nspecifying the location in your site's robots.txt file\nsending an HTTP reques\nt\nThe search engines can then retrieve your Sitemap and make the URLs available to\ntheir crawlers.\nSubmitting your Sitemap via the search\nengine's submission interface\nTo submit your Sitemap directly to a search engine, which will enable you to receive\nstatus information and any processing errors, refer to each search engine's documentation.\nSpecifying the Sitemap location in\nyour robots.txt file\nYou can specify the location of the Sitemap using a robots.txt file. To do this,\nsimply add the following line including the full URL to the sitemap:\nSitemap: http://www.example.com/sitemap.xml\nThis directive is independent of the user-agent line, so it doesn't matter where\nyou place it in your file. If you have a Sitemap index file, you can include the\nlocation of just that file. You don't need to list each individual Sitemap listed\nin the index file.\nYou can specify more than one Sitemap file per robots.txt file.\nSitemap: http://www.example.com/sitemap-host1.xml\nSitemap: http://www.example.com/sitemap-host2.xml\nSubmitting your Sitemap via an HTTP request\nTo submit your Sitemap using an HTTP request (replace <searchengine_URL> with\nthe URL provided by the search engine), issue your request to the following URL:", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17798, "chunk_char_end": 19788}
{"id": "aae3cf14-e91a-4130-8b1e-6c735850b67c", "url": "https://www.sitemaps.org/protocol.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - Protocol", "section_path": [], "text": "<searchengine_URL>/ping?sitemap=sitemap_url\nFor example, if your Sitemap is located at http://www.example.com/sitemap.gz, your\nURL will become:\n<searchengine_URL>/ping?sitemap=http://www.example.com/sitemap.gz\nURL encode everything after the /ping?sitemap=:\n<searchengine_URL>/ping?sitemap=http%3A%2F%2Fwww.yoursite.com%2Fsitemap.gz\nYou can issue the HTTP request using wget, curl, or another mechanism of your choosing.\nA successful request will return an HTTP 200 response code; if you receive a different\nresponse, you should resubmit your request. The HTTP 200 response code only indicates\nthat the search engine has received your Sitemap, not that the Sitemap itself or\nthe URLs contained in it were valid. An easy way to do this is to set up an automated\njob to generate and submit Sitemaps on a regular basis.\nNote:\nIf you are providing a Sitemap index file, you only need\nto issue one HTTP request that includes the location of the Sitemap index file;\nyou do not need to issue individual requests for each Sitemap listed in the index.\nBack to top\nExcluding content\nThe Sitemaps protocol enables you to let search engines know what content you would\nlike indexed. To tell search engines the content you don't want indexed, use a robots.txt\nfile or robots meta tag. See\nrobotstxt.org\nfor more information on how to exclude content from search engines.\nBack to top\nLast Updated: Monday, November 21, 2016\nTerms and conditions", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19789, "chunk_char_end": 21219}
{"id": "4515946a-d9b2-451c-b2ac-6adec8fa0b3d", "url": "https://www.sitemaps.org/faq.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - FAQ", "section_path": [], "text": "ï»¿\nsitemaps.org - FAQ\nsitemaps.org\nFAQ\nProtocol\nHome\nFrequently asked questions\nHow do I represent URLs in the Sitemap?\nDoes it matter which character encoding method I use\nto generate my Sitemap files?\nHow do I specify time?\nHow do I compute lastmod date?\nWhere do I place my Sitemap?\nHow big can my Sitemap be?\nMy site has tens of millions of URLs; can I somehow\nsubmit only those that have changed recently?\nWhat do I do after I create my Sitemap?\nDo URLs in the Sitemap need to be completely specified?\nMy site has both \"http\" and \"https\" versions of URLs. Do\nI need to list both?\nURLs on my site have session IDs in them. Do I need to remove\nthem?\nDoes position of a URL in a Sitemap influence its use?\nSome of the pages on my site use frames. Should I include the\nframeset URLs or the URLs of the frame contents?\nCan I zip my Sitemaps or do they have to be gzipped?\nWill the \"priority\" hint in the XML Sitemap change\nthe ranking of my pages in search results?\nIs there an XML schema that I can validate my XML Sitemap\nagainst?\nWhat if I have another question about using the protocol or submitting\na Sitemap?\nQ:\nHow do I represent URLs\nin the Sitemap?\nAs with all XML files, any data values (including URLs) must use\nentity escape codes\nfor the following characters: ampersand (&), single\nquote ('), double quote (\"), less than (<), and greater than (>). You should\nalso make sure that all URLs follow the\nRFC-3986\nstandard for URIs, the\nRFC-3987\nstandard for IRIs, and the\nXML standard\n.\nIf you are using a script to generate your URLs, you can generally URL escape them\nas part of that script. You will still need to entity escape them. For instance,\nthe following python script entity escapes http://www.example.com/view?widget=3&count>2\n$ python\nPython 2.2.2 (#1, Feb 24 2003, 19:13:11)\n>>> import xml.sax.saxutils\n>>> xml.sax.saxutils.escape(\"http://www.example.com/view?widget=3&count>2\")\nThe resulting URL from the example above is:\nhttp://www.example.com/view?widget=3&amp;count&gt;2", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1998}
{"id": "de2e7384-90ee-47d9-99bb-a45f64bca75f", "url": "https://www.sitemaps.org/faq.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - FAQ", "section_path": [], "text": "Q:\nDoes it matter which\ncharacter encoding method I use to generate my Sitemap files?\nYes. Your Sitemap files must use UTF-8 encoding.\nQ:\nHow do I specify time?\nUse\nW3C Datetime encoding\nfor the\nlastmod timestamps and all other dates and times in this protocol. For example,\n2004-09-22T14:12:14+00:00.\nThis encoding allows you to omit the time portion of the ISO8601 format; for example,\n2004-09-22 is also valid. However, if your site changes frequently, you are encouraged\nto include the time portion so crawlers have more complete information about your\nsite.\nQ:\nHow do I compute\nlastmod date?\nFor static files, this is the actual file update date. You can use the UNIX date\ncommand to get this date:\n$ date --iso-8601=seconds -u -r /home/foo/www/bar.php\n>> 2004-10-26T08:56:39+00:00\nFor many dynamic URLs, you may be able to easily compute a lastmod date based on\nwhen the underlying data was changed or by using some approximation based on periodic\nupdates (if applicable). Using even an approximate date or timestamp can help crawlers\navoid crawling URLs that have not changed. This will reduce the bandwidth and CPU\nrequirements for your web servers.\nQ:\nWhere do I place\nmy Sitemap?\nIt is strongly recommended that you place your Sitemap at the root directory of\nyour HTML server; that is, place it at http://example.com/sitemap.xml.\nIn some situations, you may want to produce different Sitemaps for different paths\non your site — e.g., if security permissions in your organization compartmentalize\nwrite access to different directories.\nWe assume that if you have the permission to upload http://example.com/path/sitemap.xml,\nyou also have permission to report metadata under http://example.com/path/.\nAll URLs listed in the Sitemap must reside on the same host as the Sitemap. For\ninstance, if the Sitemap is located at http://www.example.com/sitemap.xml, it can't\ninclude URLs from http://subdomain.example.com. If the Sitemap is located at http://www.example.com/myfolder/sitemap.xml,", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1999, "chunk_char_end": 3995}
{"id": "3fcf3920-2a0e-4b9e-b97d-ac6dd2416092", "url": "https://www.sitemaps.org/faq.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - FAQ", "section_path": [], "text": "it can't include URLs from http://www.example.com.\nBack to top\nQ:\nHow big can my Sitemap be?\nSitemaps should be no larger than 50MB (52,428,800 bytes) and can contain a maximum\nof 50,000 URLs. These limits help to ensure that your web server does not get bogged\ndown serving very large files. This means that if your site contains more than 50,000\nURLs or your Sitemap is bigger than 50MB, you must create multiple Sitemap files\nand use a\nSitemap index file\n. You should use a\nSitemap index file even if you have a small site but plan on growing beyond 50,000\nURLs or a file size of 50MB. A Sitemap index file can include up to 50,000 Sitemaps\nand must not exceed 50MB (52,428,800 bytes). You can also use gzip to compress your\nSitemaps.\nQ:\nMy site has\ntens of millions of URLs; can I somehow submit only those that have changed recently?\nYou can list the URLs that change frequently in a small number of Sitemaps and then\nuse the\nlastmod\ntag in your\nSitemap index file\nto identify those Sitemap files. Search engines can then\nincrementally crawl only the changed Sitemaps.\nQ:\nWhat do I do after\nI create my Sitemap?\nOnce you have created your Sitemap,\nlet search engines know\nabout it by submitting directly to them, pinging\nthem, or adding the Sitemap location to your robots.txt file.\nQ:\nDo URLs in the Sitemap need\nto be completely specified?\nYes. You need to include the protocol (for instance, http) in your URL. You also\nneed to include a trailing slash in your URL if your web server requires one. For\nexample, http://www.example.com/ is a valid URL for a Sitemap, whereas www.example.com\nis not.\nQ:\nMy site has both \"http\"\nand \"https\" versions of URLs. Do I need to list both?\nNo. Please list only one version of a URL in your Sitemaps. Including multiple versions\nof URLs may result in incomplete crawling of your site.\nQ:\nURLs on my site have session\nIDs in them. Do I need to remove them?\nYes. Including session IDs in URLs may result in incomplete and redundant crawling\nof your site.\nQ:", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3996, "chunk_char_end": 5996}
{"id": "5eb61565-8e0a-4b5a-9aaa-e2e0bbf9be97", "url": "https://www.sitemaps.org/faq.html", "source_domain": "www.sitemaps.org", "title": "sitemaps.org - FAQ", "section_path": [], "text": "Does position of a URL in\na Sitemap influence its use?\nNo. The position of a URL in the Sitemap is not likely to impact how it is used\nor regarded by search engines.\nQ:\nSome of the pages on my site use frames.\nShould I include the frameset URLs or the URLs of the frame contents?\nPlease include both URLs.\nQ:\nCan I zip my Sitemaps or do\nthey have to be gzipped?\nPlease use gzip to compress your Sitemaps. Remember, your Sitemap must be no larger\nthan 50MB (52,428,800 bytes), whether compressed or not.\nQ:\nWill\nthe \"priority\" hint in the XML Sitemap change the ranking of my pages in search\nresults?\nThe \"priority\" hint in your Sitemap only indicates the importance of a particular\nURL relative to other URLs\non your own site\nand does not imply any effect\non the ranking of your pages in search results.\nQ:\nIs there an XML schema that\nI can validate my XML Sitemap against?\nYes. An XML schema is available for Sitemap files at\nhttp://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\n, and a schema for\nSitemap index files is available at\nhttp://www.sitemaps.org/schemas/sitemap/0.9/siteindex.xsd\n. You can also\nread more about\nvalidating your Sitemap\n.\nQ:\nWhat if I have another question about using the\nprotocol or submitting a Sitemap?\nSee the documentation available from each search engine for more details about submission\nand usage of Sitemaps.\nBack to top\nLast Updated: Monday, November 21, 2016\nTerms and conditions", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5997, "chunk_char_end": 7420}
{"id": "a8f6462c-2596-4a9a-bed6-61d53ccec28d", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "robots.txt - Wikipedia\nJump to content\nChecked\nFrom Wikipedia, the free encyclopedia\nThis is the\nlatest accepted revision\n,\nreviewed\non\n5 December 2025\n.\nFilename used to indicate portions for web crawling\nrobots.txt\nRobots Exclusion Protocol\nExample of a simple robots.txt file, indicating that a user-agent called \"Mallorybot\" is not allowed to crawl any of the website's pages, and that other user-agents cannot crawl more than one page every 20 seconds, and are not allowed to crawl the \"secret\" folder\nStatus\nProposed Standard\nFirst published\n1994 published, formally standardized in 2022\nAuthors\nMartijn Koster (original author)\nGary Illyes, Henner Zeller, Lizzi Sassman (IETF contributors)\nWebsite\nrobotstxt\n.org\n,\nRFC 9309\nrobots.txt\nis the\nfilename\nused for implementing the\nRobots Exclusion Protocol\n, a standard used by\nwebsites\nto indicate to visiting\nweb crawlers\nand other\nweb robots\nwhich portions of the website they are allowed to visit.\nThe standard, developed in 1994, relies on\nvoluntary compliance\n. Malicious bots can use the file as a directory of which pages to visit, though standards bodies discourage countering this with\nsecurity through obscurity\n. Some archival sites ignore robots.txt. The standard was used in the 1990s to mitigate\nserver\noverload. In the 2020s, websites began denying bots that collect information for\ngenerative artificial intelligence\n.\nThe \"robots.txt\" file can be used in conjunction with\nsitemaps\n, another robot inclusion standard for websites.\nSearch engines use crawlers (bots) to index website content. Without guidance, these bots may crawl unnecessary or irrelevant pages. The Robots.txt file helps control what search engines should or should not index.\n[\n1\n]\nHistory\n[\nedit\n]\nThe standard was proposed by\nMartijn Koster\n,\n[\n2\n]\n[\n3\n]\nwhen working for\nNexor\n[\n4\n]\nin February 1994\n[\n5\n]\non the\nwww-talk\nmailing list, the main communication channel for WWW-related activities at the time.\nCharles Stross", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1964}
{"id": "937de673-6324-49ba-82ac-6efa8176b0e6", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "claims to have provoked Koster to suggest robots.txt, after he wrote a badly behaved web crawler that inadvertently caused a\ndenial-of-service attack\non Koster's server.\n[\n6\n]\nThe standard, initially RobotsNotWanted.txt, allowed\nweb developers\nto specify which bots should not access their website or which pages bots should not access. The internet was small enough in 1994 to maintain a complete list of all bots;\nserver\noverload was a primary concern. By June 1994 it had become a\nde facto\nstandard\n;\n[\n7\n]\nmost complied, including those operated by search engines such as\nWebCrawler\n,\nLycos\n, and\nAltaVista\n.\n[\n8\n]\nOn July 1, 2019, Google announced the proposal of the Robots Exclusion Protocol as an official standard under\nInternet Engineering Task Force\n.\n[\n9\n]\nA proposed standard\n[\n10\n]\nwas published in September 2022 as RFC 9309.\nStandard\n[\nedit\n]\nA site owner wishing to give instructions to web robots places a text file called\nrobots.txt\nin the root of the web site hierarchy (e.g.\nhttps://www.example.com/robots.txt\n). This text file contains the instructions in a specific format (see examples below). Robots that choose to follow the instructions try to fetch this file and read the instructions before fetching any other file from the\nwebsite\n. If this file does not exist, web robots assume that the website owner does not wish to place any limitations on crawling the entire site.\nA robots.txt file contains instructions for bots indicating which web pages they can and cannot access. Robots.txt files are particularly important for web crawlers from search engines such as Google.", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1965, "chunk_char_end": 3566}
{"id": "4f9368d6-48b2-4fe8-a185-97c29432ecf8", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "A robots.txt file on a website will function as a request that specified robots ignore specified files or directories when crawling a site. This might be, for example, out of a preference for privacy from search engine results, or the belief that the content of the selected directories might be misleading or irrelevant to the categorization of the site as a whole, or out of a desire that an application only operates on certain data. Links to pages listed in robots.txt can still appear in search results if they are linked to from a page that is crawled.\n[\n11\n]\nA robots.txt file covers one\norigin\n. For websites with multiple\nsubdomains\n, each subdomain must have its own robots.txt file. If\nexample.com\nhad a robots.txt file but\nfoo.example.com\ndid not, the rules that would apply for\nexample.com\nwould not apply to\nfoo.example.com\n. In addition, each\nURI scheme\nand\nport\nneeds its own robots.txt file;\nhttp://example.com/robots.txt\ndoes not apply to pages under\nhttp://example.com:8080/\nor\nhttps://example.com/\n.\nCompliance\n[\nedit\n]\nThe robots.txt protocol is widely complied with by bot operators.\n[\n7\n]\nThe robots.txt played a role in the 1999 legal case of\neBay v. Bidder's Edge\n,\n[\n12\n]\nwhere eBay attempted to block a bot that did not comply with robots.txt, and in May 2000 a court ordered the company operating the bot to stop crawling eBay's servers using any automatic means, by\nlegal injunction\non the basis of\ntrespassing\n.\n[\n13\n]\n[\n14\n]\n[\n12\n]\nBidder's Edge appealed the ruling, but agreed in March 2001 to drop the appeal, pay an undisclosed amount to eBay, and stop accessing eBay's auction information.\n[\n15\n]\n[\n16\n]\nIn 2007\nHealthcare Advocates v. Harding\n, a company was sued for accessing protected web pages archived via\nThe Wayback Machine\n, despite robots.txt rules denying those pages from the archive. A\nPennsylvania\ncourt ruled \"in this situation, the robots.txt file qualifies as a technological measure\" under the\nDMCA", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3567, "chunk_char_end": 5518}
{"id": "07c4df68-a0b0-4554-9bc4-41cfb92406db", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": ". Due to a malfunction at Internet Archive, Harding could temporarly access these pages from the archive and thus the court found \"the Harding firm did not circumvent the protective measure\".\n[\n17\n]\n[\n18\n]\n[\n19\n]\nIn 2013\nAssociated Press v. Meltwater U.S. Holdings, Inc.\nthe Associated Press sued Meltwater for\ncopyright infringement\nand misappropriation over copying of AP news items. Meltwater claimed that they did not require a license and that it was\nfair use\n, because the content was freely available and not protected by robots.txt. The court decided in March 2013 that \"Meltwater’s copying is not protected by the fair use doctrine\", mentioning among several factors that \"failure […] to employ the robots.txt protocol did not give Meltwater […] license to copy and publish AP content\".\n[\n20\n]\nSearch engines\n[\nedit\n]\nSome major\nsearch engines\nfollowing this standard include Ask,\n[\n21\n]\nAOL,\n[\n22\n]\nBaidu,\n[\n23\n]\nBing,\n[\n24\n]\nDuckDuckGo,\n[\n25\n]\nKagi,\n[\n26\n]\nGoogle,\n[\n27\n]\nYahoo!,\n[\n28\n]\nand Yandex.\n[\n29\n]\nArchival sites\n[\nedit\n]\nSome web archiving projects ignore robots.txt.\nArchive Team\nuses the file to discover more links, such as\nsitemaps\n.\n[\n30\n]\nCo-founder\nJason Scott\nsaid that \"unchecked, and left alone, the robots.txt file ensures no mirroring or reference for items that may have general use and meaning beyond the website's context.\"\n[\n31\n]\nIn 2017, the\nInternet Archive\nannounced that it would stop complying with robots.txt directives.\n[\n32\n]\n[\n7\n]\nAccording to\nDigital Trends\n, this followed widespread use of robots.txt to remove historical sites from search engine results, and contrasted with the nonprofit's aim to archive \"snapshots\" of the internet as it previously existed.\n[\n33\n]\nArtificial intelligence\n[\nedit\n]\nStarting in the 2020s, web operators began using robots.txt to deny access to bots collecting training data for\ngenerative AI\n. In 2023, Originality.AI found that 306 of the thousand most-visited websites blocked\nOpenAI", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5519, "chunk_char_end": 7487}
{"id": "3d592639-ee79-4483-afdb-a6f42b166c85", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "'s GPTBot in their robots.txt file and 85 blocked\nGoogle\n's Google-Extended. Many robots.txt files named GPTBot as the only bot explicitly disallowed on all pages. Denying access to GPTBot was common among news websites such as the\nBBC\nand\nThe New York Times\n. In 2023, blog host\nMedium\nannounced it would deny access to all artificial intelligence web crawlers as \"AI companies have leached value from writers in order to spam Internet readers\".\n[\n7\n]\nGPTBot complies with the robots.txt standard and gives advice to web operators about how to disallow it, but\nThe Verge\n'\ns David Pierce said this only began after \"training the underlying models that made it so powerful\". Also, some bots are used both for search engines and artificial intelligence, and it may be impossible to block only one of these options.\n[\n7\n]\n404 Media\nreported that companies like\nAnthropic\nand\nPerplexity.ai\ncircumvented robots.txt by renaming or spinning up new scrapers to replace the ones that appeared on popular\nblocklists\n.\n[\n34\n]\nIn 2025, the nonprofit\nRSL Collective\nannounced the launch of the\nReally Simple Licensing\n(RSL) open content licensing standard, allowing web publishers to set terms for AI bots in their robots.txt files. Participating companies at launch included Medium,\nReddit\n, and\nYahoo\n.\n[\n35\n]\n[\n36\n]\n[\n37\n]\nSecurity\n[\nedit\n]\nDespite the use of the terms\nallow\nand\ndisallow\n, the protocol is purely advisory and relies on the compliance of the\nweb robot\n; it cannot enforce any of what is stated in the file.\n[\n38\n]\nMalicious web robots are unlikely to honor robots.txt; some may even use the robots.txt as a guide to find disallowed links and go straight to them. While this is sometimes claimed to be a security risk,\n[\n39\n]\nthis sort of\nsecurity through obscurity\nis discouraged by standards bodies. The\nNational Institute of Standards and Technology", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7488, "chunk_char_end": 9347}
{"id": "8e05204d-9e49-401f-8132-aa8c2a2363ec", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "(NIST) in the United States specifically recommends against this practice: \"System security should not depend on the secrecy of the implementation or its components.\"\n[\n40\n]\nIn the context of robots.txt files, security through obscurity is not recommended as a security technique.\n[\n41\n]\nAlternatives\n[\nedit\n]\nMany robots also pass a special\nuser-agent\nto the web server when fetching content.\n[\n42\n]\nA web administrator could also configure the server to automatically return failure (or\npass alternative content\n) when it detects a connection using one of the robots.\n[\n43\n]\n[\n44\n]\nSome sites, such as\nGoogle\n, host a\nhumans.txt\nfile that displays information meant for humans to read.\n[\n45\n]\nSome sites such as\nGitHub\nredirect humans.txt to an\nAbout\npage.\n[\n46\n]\nPreviously, Google had a joke file hosted at\n/killer-robots.txt\ninstructing\nthe Terminator\nnot to kill the company founders\nLarry Page\nand\nSergey Brin\n.\n[\n47\n]\n[\n48\n]\nExamples\n[\nedit\n]\nThis example tells all robots that they can visit all files because the wildcard\n*\nstands for all robots and the\nDisallow\ndirective has no value, meaning no pages are disallowed. Search engine giant Google open-sourced their robots.txt parser,\n[\n49\n]\nand recommends testing and validating rules on the robots.txt file using community-built testers such as Tame the Bots\n[\n50\n]\nand Real Robots Txt.\n[\n51\n]\nUser-agent: *\nDisallow:\nThis example has the same effect, allowing all files rather than prohibiting none.\nUser-agent: *\nAllow: /\nThe same result can be accomplished with an empty or missing robots.txt file.\nThis example tells all robots to stay out of a website:\nUser-agent: *\nDisallow: /\nThis example tells all robots not to enter three directories:\nUser-agent: *\nDisallow: /cgi-bin/\nDisallow: /tmp/\nDisallow: /junk/\nThis example tells all robots to stay away from one specific file:\nUser-agent: *\nDisallow: /directory/file.html\nAll other files in the specified directory will be processed.", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9348, "chunk_char_end": 11296}
{"id": "73315026-ed48-455d-a97f-d6f5cb1e5aaa", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "This example tells one specific robot to stay out of a website:\nUser-agent: BadBot # replace 'BadBot' with the actual user-agent of the bot\nDisallow: /\nThis example tells two specific robots not to enter one specific directory:\nUser-agent: BadBot # replace 'BadBot' with the actual user-agent of the bot\nUser-agent: Googlebot\nDisallow: /private/\nExample demonstrating how comments can be used:\n# Comments appear after the \"#\" symbol at the start of a line, or after a directive\nUser-agent: * # match all bots\nDisallow: / # keep them out\nIt is also possible to list multiple robots with their own rules. The actual robot string is defined by the crawler. A few robot operators, such as\nGoogle\n, support several user-agent strings that allow the operator to deny access to a subset of their services by using specific user-agent strings.\n[\n27\n]\nExample demonstrating multiple user-agents:\nUser-agent: googlebot        # all Google services\nDisallow: /private/          # disallow this directory\nUser-agent: googlebot-news   # only the news service\nDisallow: /                  # disallow everything\nUser-agent: *                # any robot\nDisallow: /something/        # disallow this directory\nThe use of the wildcard * in rules\n[\nedit\n]\nThe directive\nDisallow: /something/\nblocks all files and subdirectories starting with\n/something/\n.\nIn contrast using a wildcard, (if supported by the crawler), allows for more complex patterns in specifying paths and files to allow or disallow from crawling, for example\nDisallow: /something/*/other\nblocks URLs such as:\n/something/foo/other\n/something/bar/other\nIt would not prevent the crawling of\n/something/foo/else\n, as that would not match the pattern.\nThe wildcard\n*\nallows greater flexibility but may not be recognized by all crawlers, although it is part of the Robots Exclusion Protocol RFC\n[\n52\n]\nA wildcard at the end of a rule in effect does nothing, as that is the standard behaviour.\nNonstandard extensions\n[\nedit\n]\nCrawl-delay directive\n[\nedit\n]", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11297, "chunk_char_end": 13296}
{"id": "20549ed0-526e-4dcd-9fbc-4480ac7c1910", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "The crawl-delay value is supported by some crawlers to throttle their visits to the host. Since this value is not part of the standard, its interpretation is dependent on the crawler reading it. It is used when the multiple burst of visits from bots is slowing down the host. Yandex interprets the value as the number of seconds to wait between subsequent visits.\n[\n29\n]\nBing defines crawl-delay as the size of a time window (from 1 to 30 seconds) during which BingBot will access a web site only once.\n[\n53\n]\nGoogle ignores this directive,\n[\n54\n]\nbut provides an interface in its\nsearch console\nfor webmasters, to control the\nGooglebot\n's subsequent visits.\n[\n55\n]\nUser-agent: bingbot\nAllow: /\nCrawl-delay: 10\nSitemap\n[\nedit\n]\nSome crawlers support a\nSitemap\ndirective, allowing multiple\nSitemaps\nin the same\nrobots.txt\nin the form\nSitemap:\nfull-url\n:\n[\n56\n]\n[\n57\n]\nSitemap: http://www.example.com/sitemap.xml\nUniversal \"*\" match\n[\nedit\n]\nThe\nRobot Exclusion Standard\ndoes not mention the \"*\" character in the\nDisallow:\nstatement.\n[\n58\n]\nContent-Signal\n[\nedit\n]\nCloudflare\nintroduced\nContent-Signal\n[\n59\n]\n[\n60\n]\nas a directive to suggest acceptable crawler behavior by type,\nai-train\n,\nai-input\n, and\nsearch\nwith values of\nyes\nor\nno\nfor each.\n[\n61\n]\nContent-Signal: ai-train=no, search=yes, ai-input=no\nMeta tags and headers\n[\nedit\n]\nIn addition to root-level robots.txt files, robots exclusion directives can be applied at a more granular level through the use of\nRobots meta tags\nand X-Robots-Tag HTTP headers. The robots meta tag cannot be used for non-HTML files such as images, text files, or PDF documents. On the other hand, the X-Robots-Tag can be added to non-HTML files by using\n.htaccess\nand\nhttpd.conf\nfiles.\n[\n62\n]\nA \"noindex\" meta tag\n[\nedit\n]\n<\nmeta\nname\n=\n\"robots\"\ncontent\n=\n\"noindex\"\n/>\nA \"noindex\" HTTP response header\n[\nedit\n]\nX-Robots-Tag: noindex", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13297, "chunk_char_end": 15166}
{"id": "ec05f58b-42c6-40fe-8879-f0f098fd9dd7", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "The X-Robots-Tag is only effective after the page has been requested and the server responds, and the robots meta tag is only effective after the page has loaded, whereas robots.txt is effective before the page is requested. Thus if a page is excluded by a robots.txt file, any robots meta tags or X-Robots-Tag headers are effectively ignored because the robot will not see them in the first place.\n[\n62\n]\nMaximum size of a robots.txt file\n[\nedit\n]\nThe Robots Exclusion Protocol requires crawlers to parse at least 500 kibibytes (512000 bytes) of robots.txt files,\n[\n63\n]\nwhich Google maintains as a 500 kibibyte file size restriction for robots.txt files.\n[\n64\n]\nSee also\n[\nedit\n]\nInternet portal\nads.txt\n, a standard for listing authorized ad sellers\nsecurity.txt\n, a file to describe the process for security researchers to follow in order to report security vulnerabilities\neBay v. Bidder's Edge\nhiQ Labs v. LinkedIn\nAutomated Content Access Protocol\n– A failed proposal to extend robots.txt\nBotSeer\n– Now inactive search engine for robots.txt files\nDistributed web crawling\nFocused crawler\nInternet Archive\nMeta elements\nfor search engines\nNational Digital Library Program\n(NDLP)\nNational Digital Information Infrastructure and Preservation Program\n(NDIIPP)\nnofollow\nnoindex\nPerma.cc\nReally Simple Licensing\nSitemaps\nSpider trap\nWeb archiving\nWeb crawler\nReferences\n[\nedit\n]\n^\n\"What is RobotsTxt File: Its Importance\"\n.\nTechxGurus\n. Retrieved\n2025-09-16\n.\n^\n\"Historical\"\n.\nGreenhills.co.uk\n.\nArchived\nfrom the original on 2017-04-03\n. Retrieved\n2017-03-03\n.\n^\nFielding, Roy (1994).\n\"Maintaining Distributed Hypertext Infostructures: Welcome to MOMspider's Web\"\n(PostScript)\n.\nFirst International Conference on the World Wide Web\n. Geneva.\nArchived\nfrom the original on 2013-09-27\n. Retrieved\nSeptember 25,\n2013\n.\n^\n\"The Web Robots Pages\"\n. Robotstxt.org. 1994-06-30.\nArchived\nfrom the original on 2014-01-12\n. Retrieved\n2013-12-29\n.\n^\nKoster, Martijn (25 February 1994).", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15167, "chunk_char_end": 17142}
{"id": "8a245100-f057-4b99-9ab9-892591ab2c28", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "\"Important: Spiders, Robots and Web Wanderers\"\n.\nwww-talk mailing list\n. Archived from\nthe original\n(\nHypermail\narchived message)\non October 29, 2013.\n^\n\"How I got here in the end, part five: \"things can only get better!\"\n\"\n.\nCharlie's Diary\n. 19 June 2006.\nArchived\nfrom the original on 2013-11-25\n. Retrieved\n19 April\n2014\n.\n^\na\nb\nc\nd\ne\nPierce, David (14 February 2024).\n\"The text file that runs the internet\"\n.\nThe Verge\n. Retrieved\n16 March\n2024\n.\n^\nBarry Schwartz (30 June 2014).\n\"Robots.txt Celebrates 20 Years Of Blocking Search Engines\"\n.\nSearch Engine Land\n.\nArchived\nfrom the original on 2015-09-07\n. Retrieved\n2015-11-19\n.\n^\n\"Formalizing the Robots Exclusion Protocol Specification\"\n.\nOfficial Google Webmaster Central Blog\n.\nArchived\nfrom the original on 2019-07-10\n. Retrieved\n2019-07-10\n.\n^\nKoster, M.; Illyes, G.; Zeller, H.; Sassman, L. (September 2022).\nRobots Exclusion Protocol\n.\nInternet Engineering Task Force\n.\ndoi\n:\n10.17487/RFC9309\n.\nRFC\n9309\n.\nProposed Standard.\n^\n\"Uncrawled URLs in search results\"\n. YouTube. Oct 5, 2009.\nArchived\nfrom the original on 2014-01-06\n. Retrieved\n2013-12-29\n.\n^\na\nb\n\"EBay Fights Spiders on the Web\"\n.\nWired\n. 2000-07-31.\nISSN\n1059-1028\n. Retrieved\n2024-08-02\n.\n^\neBay v. Bidder's Edge\n,\n100 F. Supp. 2d 1058\n(\nN.D. Cal.\n2000), archived from\nthe original\n.\n^\nHoffmann, Jay (2020-09-15).\n\"Chapter 4: Search\"\n.\nThe History of the Web\n. Retrieved\n2024-08-02\n.\n^\nBerry, Jahna (July 24, 2001).\n\"Robots in the Hen House\"\n.\nlaw.com\n. Archived from\nthe original\non 2011-06-08\n. Retrieved\nJune 20,\n2015\n.\n^\n\"EBay, Bidder's Edge Settle Suits on Web Access\"\n.\nlatimes\n. Retrieved\nJune 20,\n2015\n.\n^\n\"Use of web archive was not hacking, says US court\"\n.\nThe Register\n. Aug 2, 2007\n. Retrieved\nOct 22,\n2025\n.\n^\n\"Memorandum - Healthcare Advocates v Harding at all\"\n(PDF)\n.\ngovinfo.gov\n. July 20, 2007\n. Retrieved\nOct 22,\n2025\n.\n^\n\"Healthcare Advocates, Inc. v. Harding, Earley, Follmer & Frailey\"\n.\nwww.courtlistener.com\n. July 20, 2007\n. Retrieved\n2025-10-23\n.", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17143, "chunk_char_end": 19143}
{"id": "ba544b68-03e5-4806-936c-ed9f1c4553b3", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "^\n\"Associated Press v. Meltwater U.S. Holdings, Inc\"\n.\nwww.courtlistener.com\n. March 21, 2013\n. Retrieved\n2025-10-23\n.\n^\n\"About Ask.com: Webmasters\"\n.\nAbout.ask.com\n.\nArchived\nfrom the original on 27 January 2013\n. Retrieved\n16 February\n2013\n.\n^\n\"About AOL Search\"\n.\nSearch.aol.com\n. Archived from\nthe original\non 13 December 2012\n. Retrieved\n16 February\n2013\n.\n^\n\"Baiduspider\"\n.\nBaidu.com\n.\nArchived\nfrom the original on 6 August 2013\n. Retrieved\n16 February\n2013\n.\n^\n\"Robots Exclusion Protocol: joining together to provide better documentation\"\n.\nBlogs.bing.com\n. 3 June 2008.\nArchived\nfrom the original on 2014-08-18\n. Retrieved\n16 February\n2013\n.\n^\n\"DuckDuckGo Bot\"\n.\nDuckDuckGo.com\n.\nArchived\nfrom the original on 16 February 2017\n. Retrieved\n25 April\n2017\n.\n^\n\"Kagi Search KagiBot\"\n.\nKagi Search\n.\nArchived\nfrom the original on 12 April 2024\n. Retrieved\n20 November\n2024\n.\n^\na\nb\n\"Webmasters: Robots.txt Specifications\"\n.\nGoogle Developers\n.\nArchived\nfrom the original on 2013-01-15\n. Retrieved\n16 February\n2013\n.\n^\n\"Submitting your website to Yahoo! Search\"\n.\nArchived\nfrom the original on 2013-01-21\n. Retrieved\n16 February\n2013\n.\n^\na\nb\n\"Using robots.txt\"\n.\nHelp.yandex.com\n.\nArchived\nfrom the original on 2013-01-25\n. Retrieved\n16 February\n2013\n.\n^\n\"ArchiveBot: Bad behavior\"\n.\nwiki.archiveteam.org\n. Archive Team.\nArchived\nfrom the original on 10 October 2022\n. Retrieved\n10 October\n2022\n.\n^\nJason Scott\n.\n\"Robots.txt is a suicide note\"\n. Archive Team.\nArchived\nfrom the original on 2017-02-18\n. Retrieved\n18 February\n2017\n.\n^\n\"Robots.txt meant for search engines don't work well for web archives | Internet Archive Blogs\"\n.\nblog.archive.org\n. 17 April 2017.\nArchived\nfrom the original on 2018-12-04\n. Retrieved\n2018-12-01\n.\n^\nJones, Brad (24 April 2017).\n\"The Internet Archive Will Ignore Robots.txt Files to Maintain Accuracy\"\n.\nDigital Trends\n.\nArchived\nfrom the original on 2017-05-16\n. Retrieved\n8 May\n2017\n.\n^\nKoebler, Jason (2024-07-29).", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19144, "chunk_char_end": 21097}
{"id": "43843ac0-6e96-42bf-a1f0-9c4ba4787e40", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "\"Websites are Blocking the Wrong AI Scrapers (Because AI Companies Keep Making New Ones)\"\n.\n404 Media\n. Retrieved\n2024-07-29\n.\n^\nBrandom, Russell (September 10, 2025).\n\"RSS co-creator launches new protocol for AI data licensing\"\n.\nTechCrunch\n. Retrieved\nSeptember 10,\n2025\n.\n^\nRoth, Emma (September 10, 2025).\n\"The web has a new system for making AI companies pay up\"\n.\nThe Verge\n. Retrieved\nSeptember 10,\n2025\n.\n^\nShanklin, Will (September 10, 2025).\n\"Reddit, Yahoo, Medium and more are adopting a new licensing standard to get compensated for AI scraping\"\n.\nEngadget\n. Retrieved\nSeptember 10,\n2025\n.\n^\n\"Block URLs with robots.txt: Learn about robots.txt files\"\n.\nArchived\nfrom the original on 2015-08-14\n. Retrieved\n2015-08-10\n.\n^\n\"Robots.txt tells hackers the places you don't want them to look\"\n.\nThe Register\n.\nArchived\nfrom the original on 2015-08-21\n. Retrieved\nAugust 12,\n2015\n.\n^\nScarfone, K. A.; Jansen, W.; Tracy, M. (July 2008).\n\"Guide to General Server Security\"\n(PDF)\n.\nNational Institute of Standards and Technology\n.\ndoi\n:\n10.6028/NIST.SP.800-123\n.\nArchived\n(PDF)\nfrom the original on 2011-10-08\n. Retrieved\nAugust 12,\n2015\n.\n^\nSverre H. Huseby (2004).\nInnocent Code: A Security Wake-Up Call for Web Programmers\n. John Wiley & Sons. pp.\n91–\n92.\nISBN\n9780470857472\n.\nArchived\nfrom the original on 2016-04-01\n. Retrieved\n2015-08-12\n.\n^\n\"List of User-Agents (Spiders, Robots, Browser)\"\n. User-agents.org.\nArchived\nfrom the original on 2014-01-07\n. Retrieved\n2013-12-29\n.\n^\n\"Access Control - Apache HTTP Server\"\n. Httpd.apache.org.\nArchived\nfrom the original on 2013-12-29\n. Retrieved\n2013-12-29\n.\n^\n\"Deny Strings for Filtering Rules : The Official Microsoft IIS Site\"\n. Iis.net. 2013-11-06.\nArchived\nfrom the original on 2014-01-01\n. Retrieved\n2013-12-29\n.\n^\n\"Google humans.txt\"\n.\nArchived\nfrom the original on January 24, 2017\n. Retrieved\nOctober 3,\n2019\n.\n^\n\"Github humans.txt\"\n.\nGitHub\n.\nArchived\nfrom the original on May 30, 2016\n. Retrieved\nOctober 3,\n2019\n.\n^", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 11, "chunk_char_start": 21098, "chunk_char_end": 23076}
{"id": "83d595b9-53cb-452a-a773-91cac60b5f5b", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "Newman, Lily Hay (2014-07-03).\n\"Is This a Google Easter Egg or Proof That Skynet Is Actually Plotting World Domination?\"\n.\nSlate Magazine\n.\nArchived\nfrom the original on 2018-11-18\n. Retrieved\n2019-10-03\n.\n^\n\"/killer-robots.txt\"\n. 2018-01-10. Archived from\nthe original\non 2018-01-10\n. Retrieved\n2018-05-25\n.\n^\n\"Google Robots.txt Parser and Matcher Library\"\n.\nGitHub\n. Retrieved\nApril 13,\n2025\n.\n^\n\"Robots.txt Testing & Validator Tool - Tame the Bots\"\n. Retrieved\nApril 13,\n2025\n.\n^\n\"Robots.txt parser based on Google's open source parser from Will Critchlow, CEO of SearchPilot\"\n. Retrieved\nApril 13,\n2025\n.\n^\nKoster, Martijn; Illyes, Gary; Zeller, Henner; Sassman, Lizzi (September 2022).\nRobots Exclusion Protocol\n(Report). Internet Engineering Task Force.\n^\n\"To crawl or not to crawl, that is BingBot's question\"\n. 3 May 2012.\nArchived\nfrom the original on 2016-02-03\n. Retrieved\n9 February\n2016\n.\n^\n\"How Google interprets the robots.txt specification\"\n.\nGoogle Search Central\n. 2024-05-23\n. Retrieved\n2024-10-06\n.\n^\n\"Change Googlebot crawl rate - Search Console Help\"\n.\nsupport.google.com\n.\nArchived\nfrom the original on 2018-11-18\n. Retrieved\n22 October\n2018\n.\n^\n\"Yahoo! Search Blog - Webmasters can now auto-discover with Sitemaps\"\n. Archived from\nthe original\non 2009-03-05\n. Retrieved\n2009-03-23\n.\n^\n\"FAQ - Common Crawl\"\n. Retrieved\n2025-05-26\n.\nHow can I ensure the Common Crawl CCBot can crawl my site effectively? The crawler supports the Sitemap Protocol and utilizes any Sitemap announced in the robots.txt file.\n^\n\"Robots.txt Specifications\"\n.\nGoogle Developers\n.\nArchived\nfrom the original on November 2, 2019\n. Retrieved\nFebruary 15,\n2020\n.\n^\n\"ContentSignals website\"\n.\nArchived\nfrom the original on 2025-09-29\n. Retrieved\n2025-09-30\n.\n^\n\"Cloudflare offers way to block AI Overviews – will Google comply?\"\n.\nArchived\nfrom the original on 2025-09-26\n. Retrieved\n2025-09-30\n.\n^\n\"Giving users choice with Cloudflare's new Content Signals Policy\"\n.\nArchived", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 12, "chunk_char_start": 23077, "chunk_char_end": 25047}
{"id": "e83c82af-b49e-4c2e-98dd-db66d1cbba0f", "url": "https://en.wikipedia.org/wiki/Robots.txt", "source_domain": "en.wikipedia.org", "title": "robots.txt - Wikipedia", "section_path": [], "text": "from the original on 2025-09-30\n. Retrieved\n2025-09-30\n.\n^\na\nb\n\"Robots meta tag and X-Robots-Tag HTTP header specifications - Webmasters — Google Developers\"\n.\nArchived\nfrom the original on 2013-08-08\n. Retrieved\n2013-08-17\n.\n^\nKoster, M.; Illyes, G.; Zeller, H.; Sassman, L. (September 2022).\nRobots Exclusion Protocol\n.\nInternet Engineering Task Force\n.\ndoi\n:\n10.17487/RFC9309\n.\nRFC\n9309\n.\nProposed Standard.\nsec. 2.5: Limits.\n^\n\"How Google Interprets the robots.txt Specification | Documentation\"\n.\nGoogle Developers\n.\nArchived\nfrom the original on 2022-10-17\n. Retrieved\n2022-10-17\n.\nFurther reading\n[\nedit\n]\nAllyn, Bobby (5 July 2024).\n\"Artificial Intelligence Web Crawlers Are Running Amok\"\n.\nAll Things Considered\n.\nNPR\n.\nArchived\nfrom the original on 6 July 2024\n. Retrieved\n6 July\n2024\n.\nExternal links\n[\nedit\n]\nOfficial website\nRetrieved from \"\nhttps://en.wikipedia.org/w/index.php?title=Robots.txt&oldid=1325838858\n\"\nCategories\n:\nSearch engine optimization\nWebsites\nWeb scraping\nText files\nHidden categories:\nArticles with short description\nShort description is different from Wikidata\nWikipedia pending changes protected pages\nSearch\nSearch\nrobots.txt\n27 languages\nAdd topic", "engine": "generic", "topic": "crawling_indexing", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 13, "chunk_char_start": 25048, "chunk_char_end": 26234}
{"id": "c5d87539-dcb0-4c10-9389-69935f0316ed", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "The Open Graph protocol\nThe Open Graph protocol\nIntroduction\nThe\nOpen Graph protocol\nenables any web page to become a\nrich object in a social graph. For instance, this is used on Facebook to allow\nany web page to have the same functionality as any other object on Facebook.\nWhile many different technologies and schemas exist and could be combined\ntogether, there isn't a single technology which provides enough information to\nrichly represent any web page within the social graph. The Open Graph protocol\nbuilds on these existing technologies and gives developers one thing to\nimplement. Developer simplicity is a key goal of the Open Graph protocol which\nhas informed many of\nthe technical design decisions\n.\nBasic Metadata\nTo turn your web pages into graph objects, you need to add basic metadata to\nyour page. We've based the initial version of the protocol on\nRDFa\nwhich means that you'll place\nadditional\n<meta>\ntags in the\n<head>\nof your web page. The four required\nproperties for every page are:\nog:title\n- The title of your object as it should appear within the graph,\ne.g., \"The Rock\".\nog:type\n- The\ntype\nof your object, e.g., \"video.movie\".  Depending on\nthe type you specify, other properties may also be required.\nog:image\n- An image URL which should represent your object within the\ngraph.\nog:url\n- The canonical URL of your object that will be used as its\npermanent ID in the graph, e.g., \"https://www.imdb.com/title/tt0117500/\".\nAs an example, the following is the Open Graph protocol markup for\nThe Rock on\nIMDB\n:\n<html prefix=\"og: https://ogp.me/ns#\">\n<head>\n<title>The Rock (1996)</title>\n<meta property=\"og:title\" content=\"The Rock\" />\n<meta property=\"og:type\" content=\"video.movie\" />\n<meta property=\"og:url\" content=\"https://www.imdb.com/title/tt0117500/\" />\n<meta property=\"og:image\" content=\"https://ia.media-imdb.com/images/rock.jpg\" />\n...\n</head>\n...\n</html>\nOptional Metadata\nThe following properties are optional for any object and are generally\nrecommended:\nog:audio", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1996}
{"id": "ca3ebc31-6887-41b3-9a1f-f49c37047991", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "- A URL to an audio file to accompany this object.\nog:description\n- A one to two sentence description of your object.\nog:determiner\n- The word that appears before this object's title\nin a sentence. An\nenum\nof (a, an, the, \"\", auto). If\nauto\nis\nchosen, the consumer of your data should choose between \"a\" or \"an\".\nDefault is \"\" (blank).\nog:locale\n- The locale these tags are marked up in.\nOf the format\nlanguage_TERRITORY\n. Default is\nen_US\n.\nog:locale:alternate\n- An\narray\nof other locales this page is\navailable in.\nog:site_name\n- If your object is part of a larger web site, the name which\nshould be displayed for the overall site. e.g., \"IMDb\".\nog:video\n- A URL to a video file that complements this object.\nFor example (line-break solely for display purposes):\n<meta property=\"og:audio\" content=\"https://example.com/bond/theme.mp3\" />\n<meta property=\"og:description\"\ncontent=\"Sean Connery found fame and fortune as the\nsuave, sophisticated British agent, James Bond.\" />\n<meta property=\"og:determiner\" content=\"the\" />\n<meta property=\"og:locale\" content=\"en_GB\" />\n<meta property=\"og:locale:alternate\" content=\"fr_FR\" />\n<meta property=\"og:locale:alternate\" content=\"es_ES\" />\n<meta property=\"og:site_name\" content=\"IMDb\" />\n<meta property=\"og:video\" content=\"https://example.com/bond/trailer.swf\" />\nThe RDF schema (in\nTurtle\n)\ncan be found at\nogp.me/ns\n.\nStructured Properties\nSome properties can have extra metadata attached to them.\nThese are specified in the same way as other metadata with\nproperty\nand\ncontent\n, but the\nproperty\nwill have extra\n:\n.\nThe\nog:image\nproperty has some optional structured properties:\nog:image:url\n- Identical to\nog:image\n.\nog:image:secure_url\n- An alternate url to use if the webpage requires\nHTTPS.\nog:image:type\n- A\nMIME type\nfor this image.\nog:image:width\n- The number of pixels wide.\nog:image:height\n- The number of pixels high.\nog:image:alt\n- A description of what is in the image (not a caption). If the page specifies an og:image it should specify", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1997, "chunk_char_end": 3990}
{"id": "9d647053-4592-453c-97f7-8d23fa27209e", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "og:image:alt\n.\nA full image example:\n<meta property=\"og:image\" content=\"http://example.com/ogp.jpg\" />\n<meta property=\"og:image:secure_url\" content=\"https://secure.example.com/ogp.jpg\" />\n<meta property=\"og:image:type\" content=\"image/jpeg\" />\n<meta property=\"og:image:width\" content=\"400\" />\n<meta property=\"og:image:height\" content=\"300\" />\n<meta property=\"og:image:alt\" content=\"A shiny red apple with a bite taken out\" />\nThe\nog:video\ntag has the identical tags as\nog:image\n. Here is an example:\n<meta property=\"og:video\" content=\"http://example.com/movie.swf\" />\n<meta property=\"og:video:secure_url\" content=\"https://secure.example.com/movie.swf\" />\n<meta property=\"og:video:type\" content=\"application/x-shockwave-flash\" />\n<meta property=\"og:video:width\" content=\"400\" />\n<meta property=\"og:video:height\" content=\"300\" />\nThe\nog:audio\ntag only has the first 3 properties available\n(since size doesn't make sense for sound):\n<meta property=\"og:audio\" content=\"http://example.com/sound.mp3\" />\n<meta property=\"og:audio:secure_url\" content=\"https://secure.example.com/sound.mp3\" />\n<meta property=\"og:audio:type\" content=\"audio/mpeg\" />\nArrays\nIf a tag can have multiple values, just put multiple versions of the same\n<meta>\ntag on your page. The first tag (from top to bottom) is given\npreference during conflicts.\n<meta property=\"og:image\" content=\"https://example.com/rock.jpg\" />\n<meta property=\"og:image\" content=\"https://example.com/rock2.jpg\" />\nPut structured properties after you declare their root tag. Whenever\nanother root element is parsed, that structured property\nis considered to be done and another one is started.\nFor example:\n<meta property=\"og:image\" content=\"https://example.com/rock.jpg\" />\n<meta property=\"og:image:width\" content=\"300\" />\n<meta property=\"og:image:height\" content=\"300\" />\n<meta property=\"og:image\" content=\"https://example.com/rock2.jpg\" />\n<meta property=\"og:image\" content=\"https://example.com/rock3.jpg\" />", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3991, "chunk_char_end": 5942}
{"id": "0f555517-1fec-4ae1-ad68-d9d8bbab591e", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "<meta property=\"og:image:height\" content=\"1000\" />\nmeans there are 3 images on this page, the first image is\n300x300\n, the middle\none has unspecified dimensions, and the last one is\n1000\npx tall.\nObject Types\nIn order for your object to be represented within the graph, you need to\nspecify its type. This is done using the\nog:type\nproperty:\n<meta property=\"og:type\" content=\"website\" />\nWhen the community agrees on the schema for a type, it is added to the list\nof global types. All other objects in the type system are\nCURIEs\nof the form\n<head prefix=\"my_namespace: https://example.com/ns#\">\n<meta property=\"og:type\" content=\"my_namespace:my_type\" />\nThe global types are grouped into verticals. Each vertical has its\nown namespace. The\nog:type\nvalues for a namespace are always prefixed with\nthe namespace and then a period.\nThis is to reduce confusion with user-defined namespaced types which always\nhave colons in them.\nMusic\nNamespace URI:\nhttps://ogp.me/ns/music#\nog:type\nvalues:\nmusic.song\nmusic:duration\n-\ninteger\n>=1 - The song's length in seconds.\nmusic:album\n-\nmusic.album\narray\n-\nThe album this song is from.\nmusic:album:disc\n-\ninteger\n>=1 -\nWhich disc of the album this song is on.\nmusic:album:track\n-\ninteger\n>=1 -\nWhich track this song is.\nmusic:musician\n-\nprofile\narray\n-\nThe musician that made this song.\nmusic.album\nmusic:song\n-\nmusic.song\n- The song on this album.\nmusic:song:disc\n-\ninteger\n>=1 -\nThe same as\nmusic:album:disc\nbut in reverse.\nmusic:song:track\n-\ninteger\n>=1 -\nThe same as\nmusic:album:track\nbut in reverse.\nmusic:musician\n-\nprofile\n-\nThe musician that made this song.\nmusic:release_date\n-\ndatetime\n-\nThe date the album was released.\nmusic.playlist\nmusic:song\n- Identical to the ones on\nmusic.album\nmusic:song:disc\nmusic:song:track\nmusic:creator\n-\nprofile\n- The creator of this playlist.\nmusic.radio_station\nmusic:creator\n-\nprofile\n- The creator of this station.\nVideo\nNamespace URI:\nhttps://ogp.me/ns/video#\nog:type\nvalues:\nvideo.movie\nvideo:actor\n-\nprofile\narray\n-", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5943, "chunk_char_end": 7942}
{"id": "8dc1b3b9-0911-4c38-9b7c-3a21b5007132", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "Actors in the movie.\nvideo:actor:role\n-\nstring\n- The role they played.\nvideo:director\n-\nprofile\narray\n-\nDirectors of the movie.\nvideo:writer\n-\nprofile\narray\n-\nWriters of the movie.\nvideo:duration\n-\ninteger\n>=1 -\nThe movie's length in seconds.\nvideo:release_date\n-\ndatetime\n-\nThe date the movie was released.\nvideo:tag\n-\nstring\narray\n-\nTag words associated with this movie.\nvideo.episode\nvideo:actor\n- Identical to\nvideo.movie\nvideo:actor:role\nvideo:director\nvideo:writer\nvideo:duration\nvideo:release_date\nvideo:tag\nvideo:series\n-\nvideo.tv_show\n-\nWhich series this episode belongs to.\nvideo.tv_show\nA multi-episode TV show.\nThe metadata is identical to\nvideo.movie\n.\nvideo.other\nA video that doesn't belong in any other category.\nThe metadata is identical to\nvideo.movie\n.\nNo Vertical\nThese are globally defined objects that just don't fit into a vertical but\nyet are broadly used and agreed upon.\nog:type\nvalues:\narticle\n- Namespace URI:\nhttps://ogp.me/ns/article#\narticle:published_time\n-\ndatetime\n-\nWhen the article was first published.\narticle:modified_time\n-\ndatetime\n-\nWhen the article was last changed.\narticle:expiration_time\n-\ndatetime\n-\nWhen the article is out of date after.\narticle:author\n-\nprofile\narray\n-\nWriters of the article.\narticle:section\n-\nstring\n- A high-level section name. E.g. Technology\narticle:tag\n-\nstring\narray\n-\nTag words associated with this article.\nbook\n- Namespace URI:\nhttps://ogp.me/ns/book#\nbook:author\n-\nprofile\narray\n- Who wrote this book.\nbook:isbn\n-\nstring\n-\nThe\nISBN\nbook:release_date\n-\ndatetime\n- The date the book was released.\nbook:tag\n-\nstring\narray\n-\nTag words associated with this book.\npayment.link\n- Namespace URI:\nhttps://ogp.me/ns/payment#\n🚧\nBeta only\npayment:description\n-\nstring\n- Description about the payment link.\npayment:currency\n-\nstring\n- The currency code\nISO 4217\nof the payment.\npayment:amount\n-\nfloat\n- An amount requested on the payment link in decimal format.\npayment:expires_at\n-\ndatetime", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7943, "chunk_char_end": 9897}
{"id": "c9145619-dba5-4148-8968-7c30883e1751", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "- The date and time including minutes and seconds on which the payment link expires.\npayment:status\n-\nenum\n(PENDING, PAID, FAILED, EXPIRED) - Status of the payment.\npayment:id\n-\nstring\n- The unique identifier associated with the payment link for a given payment gateway or service provider.\npayment:success_url\n-\nurl\n- A valid URL that gets redirected when payment is success.\nprofile\n- Namespace URI:\nhttp://ogp.me/ns/profile#\nprofile:first_name\n-\nstring\n- A name normally given to an individual by a parent or self-chosen.\nprofile:last_name\n-\nstring\n- A name inherited from a family or marriage and by which the individual is commonly known.\nprofile:username\n-\nstring\n- A short unique string to identify them.\nprofile:gender\n-\nenum\n(male, female) - Their gender.\nwebsite\n- Namespace URI:\nhttps://ogp.me/ns/website#\nNo additional properties other than the basic ones.\nAny non-marked up webpage should be treated as\nog:type\nwebsite.\nTypes\nThe following types are used when defining attributes in Open Graph protocol.\nType\nDescription\nLiterals\nBoolean\nA Boolean represents a true or false value\ntrue, false, 1, 0\nDateTime\nA DateTime represents a temporal value composed of a date\n(year, month, day) and an optional time component (hours, minutes)\nISO 8601\nEnum\nA type consisting of bounded set of constant string values\n(enumeration members).\nA string value that is a member of the enumeration\nFloat\nA 64-bit signed floating point number\nAll literals that conform to the following formats:\n1.234\n-1.234\n1.2e3\n-1.2e3\n7E-10\nInteger\nA 32-bit signed integer. In many languages integers over 32-bits become\nfloats, so we limit Open Graph protocol for easy multi-language use.\nAll literals that conform to the following formats:\n1234\n-123\nString\nA sequence of Unicode characters\nAll literals composed of Unicode characters with no escape characters\nURL\nA sequence of Unicode characters that identify an Internet resource.\nAll valid URLs that utilize the http:// or https:// protocols\nImplementations", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9898, "chunk_char_end": 11890}
{"id": "26b51cd5-7302-49ac-bff1-23fde87039b0", "url": "https://ogp.me/", "source_domain": "ogp.me", "title": "The Open Graph protocol", "section_path": [], "text": "The open source community has developed a number of parsers and publishing\ntools. Let the Facebook group know if you've built something awesome too!\nFacebook Object Debugger\n- Facebook's official parser and debugger.\nGoogle Rich Snippets Testing Tool\n- Open Graph protocol support in specific verticals and Search Engines.\nPHP Validator and Markup Generator\n- OGP 2011 input validator and markup generator in PHP5 objects.\nPHP Consumer\n- A small library for accessing of Open Graph Protocol data in PHP.\nOpenGraphNode in PHP\n- A simple parser for PHP.\nPyOpenGraph\n- A library written in Python for parsing Open Graph protocol information from web sites.\nOpenGraph Ruby\n- Ruby Gem which parses web pages and extracts Open Graph protocol markup.\nOpenGraph for Java\n- Small Java class used to represent the Open Graph protocol.\nRDF::RDFa::Parser\n- Perl RDFa parser which understands the Open Graph protocol.\nThe Open Graph protocol was originally created at Facebook and is inspired by\nDublin Core\n,\nlink-rel canonical\n,\nMicroformats\n, and\nRDFa\n. The specification described on this page is available under the\nOpen Web Foundation Agreement, Version 0.9\n. This website is\nOpen Source\n.", "engine": "generic", "topic": "social_metadata", "doc_type": "spec", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11891, "chunk_char_end": 13073}
{"id": "6ccccba7-99c9-412a-a966-1dc47dafa2d6", "url": "https://developer.x.com/en/docs/x-for-websites/cards/overview/markup", "source_domain": "developer.x.com", "title": "Cards markup | Docs | X Developer Platform", "section_path": [], "text": "Cards markup | Docs | X Developer Platform\nCards markup\nCards Markup Tag Reference\nThe table in this section explains the OpenGraph fallback behavior for each Twitter tag.\nOverview of all Twitter Card Tags\nÂ¶\nProperty\nOpenGraph\ntwitter:card\nThe card type\nUsed with all cards\nog:type\nIf an og:type, og:title and og:description exist in the markup but twitter:card is absent, then a summary card may be rendered.\ntwitter:site\n@username of website. Either twitter:site or twitter:site:id is required.\nUsed with summary, summary_large_image, app, player cards\nn/a\ntwitter:site:id\nSame as twitter:site, but the userâs Twitter ID. Either twitter:site or twitter:site:id is required.\nUsed with summary, summary_large_image, player cards\nn/a\ntwitter:creator\n@username of content creator\nUsed with summary_large_image cards\nn/a\ntwitter:creator:id\nTwitter user ID of content creator\nUsed with summary, summary_large_image cards\nn/a\ntwitter:description\nDescription of content (maximum 200 characters)\nUsed with summary, summary_large_image, player cards\nog:description\ntwitter:title\nTitle of content (max 70 characters)\nUsed with summary, summary_large_image, player cards\nog:title\ntwitter:image\nURL of image to use in the card. Images must be less than 5MB in size. JPG, PNG, WEBP and GIF formats are supported. Only the first frame of an animated GIF will be used. SVG is not supported.\nUsed with summary, summary_large_image, player cards\nog:image\ntwitter:image:alt\nA text description of the image conveying the essential nature of an image to users who are visually impaired. Maximum 420 characters.\nUsed with summary, summary_large_image, player cards\nog:image:alt\ntwitter:player\nHTTPS URL of player iframe\nUsed with player card\nn/a\ntwitter:player:width\nWidth of iframe in pixels\nUsed with player card\nn/a\ntwitter:player:height\nHeight of iframe in pixels\nUsed with player card\nn/a\ntwitter:player:stream\nURL to raw video or audio stream\nUsed with player card\nn/a\ntwitter:app:name:iphone", "engine": "generic", "topic": "social_metadata", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1981}
{"id": "563aaa5a-2b32-480d-b725-1587e7978f2b", "url": "https://developer.x.com/en/docs/x-for-websites/cards/overview/markup", "source_domain": "developer.x.com", "title": "Cards markup | Docs | X Developer Platform", "section_path": [], "text": "Name of your iPhone app\nUsed with app card\nn/a\ntwitter:app:id:iphone\nYour app ID in the iTunes App Store (Note: NOT your bundle ID)\nUsed with app card\nn/a\ntwitter:app:url:iphone\nYour appâs custom URL scheme (you must include â://â after your scheme name)\nUsed with app card\nn/a\ntwitter:app:name:ipad\nName of your iPad optimized app\nUsed with app card\nn/a\ntwitter:app:id:ipad\nYour app ID in the iTunes App Store\nUsed with app card\nn/a\ntwitter:app:url:ipad\nYour appâs custom URL scheme\nUsed with app card\nn/a\ntwitter:app:name:googleplay\nName of your Android app\nUsed with app card\nn/a\ntwitter:app:id:googleplay\nYour app ID in the Google Play Store\nUsed with app card\nn/a\ntwitter:app:url:googleplay\nYour appâs custom URL scheme\nUsed with app card\nn/a\nDid someone say â¦ cookies?\nX and its partners use cookies to provide you with a better, safer and\nfaster service and to support our business. Some cookies are necessary to use\nour services, improve our services, and make sure they work properly.\nShow more about your choices\n.\nAccept all cookies\nRefuse non-essential cookies", "engine": "generic", "topic": "social_metadata", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1982, "chunk_char_end": 3066}
{"id": "2a102170-6648-4937-b901-5e1b9ffded6f", "url": "https://www.bing.com/webmasters/help/webmaster-guidelines-30fba23a", "source_domain": "www.bing.com", "title": "Bing Webmaster Tools - Help Documentation", "section_path": [], "text": "Bing Webmaster Tools - Help Documentation", "engine": "bing", "topic": "bing_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 41}
{"id": "cc857588-030d-4044-b501-27aded665d10", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nCreating helpful, reliable, people-first content\nGoogle's\nautomated ranking systems\nare designed to prioritize helpful, reliable information that's created to benefit people, and\nnot content that's created to manipulate search engine rankings. This page is designed to help\ncreators evaluate if they're producing such content.\nSelf-assess your content\nEvaluating your own content against these questions can help you gauge if the content you're\nmaking is helpful and reliable. Beyond asking yourself these questions, consider having others\nyou trust but who are unaffiliated with your site provide an honest assessment.\nAlso consider an audit of the drops you may have experienced. What pages were most impacted\nand for what types of searches? Look closely at these to understand how they're assessed\nagainst some of the questions outlined here.\nContent and quality questions\nDoes the content provide original information, reporting, research, or analysis?\nDoes the content provide a substantial, complete, or comprehensive description of the topic?\nDoes the content provide insightful analysis or interesting information that is beyond the\nobvious?\nIf the content draws on other sources, does it avoid simply copying or rewriting those\nsources, and instead provide substantial additional value and originality?\nDoes the main heading or page title provide a descriptive, helpful summary of the content?\nDoes the main heading or page title avoid exaggerating or being shocking in nature?", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1998}
{"id": "af6f0a39-3c67-4ec4-a0c8-1f3f4e4eb1b6", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Is this the sort of page you'd want to bookmark, share with a friend, or recommend?\nWould you expect to see this content in or referenced by a printed magazine, encyclopedia, or book?\nDoes the content provide substantial value when compared to other pages in search results?\nDoes the content have any spelling or stylistic issues?\nIs the content produced well, or does it appear sloppy or hastily produced?\nIs the content mass-produced by or outsourced to a large number of creators, or spread\nacross a large network of sites, so that individual pages or sites don't get as much attention or care?\nExpertise questions\nDoes the content present information in a way that makes you want to trust it, such as clear\nsourcing, evidence of the expertise involved, background about the author or the site that\npublishes it, such as through links to an author page or a site's About page?\nIf someone researched the site producing the content, would they come away with an impression\nthat it is well-trusted or widely-recognized as an authority on its topic?\nIs this content written or reviewed by an expert or enthusiast who demonstrably knows the topic well?\nDoes the content have any easily-verified factual errors?\nProvide a great page experience\nGoogle's core ranking systems look to reward content that provides a good page experience.\nSite owners seeking to be successful with our systems should not focus on only one or two\naspects of page experience. Instead, check if you're providing an overall great page\nexperience across many aspects. For more advice, see our page,\nUnderstanding page experience in Google Search results\n.\nFocus on people-first content\nPeople-first content means content that's created primarily for people, and not to manipulate\nsearch engine rankings. How can you evaluate if you're creating people-first content? Answering\nyes to the questions below means you're probably on the right track with a people-first approach:", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1999, "chunk_char_end": 3943}
{"id": "8d76da46-712f-4160-ba84-ae2d7cadd08b", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Do you have an existing or intended audience for your business or site that would find the\ncontent useful if they came directly to you?\nDoes your content clearly demonstrate first-hand expertise and a depth of knowledge (for\nexample, expertise that comes from having actually used a product or service, or visiting\na place)?\nDoes your site have a primary purpose or focus?\nAfter reading your content, will someone leave feeling they've learned enough about a\ntopic to help achieve their goal?\nWill someone reading your content leave feeling like they've had a satisfying experience?\nAvoid creating search engine-first content\nWe recommend that you focus on creating people-first content to be successful with Google\nSearch, rather than search engine-first content made primarily to gain search engine rankings.\nAnswering yes to some or all of the questions below is a warning sign that you should\nreevaluate how you're creating content:\nIs the content primarily made to attract visits from search engines?\nAre you producing lots of content on many different topics in hopes that some of it might\nperform well in search results?\nAre you using extensive automation to produce content on many topics?\nAre you mainly summarizing what others have to say without adding much value?\nAre you writing about things simply because they seem trending and not because you'd write\nabout them otherwise for your existing audience?\nDoes your content leave readers feeling like they need to search again to get better\ninformation from other sources?\nAre you writing to a particular word count because you've heard or read that Google has a\npreferred word count? (No, we don't.)\nDid you decide to enter some niche topic area without any real expertise, but instead\nmainly because you thought you'd get search traffic?\nDoes your content promise to answer a question that actually has no answer, such as\nsuggesting there's a release date for a product, movie, or TV show when one isn't confirmed?", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3944, "chunk_char_end": 5920}
{"id": "83c344c8-6bb9-4512-816e-2689decc688b", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Are you changing the date of pages to make them seem fresh when the content has not\nsubstantially changed?\nAre you adding a lot of new content or removing a lot of older content primarily because\nyou believe it will help your search rankings overall by somehow making your site seem\n\"fresh?\" (No, it won't)\nWhat about SEO? Isn't that search engine-first?\nThere are some things you could do that are specifically meant to help search engines better\ndiscover and understand your content. Collectively, this is called \"search engine optimization\"\nor SEO, for short.\nGoogle's own SEO guide\ncovers best practices to consider. SEO can be a helpful activity when it is applied to\npeople-first content, rather than search engine-first content.\nGet to know E-E-A-T and the quality rater guidelines\nGoogle's automated systems are designed to use\nmany different factors\nto rank great content. After identifying relevant content, our systems aim to prioritize those\nthat seem most helpful. To do this, they identify a mix of factors that can help determine\nwhich content demonstrates aspects of experience, expertise, authoritativeness, and trustworthiness,\nor what we call E-E-A-T.\nOf these aspects, trust is most important. The others contribute to trust, but content doesn't\nnecessarily have to demonstrate all of them. For example, some content might be helpful based\non the experience it demonstrates, while other content might be helpful because of the\nexpertise it shares.\nWhile E-E-A-T itself isn't a specific ranking factor, using a mix of factors that can identify\ncontent with good E-E-A-T is useful. For example, our systems give even more weight to content\nthat aligns with strong E-E-A-T for topics that could significantly impact the health,\nfinancial stability, or safety of people, or the welfare or well-being of society. We call these\n\"Your Money or Your Life\" topics, or YMYL for short.\nSearch quality raters", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5921, "chunk_char_end": 7837}
{"id": "a5f56dd1-b155-4d85-bd34-e67ed5955d36", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "are people who give us insights on if our algorithms seem to be providing good results, a way\nto help confirm our changes are working well. In particular, raters are trained to understand\nif content has strong E-E-A-T. The criteria they use to do this is outlined in our\nsearch quality rater guidelines\n.\nReading the guidelines may help you self-assess how your content is doing from an E-E-A-T\nperspective, improvements to consider, and help align it conceptually with the different\nsignals that our automated systems use to rank content.\nAsk \"Who, How, and Why\" about your content\nConsider evaluating your content in terms of \"Who, How, and Why\" as a way to stay on course\nwith what our systems seek to reward.\nWho (created the content)\nSomething that helps people intuitively understand the E-E-A-T of content is when it's clear who\ncreated it. That's the\n\"Who\"\nto consider. When creating content, here are some who-related\nquestions to ask yourself:\nIs it self-evident to your visitors who authored your content?\nDo pages carry a byline, where one might be expected?\nDo bylines lead to further information about the author or authors involved, giving\nbackground about them and the areas they write about?\nIf you're clearly indicating who created the content, you're likely aligned with the concepts of\nE-E-A-T and on a path to success. We strongly encourage adding accurate authorship information,\nsuch as bylines to content where readers might expect it.\nHow (the content was created)\nIt's helpful to readers to know how a piece of content was produced: this is the\n\"How\"\nto consider including in your content.\nFor example, with product reviews, it can build trust with readers when they understand the\nnumber of products that were tested, what the test results were, and how the tests were\nconducted, all accompanied by evidence of the work involved, such as photographs. It's\nadvice we share more about in our\nWrite high quality product reviews\nhelp page.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7838, "chunk_char_end": 9800}
{"id": "f095996f-5aa9-42e0-b314-84cd753ef14f", "url": "https://developers.google.com/search/docs/fundamentals/creating-helpful-content", "source_domain": "developers.google.com", "title": "Creating Helpful, Reliable, People-First Content | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Many types of content may have a \"How\" component to them. That can include automated,\nAI-generated, and AI-assisted content. Sharing details about the processes involved can help\nreaders and visitors better understand any unique and useful role automation may have served.\nIf automation is used to substantially generate content, here are some questions to ask yourself:\nIs the use of automation, including AI-generation, self-evident to visitors through disclosures or in other ways?\nAre you providing background about how automation or AI-generation was used to create content?\nAre you explaining why automation or AI was seen as useful to produce content?\nOverall, AI or automation disclosures are useful for content where someone might think\n\"How was this created?\" Consider adding these when it would be reasonably expected. For more,\nsee our blog post and FAQ:\nHow Google Search views AI-generated content\n.\nWhy (was the content created)\n\"Why\"\nis perhaps the most important question to answer about your content. Why is it\nbeing created in the first place?\nThe \"why\" should be that you're creating content primarily to help people, content that is\nuseful to visitors if they come to your site directly. If you're doing this, you're aligning\nwith E-E-A-T generally and what our\ncore ranking systems\nseek to reward.\nIf the \"why\" is that you're primarily making content to attract search engine visits, that's\nnot aligned with what our systems seek to reward. If you use automation, including AI-generation,\nto produce content for the primary purpose of manipulating search rankings, that's a\nviolation of our spam policies\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-09-22 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9801, "chunk_char_end": 11769}
{"id": "8cfef867-16f4-4b99-bbd0-6d7f5f938552", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nA guide to Google Search ranking systems\nGoogle uses automated ranking systems that\nlook at many factors and signals\nabout hundreds of billions of web pages and other content in our Search index to present the\nmost relevant, useful results, all in a fraction of a second.\nThis page is a guide to understanding some of our more notable ranking systems. It covers some\nsystems that are part of our core ranking systems, which are the underlying technologies that\nproduce search results in response to queries. It also covers some systems involved with specific\nranking needs.\nOur ranking systems are designed to work on the page level, using a variety of signals and\nsystems to understand how to rank individual pages. Site-wide signals and classifiers are also\nused and contribute to our understanding of pages. Having some good site-wide signals does not\nmean that all content from a site will always rank highly, just as having some poor site-wide\nsignals does not mean all the content from a site will rank poorly.\nWe regularly improve our ranking systems through\nrigorous testing and evaluation\nand provide notice of\nupdates to our ranking systems\nwhen those might be useful to content creators and others.\nYou can also visit our\nHow Search Works site\nto understand how our\nranking systems\n,\ncombined with other processes, work together so that Google Search delivers on our mission to\norganize the world's information and make it universally accessible and useful.\nBERT", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1977}
{"id": "3780c451-fdf4-4ee8-86ab-b1c11c9e16a0", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Bidirectional Encoder Representations from Transformers (\nBERT\n)\nis an AI system Google uses that allows us to understand how combinations of words express\ndifferent meanings and intent.\nCrisis information systems\nGoogle has developed systems to provide helpful and timely information during times of crisis,\nwhether those involve personal crisis situations, natural disasters, or other wide-spread\ncrisis situations:\nPersonal crisis:\nOur systems work to understand when people are seeking\ninformation about personal crisis situations to display hotlines and content from trusted\norganizations for certain queries related to suicide, sexual assault, poison ingestion,\ngender-based violence, or drug addiction. Learn more about how\npersonal crisis information is displayed in Google Search\n.\nSOS Alerts:\nDuring times of natural disasters or wide-spread crisis\nsituations, our SOS Alerts system works to show updates from local, national, or\ninternational authorities. These updates may include emergency phone numbers and websites,\nmaps, translations of useful phrases, donation opportunities, and more. Learn more about\nhow SOS Alerts work\nand how they're part\nof Google's\ncrisis alerts\nthat help in times of floods, wildfires, earthquakes, hurricanes, and other disasters.\nDeduplication systems\nSearches on Google may find thousands or even millions of matching web pages. Some of these\nmay be very similar to each other. In such cases, our systems show only the most relevant\nresults to avoid unhelpful duplication. Learn more about\nhow deduplication works and how to see omitted results\nif desired, when deduplication happens.\nDeduplication also happens with\nfeatured snippets\n.\nIf a web page listing is elevated to become a featured snippet, we don't repeat the listing\nlater on the first page of results. This declutters the results and helps people locate relevant\ninformation more easily.\nExact match domain system", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1978, "chunk_char_end": 3899}
{"id": "20079e72-d8fe-4a8a-81e4-046c8f4a716a", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Our ranking systems consider the words in domain names as one of many factors to determine if\ncontent is relevant to a search. However, our exact match domain system works to ensure we don't\ngive too much credit for content hosted under domains designed to exactly match particular\nqueries. For example, someone might create a domain name containing the words \"best-places-to-eat-lunch\"\nin hopes all those words in the domain name would propel content high in the rankings. Our\nsystem adjusts for this.\nFreshness systems\nWe have various \"query deserves freshness\" systems designed to show fresher content for queries\nwhere it would be expected.  For example, if someone is searching about a movie that's\njust been released, they probably want recent reviews rather than older articles from when\nproduction began. For another example, ordinarily a search for \"earthquake\" might bring back\nmaterial about preparation and resources. However, if an earthquake happened recently, then\nnews articles and fresher content might appear.\nLink analysis systems and PageRank\nWe have various systems that understand how pages link to each other as a way to determine\nwhat pages are about and which might be most helpful in response to a query. Among these is\nPageRank, one of our core ranking systems used when Google first launched. Those curious can\nlearn more by reading the original\nPageRank research paper\nand\npatent\n.\nHow PageRank works has evolved a lot since then, and it continues to be part of our core\nranking systems.\nLocal news systems\nWe have systems that work to identify and surface local sources of news whenever relevant,\nsuch as through\nour \"Top stories\" and \"Local news\" features.\nMUM\nMultitask Unified Model (\nMUM\n)\nis an AI system capable of both understanding and generating language. It's not currently\nused for general ranking in Search but rather for some specific applications such as to\nimprove searches for COVID-19 vaccine information\nand to", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3900, "chunk_char_end": 5858}
{"id": "a05076e0-481d-455b-8cfc-4ed9f7d2f1b8", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "improve featured snippet callouts we display\n.\nNeural matching\nNeural matching\nis an AI system that Google uses to understand representations of concepts in queries and\npages and match them to one another.\nOriginal content systems\nWe have systems to help ensure we are showing original content prominently in search results,\nincluding original reporting\n,\nahead of those who merely cite it. This includes support of a special\ncanonical markup\ncreators can use to help us better understand what is the primary page if a page has been\nduplicated in several places.\nRemoval-based demotion systems\nGoogle has policies that allow the removal of certain types of content. If we process a significant\nvolume of such removals involving a particular site, we use that as a signal to improve our\nresults. In particular:\nLegal removals:\nWhen we receive a significant volume of\nvalid copyright removal requests\ninvolving a given site,\nwe are able to use that\nto demote other content from the site in our results. This way, if there is other\ninfringing content, people are less likely to encounter it versus the original content. We\napply similar demotion signals to complaints involving defamation, counterfeit goods, and\ncourt-ordered removals. In the case of child sexual abuse material (CSAM), we always remove\nsuch content when it is identified and we demote all content from sites with a significant\nproportion of CSAM content.\nPersonal information removals:\nIf we process a significant volume of personal information removals involving a site with\nexploitative removal practices\n,\nwe demote other content from the site in our results.\nWe also look to see\nif the same pattern of behavior is happening with other sites and, if so, apply demotions to\ncontent on those sites. We may apply similar demotion practices for sites that receive a significant\nvolume of removals of content involving\ndoxxing content\n,\nexplicit\npersonal imagery created or shared without consent\n, or\nexplicit", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5859, "chunk_char_end": 7833}
{"id": "93611b67-f0cf-4126-8768-0884f75defe1", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "non-consensual fake content\n.\nPassage ranking system\nPassage ranking\nis an AI system we use to identify individual sections or \"passages\" of a web page to\nbetter understand how relevant a page is to a search.\nRankBrain\nRankBrain\nis an AI system that helps us understand how words are related to concepts. It means we can\nbetter return relevant content even if it doesn't contain all the exact words used in a search,\nby understanding the content is related to other words and concepts.\nReliable information systems\nMultiple systems work in various ways to show the most reliable information possible, such as\nto\nhelp surface more authoritative pages and demote low-quality content\nand\nto elevate quality journalism\n.\nIn cases where reliable information might be lacking, our systems automatically display\ncontent advisories\nabout rapidly-changing topics or when our systems don't have high confidence in the overall\nquality of the results available for the search. These provide tips on how to search in ways\nthat might lead to more helpful results. Learn more about our\napproach to delivering high-quality\ninformation in Search\n.\nReviews system\nThe\nreviews system\naims to better\nreward high quality reviews, content that provides insightful analysis and original research,\nand is written by experts or enthusiasts who know the topic well.\nSite diversity system\nOur site diversity system works so that we generally won't show more than two web page listings\nfrom the same site in our top results, so that no single site tends to dominate all the top\nresults. However, we may still show more than two listings in cases where our systems determine\nit's especially relevant to do so for a particular search. Site diversity generally treats\nsubdomains as part of a root domain. IE: listings from a subdomain (subdomain.example.com) and\nthe root domain (example.com) will all be considered from the same single site. However,", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7834, "chunk_char_end": 9754}
{"id": "fd9de3fa-503e-4caf-b40b-cc9a21d60083", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "sometimes subdomains are treated as separate sites for diversity purposes when deemed relevant\nto do so.\nSpam detection systems\nNo one wants their email inbox filled with spam, which is why spam filters are so helpful.\nSearch faces a similar challenge, because the internet includes huge amounts of spam that,\nif not dealt with, would prevent us from showing the most helpful and relevant results. We\nemploy a range of\nspam detection systems\n,\nincluding\nSpamBrain\n, to deal with\ncontent and behaviors that violate our\nspam policies\n.\nThese systems are constantly\nupdated\nto keep up\nwith the latest ways that the spam threat evolves.\nRetired systems\nThe following systems are noted for historical purposes. They've either been incorporated into\nsuccessor systems or made part of our core ranking systems.\nHelpful content system\nAnnounced in 2022\nas the\n\"Helpful Content Update\", this was a system designed to better ensure people see original, helpful content written by\npeople, for people, in search results, rather than content made primarily to gain search engine\ntraffic. In March 2024, it evolved and\nbecame part of\nour core ranking systems, as our systems use a variety of signals and systems to\npresent helpful results to users.\nHummingbird\nThis was a major improvement to our overall ranking systems made in August 2013. Our\nranking systems have continued to evolve since then, just as they had been evolving before.\nPanda system\nThis was a system designed  to better ensure high-quality and original content was appearing\nin our search results.\nAnnounced in 2011\nand given the nickname of the \"Panda,\" it evolved and became part of our core ranking systems in 2015.\nPenguin system\nThis was a system designed to combat link spam.\nAnnounced in 2012\nand given the nickname of the \"Penguin Update\", it was\nintegrated\ninto our core ranking systems in 2016.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9755, "chunk_char_end": 11743}
{"id": "cd8f7e0f-cd56-40ca-bb03-600dc1ca7999", "url": "https://developers.google.com/search/docs/appearance/ranking-systems-guide", "source_domain": "developers.google.com", "title": "A Guide to Google Search Ranking Systems | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": ", and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-10 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11744, "chunk_char_end": 11954}
{"id": "e943b186-2d1d-423e-9abb-286de49ad992", "url": "https://developers.google.com/search/docs/fundamentals/how-search-works", "source_domain": "developers.google.com", "title": "In-Depth Guide to How Google Search Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "In-Depth Guide to How Google Search Works | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nIn-depth guide to how Google Search works\nGoogle Search is a fully-automated search engine that uses software known as web crawlers that\nexplore the web regularly to find pages to add to our index. In fact, the vast majority of\npages listed in our results aren't manually submitted for inclusion, but are found and added\nautomatically when our web crawlers explore the web. This document explains the stages of how\nSearch works in the context of your website. Having this base knowledge can help you fix\ncrawling issues, get your pages indexed, and learn how to optimize how your site appears in\nGoogle Search.\nA few notes before we get started\nBefore we get into the details of how Search works, it's important to note that Google doesn't\naccept payment to crawl a site more frequently, or rank it higher. If anyone tells you\notherwise, they're wrong.\nGoogle doesn't guarantee that it will crawl, index, or serve your page, even if your page\nfollows the\nGoogle Search Essentials\n.\nIntroducing the three stages of Google Search\nGoogle Search works in three stages, and not all pages make it through each stage:\nCrawling:\nGoogle downloads text, images, and videos\nfrom pages it found on the internet with automated programs called crawlers.\nIndexing:\nGoogle analyzes the text, images, and\nvideo files on the page, and stores the information in the Google index, which is a large\ndatabase.\nServing search results:\nWhen a user searches on", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1940}
{"id": "2b0f4352-2e64-4636-b720-1f776e3e1bfb", "url": "https://developers.google.com/search/docs/fundamentals/how-search-works", "source_domain": "developers.google.com", "title": "In-Depth Guide to How Google Search Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Google, Google returns information that's relevant to the user's query.\nCrawling\nThe first stage is finding out what pages exist on the web. There isn't a central registry of\nall web pages, so Google must constantly look for new and updated pages and add them to its\nlist of known pages. This process is called \"URL discovery\". Some pages are known because\nGoogle has already visited them. Other pages are discovered when Google extracts a link from a\nknown page to a new page: for example, a hub page, such as a category page, links to a new\nblog post. Still other pages are discovered when you submit a list of pages (a\nsitemap\n) for Google to crawl.\nOnce Google discovers a page's URL, it may visit (or \"crawl\") the page to find out what's on\nit. We use a huge set of computers to crawl billions of pages on the web. The program that\ndoes the fetching is called\nGooglebot\n(also known as a crawler, robot, bot, or spider). Googlebot uses an algorithmic process to\ndetermine which sites to crawl, how often, and how many pages to fetch from each site.\nGoogle's crawlers\nare also programmed such that they try not to crawl the site too fast to avoid overloading it.\nThis mechanism is based on the responses of the site (for example,\nHTTP 500 errors mean \"slow down\"\n).\nHowever, Googlebot doesn't crawl all the pages it discovered. Some pages may be\ndisallowed for crawling\nby the\nsite owner, other pages may not be accessible without logging in to the site.\nDuring the crawl, Google renders the page and\nruns any JavaScript it finds\nusing a recent version of\nChrome\n, similar to how your\nbrowser renders pages you visit. Rendering is important because websites often rely on\nJavaScript to bring content to the page, and without rendering Google might not see that\ncontent.\nCrawling depends on whether Google's crawlers can access the site. Some common issues with\nGooglebot accessing sites include:\nProblems with the server handling the site\nNetwork issues", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1941, "chunk_char_end": 3897}
{"id": "4d184f44-53e2-49b7-bedf-93c96dab3f78", "url": "https://developers.google.com/search/docs/fundamentals/how-search-works", "source_domain": "developers.google.com", "title": "In-Depth Guide to How Google Search Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "robots.txt rules preventing Googlebot's access to the page\nIndexing\nAfter a page is crawled, Google tries to understand what the page is about. This stage is\ncalled indexing and it includes processing and analyzing the textual content and key content\ntags and attributes, such as\n<title>\nelements\nand alt attributes,\nimages\n,\nvideos\n, and\nmore.\nDuring the indexing process, Google determines if a page is a\nduplicate of another page on the internet or canonical\n.\nThe canonical is the page that may be shown in search results. To select the canonical, we\nfirst group together (also known as clustering) the pages that we found on the internet that\nhave similar content, and then we select the one that's most representative of the group. The\nother pages in the group are alternate versions that may be served in different contexts, like\nif the user is searching from a mobile device or they're looking for a very specific page from\nthat cluster.\nGoogle also collects signals about the canonical page and its contents, which may be used in\nthe next stage, where we serve the page in search results. Some signals include the language\nof the page, the country the content is local to, and the usability of the page.\nThe collected information about the canonical page and its cluster may be stored in the Google\nindex, a large database hosted on thousands of computers. Indexing isn't guaranteed; not every\npage that Google processes will be indexed.\nIndexing also depends on the content of the page and its metadata. Some common indexing issues\ncan include:\nThe quality of the content on page is low\nRobots\nmeta\nrules disallow indexing\nThe design of the website might make indexing difficult\nServing search results\nWhen a user enters a query, our machines search the index for matching pages and return the\nresults we believe are the highest quality and most relevant to the user's query. Relevancy is\ndetermined by hundreds of factors, which could include information such as the user's", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3898, "chunk_char_end": 5882}
{"id": "f5af494f-6172-41cc-b725-3dd01acb8815", "url": "https://developers.google.com/search/docs/fundamentals/how-search-works", "source_domain": "developers.google.com", "title": "In-Depth Guide to How Google Search Works | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "location, language, and device (desktop or phone). For example, searching for \"bicycle repair\nshops\" would show different results to a user in Paris than it would to a user in Hong Kong.\nBased on the user's query the search features that appear on the search results page also\nchange. For example, searching for \"bicycle repair shops\" will likely show local results and\nno\nimage results\n,\nhowever searching for \"modern bicycle\" is more likely to show image results, but not local\nresults. You can explore the most common UI elements of Google web search in our\nVisual Element gallery\n.\nSearch Console might tell you that a page is indexed, but you don't see it in search results.\nThis might be because:\nThe content on the page is irrelevant to users' queries\nThe quality of the content is low\nRobots\nmeta\nrules prevent serving\nWhile this guide explains how Search works, we are always working on improving our algorithms.\nYou can keep track of these changes by following the\nGoogle Search Central blog\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-03-06 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5883, "chunk_char_end": 7226}
{"id": "a5bd2c95-6e07-4b96-9511-d63809d54109", "url": "https://developers.google.com/search/docs/crawling-indexing", "source_domain": "developers.google.com", "title": "Google Crawling and Indexing | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Google Crawling and Indexing | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nOverview of crawling and indexing topics\nThe topics in this section describe how you can control Google's ability to find and parse\nyour content in order to show it in Search and other Google properties, as well as how to\nprevent Google from crawling specific content on your site.\nHere's a brief description of each page. To get an overview of crawling and indexing, read\nour\nHow Search works\nguide.\nTopics\nFile types indexable by Google\nGoogle can index the content of most types of pages and files. Explore a list of the\nmost common file types that Google Search can index.\nURL structure\nConsider organizing your content so that URLs are constructed logically and in a manner that is most intelligible to humans.\nSitemaps\nTell Google about pages on your site that are new or updated.\nCrawler management\nAsk Google to recrawl your URLs\nManaging crawling of faceted navigation URLs\nLarge site owner's guide to managing your crawl budget\nHow HTTP status codes, and network and DNS errors affect Google Search\nGoogle crawlers\nrobots.txt\nA robots.txt file tells search engine crawlers which pages or files the crawler can or\ncan't request from your site.\nCanonicalization\nLearn what URL canonicalization is and how to tell Google about any duplicate pages on\nyour site in order to avoid excessive crawling. Learn how Google auto-detects duplicate\ncontent, how it treats duplicate content, and how it assigns a\ncanonical URL\nto\nany duplicate page groups found.\nMobile sites", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1962}
{"id": "5edf00d5-e2d1-4d6c-b47d-bd32e5ae3396", "url": "https://developers.google.com/search/docs/crawling-indexing", "source_domain": "developers.google.com", "title": "Google Crawling and Indexing | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Learn how you can optimize your site for mobile devices and ensure that it's crawled and indexed properly.\nAMP\nIf you have AMP pages, learn how AMP works in Google Search.\nJavaScript\nThere are some differences and limitations that you need to account for when designing your\npages and applications to accommodate how crawlers access and render your content.\nPage and content metadata\nUse valid HTML to specify page metadata\nAll\nmeta\ntags that Google understands\nRobots\nmeta\ntag,\ndata-nosnippet\n, and X-Robots-Tag specifications\nBlock indexing with the\nnoindex\nmeta\ntag\nMake your links crawlable\nQualify your outbound links to Google with\nrel\nattributes\nRemovals\nControl what you share with Google\nRemove a page hosted on your site from Google\nRemove images hosted on your page from search results\nKeep redacted information out of Google Search\nSite moves and changes\nRedirects and Google Search\nSite moves\nMinimize A/B testing impact in Google Search\nTemporarily pause or disable a website\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-04 UTC.", "engine": "google", "topic": "crawling_indexing", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1963, "chunk_char_end": 3292}
{"id": "988c51a8-17a6-4d89-9e30-38a82a6c1193", "url": "https://support.google.com/websearch/answer/10622781?hl=en", "source_domain": "support.google.com", "title": "Content policies for Google Search - Google Search Help", "section_path": [], "text": "Content policies for Google Search - Google Search Help\nSkip to main content\nContent policies for Google Search\nGoogle uses automated systems to discover content from the web and other sources. These systems generate search results that provide useful and reliable responses to billions of search requests that we process daily.\nGoogle Search encompasses trillions of web pages, images, videos and other content, and the results might contain material that some could find objectionable, offensive or problematic.\nGoogle’s automated systems help protect against objectionable material. Search results should be useful and relevant, and limit spam responses. We may manually remove content that goes against Google’s content policies, after a case-by-case review from our trained experts. We may also demote sites, such as when we find a high volume of policy content violations within a site.\nFor more information, visit these sites:\nLearn how Google maximizes access to information\nGoogle’s Search Essentials\nSpam policies for Google web search\nOverall content policies for Google Search\nThese policies apply to content surfaced anywhere within Google Search, which includes web results. Web results are web pages, images, videos, news content or other material that Google finds from across the web.\nExpand all\nCollapse all\nChild sexual abuse imagery or exploitation material\nWe block search results that lead to child sexual abuse imagery or material that appears to victimize, endanger, or otherwise exploit children.\nLearn how to report child sexual abuse imagery\n.\nHighly personal information\nGoogle might remove certain personal information that creates significant risks of identity theft, financial fraud, or other specific harms, which include, but aren't limited to, doxxing content, explicit personal images, and involuntary fake pornography.\nLearn how to remove your personal information from Google\n.\nSpam", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1919}
{"id": "48a451fc-5d10-4d79-ad1f-2e34bdf6f4a6", "url": "https://support.google.com/websearch/answer/10622781?hl=en", "source_domain": "support.google.com", "title": "Content policies for Google Search - Google Search Help", "section_path": [], "text": "Google Search detects spam through automated systems. Google takes action against spam, which refers to techniques used to deceive users or manipulate our Search systems into ranking content highly. We issue a\nmanual action\nagainst a site after Google human reviewers determine that its pages are engaged in spam as described in\nGoogle's Search Essentials\n.\nLearn more about our spam policies for Google web search\n.\nWebmaster & site owner requests\nUpon request, we remove content that webmasters or site owners wish to block from our web results.\nLearn how to block access to your content\nand\nhow to remove information from Google\n.\nValid legal requests\nWe remove content or features from our Search results for legal reasons. For example, we remove content if we receive valid notification under the US Digital Millennium Copyright Act (DMCA). We also remove content from local versions of Google, consistent with local law, when we're notified that content is an issue. For example, we remove content that illegally glorifies the Nazi party from our German service, or that unlawfully insults religion from our Indian service. We\ndelist pages\non name queries, based on data-protection requests, under what’s commonly known as the “\nRight to be Forgotten\n” in the EU. We scrutinize these requests to ensure that they're well-founded, and we frequently refuse to remove content when there's no clear basis in law.\nWhen possible, we display a notification that results have been removed and report these removals to\nLumen Database\n, a project run by the\nBerkman Center for Internet and Society\n, which tracks online restrictions on speech. We also disclose certain details about legal removals from our Search results through our\nTransparency Report\n.\nLearn how to make a Legal Removals Request\n.\nSearch features policies", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1920, "chunk_char_end": 3741}
{"id": "a1fa6932-a1b8-417b-8cbf-6709c1187e0b", "url": "https://support.google.com/websearch/answer/10622781?hl=en", "source_domain": "support.google.com", "title": "Content policies for Google Search - Google Search Help", "section_path": [], "text": "These policies apply to many of our search features. Even though these features and the content within them is automatically generated as with web results, how they're presented might be interpreted as having greater quality or credibility than web results. We also don't want predictive or refinement features to unexpectedly shock or offend people. Search features covered by these policies include panels, carousels, enhancements to web listings (such as through\nstructured data\n), predictive and refinement features, and results and features spoken aloud. These policies don't apply to web results.\nExpand all\nCollapse all\nAdvertisements\nWe don’t allow content that primarily advertises products or services, which includes direct calls to purchase, links to other websites, company contact information, and other promotional tactics. We don’t allow sponsored content that’s concealed or misrepresented as independent content.\nLast updated:\nFebruary 2022\nDangerous content\nWe don’t allow content that could directly facilitate serious and immediate harm to people or animals. This includes, but isn't limited to, dangerous goods, services or activities, and self-harm, such as mutilation, eating disorders, or drug abuse.\nLast updated:\nSeptember 2022\nDeceptive practices\nWe don’t allow content or accounts that impersonate any person or organization, misrepresent or hide ownership or primary purpose, or engage in false or coordinated behavior to deceive, defraud, or mislead. This includes, but isn’t limited to:\nMisrepresentation or concealment of country of origin, government or political interest group affiliation.\nDirecting content to users in another country under false premises.\nWorking together in ways that conceal or misrepresent information about relationships or editorial independence.\nThis policy doesn't cover content with certain artistic, educational, historical, documentary, or scientific considerations, or other substantial benefits to the public.\nLast updated:", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3742, "chunk_char_end": 5732}
{"id": "e8740883-abf1-4d83-841b-2c706b868558", "url": "https://support.google.com/websearch/answer/10622781?hl=en", "source_domain": "support.google.com", "title": "Content policies for Google Search - Google Search Help", "section_path": [], "text": "March 2023\nHarassing content\nWe don’t allow harassment, bullying, or threatening content. This includes, but isn't limited to, content which might:\nSingle someone out for malicious abuse.\nThreaten someone with serious harm.\nSexualize someone in an unwanted way.\nExpose private information of someone that could be used to carry out threats.\nDisparage or belittle victims of violence or tragedy.\nDeny an atrocity.\nCause harassment in other ways.\nLast updated:\nOctober 2022\nHateful content\nWe don't allow content that promotes or condones violence, promotes discrimination, disparages or has the primary purpose of inciting hatred against a group. This includes, but isn't limited to, targeting on the basis of race, ethnic origin, religion, disability, age, nationality, veteran status, sexual orientation, gender, gender identity, or any other characteristic that's associated with systemic discrimination or marginalization (like refugee status, immigration status, caste, the impoverished, and the homeless).\nLast updated:\nMarch 2023\nManipulated media\nWe don't allow audio, video, or image content that's been manipulated to deceive, defraud, or mislead by means of creating a representation of actions or events that verifiably didn't take place. This includes if such content would cause a reasonable person to have a fundamentally different understanding or impression, such that it might cause significant harm to groups or individuals, or significantly undermine participation or trust in electoral or civic processes.\nLast updated:\nDecember 2022\nMedical content\nWe don't allow content that contradicts or runs contrary to scientific or medical consensus and evidence-based best practices.\nLast updated:\nSeptember 2023\nRegulated goods\n​We don’t allow content that primarily facilitates the promotion or sale of regulated goods and services such as alcohol, gambling, pharmaceuticals, unapproved supplements, tobacco, fireworks, weapons, or health and medical devices.\nSexually explicit content", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5733, "chunk_char_end": 7733}
{"id": "acbfe412-ed08-4e98-ae5f-e6332d3d302f", "url": "https://support.google.com/websearch/answer/10622781?hl=en", "source_domain": "support.google.com", "title": "Content policies for Google Search - Google Search Help", "section_path": [], "text": "We don’t allow content that contains nudity, graphic sex acts, or sexually explicit material. Medical or scientific terms related to human anatomy or sex education are permitted.\nLast updated:\nMarch 2023\nViolent extremist content\nWe don’t allow content produced by organizations or individuals that use violence against civilians to achieve political, religious, or ideological aims. This includes government-designated terrorist groups and other violent organizations and movements that pose real-world harm to our users. We also don’t allow content that facilitates or promotes the activities of these groups, such as recruiting, sharing materials that could facilitate harm, inciting violence, or celebrating attacks. Content related to violent extremism may be allowed in an educational or documentary context, but enough information must be provided to help people understand the context.\nLast updated:\nSeptember 2023\nViolent & gory content\nWe don’t allow violent or gory content that's shocking, sensational, or gratuitous. This includes graphic depictions of:\nViolent acts or incidents that result in extreme injury or death without historical or educational context\nExposed internal organs or bones without medical or scientific context\nIdentifiable victims of violent deaths\nAnimal abuse or killings outside of standard hunting, food processing, or cultural practices\nLast updated:\nFebruary 2024\nVulgar language & profanity\nWe don’t allow obscenities or profanities that are primarily intended to be shocking, sensational, or gratuitous.\nLast updated:\nMay 2023\nFeature-specific policies\nSome search features have specific policies that are necessary due to the particular ways they work. To learn more, go to the following pages:\nAutocomplete\n(\nLast updated:\nFebruary 2024)\nDictionary boxes\n(\nLast updated:\nFebruary 2023)\nFeatured snippets\n(\nLast updated:\nOctober 2022)\nGoogle Discover\n(\nLast updated:\nFebruary 2024)\nGoogle News\n(\nLast updated:\nSeptember 2023)\nGoogle Podcasts\n(", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7734, "chunk_char_end": 9721}
{"id": "f8f64b9d-4f86-43f6-83b5-f58cb3a6223a", "url": "https://support.google.com/websearch/answer/10622781?hl=en", "source_domain": "support.google.com", "title": "Content policies for Google Search - Google Search Help", "section_path": [], "text": "Last updated:\nMarch 2023)\nImage & video boxes\n(\nLast updated:\nDecember 2023)\nKnowledge Graph & Knowledge Panels\n(\nLast updated:\nJuly 2023)\nProduct listings\n(\nLast updated:\nDecember 2022)\nStructured data\nStructured data for job postings\n(\nLast updated:\nMay 2020)\nUser content on Search\nWas this helpful?\nHow can we improve it?\nYes\nNo\nSubmit\nNeed more help?\nTry these next steps:\nPost to the help community\nGet answers from community members\nfalse\nSearch\nClear search\nClose search\nGoogle apps\nMain menu\n6494754219070324074\ntrue\nSearch Help Center\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\n100334\nfalse\nfalse\nfalse\nfalse", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9722, "chunk_char_end": 10328}
{"id": "9df722f1-114c-4a7d-a8db-94b7b392ccd9", "url": "https://developers.google.com/search/docs/monitor-debug/search-console-start", "source_domain": "developers.google.com", "title": "How To Use Search Console | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "How To Use Search Console | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nGet started with Search Console\nSearch Console\nis a tool from\nGoogle that can help anyone with a website to understand how they are performing on Google Search,\nand what they can do to improve their appearance on search to bring more relevant traffic to their\nwebsites.\nSearch Console provides information on\nhow Google crawls, indexes, and serves websites\n.\nThis can help website owners to monitor and optimize Search performance.\nThere is no need to sign in to the tool every day. If new issues are found by Google on your site,\nyou'll receive an email from Search Console alerting you. But you might want to check your account\naround once every month, or when you make changes to the site's content, to make sure the data is\nstable. Learn more about\nmanaging your site with Search Console\n.\nTo get started, follow these steps:\nVerify site ownership\n. Get access to all of the information Search Console makes available.\nLearn more about\nhow to verify your site ownership\n.\nMake sure Google can find and read your pages\n. The\nIndex coverage report\ngives you an overview of all the pages Google indexed or tried to index in your website.\nReview the list available and try to fix page errors and warnings.\nConsider submitting a sitemap to Search Console\n. Pages from your site can be discovered\nby Google without this step. However, submitting a sitemap using Search Console might speed up\nyour site's discovery. If you decide to submit it through the tool, you'll be able to monitor", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1972}
{"id": "8fa695bc-e890-46ca-abe0-7659d9ebfbd0", "url": "https://developers.google.com/search/docs/monitor-debug/search-console-start", "source_domain": "developers.google.com", "title": "How To Use Search Console | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "information related to it. Learn more about the\nSitemaps report\n.\nMonitor your site's performance\n. The\nSearch performance report\nshows how much traffic you're getting from Google Search, including breakdowns by queries,\npages, and countries. For each of those breakdowns, you can see trends for impressions, clicks,\nand other metrics. If your traffic is going down, consider\ndebugging the traffic drop\n,\nwhich can help you prioritize your efforts.\nIf you'd like to learn Search Console more in-depth, there are broadly two areas you could focus on. We provide here a list of reports\nthat would be most relevant to web developers and those that would be most relevant to SEO specialists, digital marketers,\nand site administrators. While the groups have several intersection points, it's still useful to try and provide\nthe most relevant reports for each group.\nHelpful reports for SEO specialists, digital marketers, and site administrators\nThe following list includes the most useful Search Console reports to help you manage various aspects of\nhow Google Search indexes, crawls, and serves your site.\nLearn if your site has any Google Search manual actions issued against it\n. If a site has a manual action,\nsome or all of that site may not be shown in Google Search results. The\nManual\nActions report\nshows any issues, in which section of your site, and where to learn more about it.\nTemporarily hide pages from Google Search\n. The\nRemovals tool\nis a way to quickly remove content on your site from Google Search results. A successful request lasts\nonly about six months, to allow time for you to find a solution to either allow\nthe content to be seen or to remove it permanently.\nTell Google about a site migration\n. If you move your website from one domain or subdomain to another,\nthe\nChange of Address tool\ntells Google about your change,\nand helps to migrate your Google Search results from your old site to your new site.\nReview issues with your structured data implementation\n. The", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1973, "chunk_char_end": 3965}
{"id": "a5727906-0786-4706-9345-eb3d5b1b953a", "url": "https://developers.google.com/search/docs/monitor-debug/search-console-start", "source_domain": "developers.google.com", "title": "How To Use Search Console | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Rich result status reports\nshow what structured data Google could or couldn't read from your site. You'll find details on\nerrors that prevent showing your pages as rich results, warnings that might limit your appearance,\nand information on how to debug and fix issues.\nHelpful reports for web developers\nThe following reports can help developers build websites that are healthy, findable, and\noptimized for Google Search.\nUnderstand site-wide Search indexing issues\n. The\nIndex Coverage report\nshows which pages have errors, warnings, or are excluded from Search. In addition, it shows the\nnumber of impressions the website pages accrued on Google Search, which helps understanding how\nissues might have affected your organic traffic.\nDebug page-level Search indexing issues\n. The\nURL Inspection tool\nprovides the current index status of website pages and options to test a live URL, to ask Google\nto crawl a specific page, and to view detailed information about the page's loaded resources and\nother information.\nFind and fix threats affecting your site\n. The\nSecurity Issues report\nreport shows warnings when Google finds that a website might have been hacked, or used in ways\nthat could potentially harm a visitor or their device.\nMake sure your website provides a great page experience to your users\n. The\nCore Web Vitals report\nshows how pages perform based on real world usage data, sometimes called field data.\nGet a full list of Search Console reports and tools\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-03-06 UTC.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3966, "chunk_char_end": 5775}
{"id": "756f0d7d-df1a-4526-900e-3605978b2648", "url": "https://search.google.com/search-console/about", "source_domain": "search.google.com", "title": "Google Search Console", "section_path": [], "text": "Google Search Console\nStart now\nImprove your performance on Google Search\nSearch Console tools and reports help you measure your site's Search traffic and performance, fix issues, and make your site shine in Google Search results\nStart now\nImprove your performance on Google Search\nSearch Console tools and reports help you measure your site's Search traffic and performance, fix issues, and make your site shine in Google Search results\nStart now\nOptimize your content with Search Analytics\nSee which queries bring users to your site. Analyze your site's impressions, clicks, and position on Google Search.\nGet your content on Google\nSubmit sitemaps and individual URLs for crawling. Review your index coverage to make sure that Google has the freshest view of your site.\nGet alerted on issues and fix your site\nReceive email alerts when Google identifies issues on your site. See which URLs are affected by these issues and tell Google when you’ve fixed them.\nUnderstand how Google Search sees your pages\nThe URL Inspection tool provides detailed crawl, index, and serving information about your pages, directly from the Google index.\nSearch Console Training\nLearn how to optimize your search appearance on Google and increase organic traffic to your website\n\nIntro to Google Search Console\n\n7 ways to verify site ownership\n\nPerformance reports in Search Console\n\nURL Inspection Tool\nMore videos\nOptimize and enhance your site:\nAMP\nMonitor, test, and track your AMP pages with Search Console tools and reports.\nLearn more\nCore Web Vitals\nMonitor your site's Core Web Vitals on mobile and desktop with Search Console reports.\nLearn more\nRich Results\nYour recipes, jobs, or other structured data can appear as rich results on Google Search. Monitor and improve them using Search Console reports.\nLearn more\nMake your site shine in Google Search results.\nGo to Google Search Console\n\nFollow us on:\n\n\n\nWebmaster resources\nHelp Center\nHelp Forum\nWebmaster Academy\nBusiness essentials", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1988}
{"id": "1e3fe7d1-dc9a-4113-9223-6d63194c83df", "url": "https://search.google.com/search-console/about", "source_domain": "search.google.com", "title": "Google Search Console", "section_path": [], "text": "Google Business Solutions\nGoogle Ads\nDigital Garage\nDeveloper kit\nSearch for Developers\nWeb Fundamentals\nApp Indexing\nRelated products\nGoogle Analytics\nG Suite\nHelp\nGoogle\nPrivacy\nTerms", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1989, "chunk_char_end": 2174}
{"id": "402a9b55-aa73-44a6-b93f-f24f87db30d3", "url": "https://developers.google.com/search/docs/appearance/structured-data", "source_domain": "developers.google.com", "title": "Schema Markup Testing Tool | Google Search Central  |  Google for Developers", "section_path": [], "text": "Schema Markup Testing Tool | Google Search Central  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nHome\nSearch Central\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nTest your structured data\nGoogle recommends that you start with the\nRich Results Test\nto see what Google rich results can be generated for your page. For generic schema validation, use the\nSchema Markup Validator\nto test all types of schema.org markup, without Google-specific validation.\nRich Results Test\nThe official Google tool for testing your structured data to see which Google rich results can be generated by the\nstructured data\non your page. You can also preview how rich results can look in Google Search.\nGo to the Rich Results Test\nSchema Markup Validator\nValidate all Schema.org-based structured data that's embedded in web pages, without Google feature specific warnings.\nGo to the Schema Markup Validator\nLooking for the Structured Data Testing Tool?\nWe removed Google-specific validation from the Structured Data Testing Tool and migrated the tool to a new domain,\nSchema Markup Validator\n. Learn more about the change in our blog post.\nRead the blog post", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1401}
{"id": "5940b46b-cac8-470b-8718-b3d5b89c1e64", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nOrganization (\nOrganization\n) structured data\nMerchant knowledge panel in Google Search results\nAdding organization structured data to your home page can help Google better understand your\norganization's administrative details and disambiguate your organization in search results. Some\nproperties are used behind the scenes to disambiguate your organization from other organizations\n(like\niso6523\nand\nnaics\n), while others can influence visual elements in\nSearch results (such as which\nlogo\nis shown in Search results and your\nknowledge panel\n).\nIf you're a merchant, you can influence more details in your\nmerchant knowledge panel\nand\nbrand profile\n,\nsuch as return policy, address, and contact information. There are no required properties;\ninstead, we recommend adding as many properties that are relevant to your organization.\nHow to add structured data\nStructured data is a standardized format for providing information about a page and classifying the page\ncontent. If you're new to structured data, you can learn more about\nhow structured data works\n.\nHere's an overview of how to build, test, and release structured data.\nAdd as many\nrecommended properties\nthat apply\nto your web page. There are no required properties; instead, add the properties that apply to\nyour content. Based on the format you're using, learn where to\ninsert structured data on the page\n.\nFollow the\nguidelines\n.\nValidate your code using the\nRich Results Test", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1930}
{"id": "08269889-d1c7-46a9-bec4-9cea4fb805e2", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "and fix any critical errors. Consider also fixing any non-critical issues that may be flagged\nin the tool, as they can help improve the quality of your structured data (however, this isn't necessary to be eligible for rich results).\nDeploy a few pages that include your structured data and use the\nURL Inspection tool\nto test how Google sees the page. Be sure that your page is\naccessible to Google and not blocked by a robots.txt file, the\nnoindex\ntag, or\nlogin requirements. If the page looks okay, you can\nask Google to recrawl your URLs\n.\nTo keep Google informed of future changes, we recommend that you\nsubmit a sitemap\n. You can automate this with the\nSearch Console Sitemap API\n.\nExamples\nOrganization\nHere's an example of organization information in JSON-LD code.\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"Organization\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\"https://example.net/profile/example1234\", \"https://example.org/example1234\"],\n\"logo\": \"https://www.example.com/images/logo.png\",\n\"name\": \"Example Corporation\",\n\"description\": \"The example corporation is well-known for producing high-quality widgets\",\n\"email\": \"contact@example.com\",\n\"telephone\": \"+47-99-999-9999\",\n\"address\": {\n\"@type\": \"PostalAddress\",\n\"streetAddress\": \"Rue Improbable 99\",\n\"addressLocality\": \"Paris\",\n\"addressCountry\": \"FR\",\n\"addressRegion\": \"Ile-de-France\",\n\"postalCode\": \"75001\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\"\n}\n</script>\n</head>\n<body>\n</body>\n</html>\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"Organization\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\"https://example.net/profile/example1234\", \"https://example.org/example1234\"],\n\"logo\": \"https://www.example.com/images/logo.png\",\n\"name\": \"Example Corporation\",\n\"description\": \"The example corporation is well-known for producing high-quality widgets\",", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1931, "chunk_char_end": 3930}
{"id": "1a63a98f-f926-4936-af12-b0ce594878a2", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"email\": \"contact@example.com\",\n\"telephone\": \"+47-99-999-9999\",\n\"address\": {\n\"@type\": \"PostalAddress\",\n\"streetAddress\": \"Rue Improbable 99\",\n\"addressLocality\": \"Paris\",\n\"addressCountry\": \"FR\",\n\"addressRegion\": \"Ile-de-France\",\n\"postalCode\": \"75001\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\"\n}\n</script>\n</head>\n<body>\n</body>\n</html>\nOnlineStore\n(subtype of\nOrganization\n) with a shipping policy and return policy\nHere's an example of an online store with both a shipping policy and a return policy in JSON-LD code.\nRefer to the separate\nMerchant return policy markup\ndocumentation for more examples and detailed information for merchant-level standard return policies.\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"OnlineStore\",\n\"name\": \"Example Online Store\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\n\"https://example.net/profile/example12\",\n\"https://example.org/@example34\"\n],\n\"logo\": \"https://www.example.com/assets/images/logo.png\",\n\"contactPoint\": {\n\"contactType\": \"Customer Service\",\n\"email\": \"support@example.com\",\n\"telephone\": \"+47-99-999-9900\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\",\n\"hasShippingService\": [\n{\n\"@type\": \"ShippingService\",\n\"name\": \"shipping to CH and FR\",\n\"description\": \"Shipping to CH 5% of order value, shipping to FR always free\",\n\"fulfillmentType\": \"FulfillmentTypeDelivery\",\n\"shippingConditions\": [\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"CH\"\n},\n\"shippingRate\": {\n\"@type\": \"ShippingRateSettings\",\n\"orderPercentage\": \"0.05\"\n}\n},\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingRate\": {\n\"@type\": \"MonetaryAmount\",\n\"value\": \"0\",\n\"currency\": \"EUR\"\n}\n}\n]", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3931, "chunk_char_end": 5931}
{"id": "5c22ab5a-052e-4d9a-9102-2137426a1d48", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "}\n],\n\"hasMerchantReturnPolicy\": {\n\"@type\": \"MerchantReturnPolicy\",\n\"applicableCountry\": [\n\"FR\",\n\"CH\"\n],\n\"returnPolicyCountry\": \"FR\",\n\"returnPolicyCategory\": \"https://schema.org/MerchantReturnFiniteReturnWindow\",\n\"merchantReturnDays\": 60,\n\"returnMethod\": \"https://schema.org/ReturnByMail\",\n\"returnFees\": \"https://schema.org/FreeReturn\",\n\"refundType\": \"https://schema.org/FullRefund\"\n}\n// Other Organization-level properties\n// ...\n}\n</script>\n</head>\n<body>\n</body>\n</html>\n<html>\n<head>\n<title>About Us</title>\n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"OnlineStore\",\n\"name\": \"Example Online Store\",\n\"url\": \"https://www.example.com\",\n\"sameAs\": [\n\"https://example.net/profile/example12\",\n\"https://example.org/@example34\"\n],\n\"logo\": \"https://www.example.com/assets/images/logo.png\",\n\"contactPoint\": {\n\"contactType\": \"Customer Service\",\n\"email\": \"support@example.com\",\n\"telephone\": \"+47-99-999-9900\"\n},\n\"vatID\": \"FR12345678901\",\n\"iso6523Code\": \"0199:724500PMK2A2M1SQQ228\",\n\"hasShippingService\": [\n{\n\"@type\": \"ShippingService\",\n\"name\": \"shipping to CH and FR\",\n\"description\": \"Shipping to CH 5% of order value, shipping to FR always free\",\n\"fulfillmentType\": \"FulfillmentTypeDelivery\",\n\"shippingConditions\": [\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"CH\"\n},\n\"shippingRate\": {\n\"@type\": \"ShippingRateSettings\",\n\"orderPercentage\": \"0.05\"\n}\n},\n{\n\"@type\": \"ShippingConditions\",\n\"shippingOrigin\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingDestination\": {\n\"@type\": \"DefinedRegion\",\n\"addressCountry\": \"FR\"\n},\n\"shippingRate\": {\n\"@type\": \"MonetaryAmount\",\n\"value\": \"0\",\n\"currency\": \"EUR\"\n}\n}\n]\n}\n],\n\"hasMerchantReturnPolicy\": {\n\"@type\": \"MerchantReturnPolicy\",\n\"applicableCountry\": [\n\"FR\",\n\"CH\"\n],\n\"returnPolicyCountry\": \"FR\",\n\"returnPolicyCategory\": \"https://schema.org/MerchantReturnFiniteReturnWindow\",", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5932, "chunk_char_end": 7917}
{"id": "08b0049c-89cc-4950-98aa-03ba1a0a74f5", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"merchantReturnDays\": 60,\n\"returnMethod\": \"https://schema.org/ReturnByMail\",\n\"returnFees\": \"https://schema.org/FreeReturn\",\n\"refundType\": \"https://schema.org/FullRefund\"\n}\n// Other Organization-level properties\n// ...\n}\n</script>\n</head>\n<body>\n</body>\n</html>\nGuidelines\nYou must follow these guidelines to enable structured data to be eligible for inclusion in Google\nSearch results.\nTechnical guidelines\nSearch Essentials\nGeneral structured data guidelines\nTechnical guidelines\nWe recommend placing this information on your home page, or a single page that describes your\norganization, for example the\nabout us\npage. You don't need to include it on every\npage of your site.\nWe recommend using the most specific schema.org subtype of\nOrganization\nthat matches your organization. For example, if you have an ecommerce site, then we recommend using the\nOnlineStore\nsubtype instead of\nOnlineBusiness\n.\nAnd if your site is about a local business, for example a restaurant or a physical store, then we\nrecommend providing your administrative details using the most specific\nsubtype(s)\nof\nLocalBusiness\nand following the required and recommended fields for\nLocal business\nin addition to the fields recommended in this guide.\nStructured data type definitions\nGoogle recognizes the following properties of an\nOrganization\n.\nTo help Google better understand your page, include as many recommended properties that apply\nto your web page. There are no required properties; instead, add the properties that apply to\nyour organization.\nRecommended properties\naddress\nPostalAddress\nThe address (physical or mailing) of your organization, if applicable. Include all properties that apply to your country. The more\nproperties you provide, the higher quality the result is for users.\nYou can provide multiple addresses if you have a location in multiple cities, states, or countries.\nFor example:\n\"address\"\n:\n[{\n\"@type\"\n:\n\"PostalAddress\"\n,\n\"streetAddress\"\n:\n\"999 W Example St Suite 99 Unit 9\"\n,\n\"addressLocality\"\n:", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7918, "chunk_char_end": 9917}
{"id": "df1eec9f-8636-4475-8ea3-fe31abd85661", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "\"New York\"\n,\n\"addressRegion\"\n:\n\"NY\"\n,\n\"postalCode\"\n:\n\"10019\"\n,\n\"addressCountry\"\n:\n\"US\"\n},{\n\"streetAddress\"\n:\n\"999 Rue due exemple\"\n,\n\"addressLocality\"\n:\n\"Paris\"\n,\n\"postalCode\"\n:\n\"75001\"\n,\n\"addressCountry\"\n:\n\"FR\"\n}]\naddress.addressCountry\nText\nThe country for your postal address, using the two-letter\nISO 3166-1 alpha-2 country code.\naddress.addressLocality\nText\nThe city of your postal address.\naddress.addressRegion\nText\nThe region of your postal address, if applicable. For example, a state.\naddress.postalCode\nText\nThe postal code for your address.\naddress.streetAddress\nText\nThe full street address of your postal address.\nalternateName\nText\nAnother common name that your organization goes by, if applicable.\ncontactPoint\nContactPoint\nThe best way for a user to contact your business, if applicable. Include all support methods available to your users\nfollowing Google recommended\nbest practices\n. For example:\n\"contactPoint\"\n:\n{\n\"@type\"\n:\n\"ContactPoint\"\n,\n\"telephone\"\n:\n\"+9-999-999-9999\"\n,\n\"email\"\n:\n\"contact@example.com\"\n}\ncontactPoint.email\nText\nThe email address to contact your business, if applicable.\nIf you are using a\nLocalBusiness\ntype, specify a primary email address at\nthe\nLocalBusiness\nlevel before using\ncontactPoint\nto specify\nmultiple ways to reach your organization.\ncontactPoint.telephone\nText\nThe phone number to contact your business, if applicable.\nBe sure to include the country code and area code in the phone number.\nIf you are using a\nLocalBusiness\ntype, specify a primary phone number at\nthe\nLocalBusiness\nlevel before using\ncontactPoint\nto specify\nmultiple ways to reach your organization.\ndescription\nText\nA detailed description of your organization, if applicable.\nduns\nText\nThe Dun & Bradstreet DUNS number for identifying your\nOrganization\n, if\napplicable. We encourage using the\niso6523Code\nfield with prefix\n0060:\ninstead.\nemail\nText\nThe email address to contact your business, if applicable.\nfoundingDate\nDate\nThe date your\nOrganization\nwas founded in", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9918, "chunk_char_end": 11909}
{"id": "254200cb-5bff-4fe6-a4e9-38c6e52276a5", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "ISO 8601 date format\n, if applicable.\nglobalLocationNumber\nText\nThe GS1 Global Location Number identifying the location of your\nOrganization\n,\nif applicable.\nhasMerchantReturnPolicy\nRepeated\nMerchantReturnPolicy\nThe return policy of your\nOrganization\n, if applicable. See\nMerchant return policy markup\nfor detailed information on required and optional properties for\nMerchantReturnPolicy\n.\nhasMemberProgram\nRepeated\nMemberProgram\nA member (loyalty) program that you provide, if applicable.\nSee\nMember program markup\nfor detailed information on required and optional properties for\nMemberProgram\n.\nhasShippingService\nRepeated\nShippingService\nThe shipping policy of your\nOrganization\n, if applicable. See\nMerchant shipping policy markup\nfor detailed information on required and optional properties for\nShippingService\n.\niso6523Code\nText\nThe ISO 6523 identifier of your organization, if applicable.\nThe first part of an ISO 6523 identifier is an\nICD\n(International Code Designator)\nwhich defines which identification scheme is used.\nThe second part is the actual identifier. We recommend separating the ICD and the\nidentifier with a colon character (\nU+003A\n). Common ICD values include:\n0060\n: Dun & Bradstreet Data Universal Numbering System (DUNS)\n0088\n: GS1 Global Location Number (GLN)\n0199\n: Legal Entity Identifier (LEI)\nlegalName\nText\nThe registered, legal name of your\nOrganization\n, if applicable and different\nfrom the\nname\nproperty.\nleiCode\nText\nThe identifier for your\nOrganization\nas defined in ISO 17442, if applicable.\nWe encourage using the\niso6523Code\nfield with prefix\n0199:\ninstead.\nlogo\nURL\nor\nImageObject\nA logo that is representative of your organization, if applicable. Adding this property can help Google\nbetter understand which logo you want to show, for example in Search results and knowledge\npanels.\nImage guidelines:\nThe image must be 112x112px, at minimum.\nThe image URL must be crawlable and indexable.\nThe image file format must be\nsupported by Google Images\n.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11910, "chunk_char_end": 13901}
{"id": "1c9b476e-ef16-4834-bc5b-7e62bd7be443", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Make sure the image looks how you intend it to look on a purely white background (for\nexample, if the logo is mostly white or gray, it may not look how you want it to look when\ndisplayed on a white background).\nIf you use the\nImageObject\ntype,\nmake sure that it has a valid\ncontentUrl\nproperty or\nurl\nproperty that follows the same guidelines as a\nURL\ntype.\nnaics\nText\nThe\nNorth American Industry Classification System (NAICS) code\nfor your\nOrganization\n, if applicable.\nname\nText\nThe name of your organization. Use the same\nname\nand\nalternateName\nthat you're using for your\nsite name\n.\nnumberOfEmployees\nQuantitativeValue\nThe number of employees in your\nOrganization\n, if applicable.\nExample with a specific number of employees:\n\"numberOfEmployees\"\n:\n{\n\"@type\"\n:\n\"QuantitativeValue\"\n,\n\"value\"\n:\n2056\n}\nExample with the number of employees in a range:\n\"numberOfEmployees\"\n:\n{\n\"@type\"\n:\n\"QuantitativeValue\"\n,\n\"minValue\"\n:\n100\n,\n\"maxValue\"\n:\n999\n}\nsameAs\nURL\nThe URL of a page on another website with additional information about your organization,\nif applicable. For example, a URL to your organization's profile page on a social media or\nreview site. You can provide multiple\nsameAs\nURLs.\ntaxID\nText\nThe tax ID associated with your\nOrganization\n, if applicable. Make sure\ntaxID\nmatches the country that you provided in the\naddress\nfield.\ntelephone\nText\nA business phone number meant to be the primary contact method for customers, if applicable.\nBe sure to include the country code and area code in the phone number.\nurl\nURL\nThe URL of the website of your organization, if applicable. This helps Google uniquely\nidentify your organization.\nvatID\nText\nThe VAT (Value Added Tax) code associated with your\nOrganization\n, if applicable\nto your country and business. This is an important trust signal for users (for example, users\ncan look up your business in public VAT registries).\nTroubleshooting\nIf you're having trouble implementing or debugging structured data, here are some resources that", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13902, "chunk_char_end": 15893}
{"id": "40740680-4ba4-45c4-99b1-9fc94fb8229e", "url": "https://developers.google.com/search/docs/appearance/structured-data/organization", "source_domain": "developers.google.com", "title": "Organization Schema Markup | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "may help you.\nIf you're using a content management system (CMS) or someone else is taking care of your site,\nask them to help you. Make sure to forward any Search Console message that details the issue to them.\nGoogle does not guarantee that features that consume structured data will show up in search results.\nFor a list of common reasons why Google may not show your content in a rich result, see the\nGeneral Structured Data Guidelines\n.\nYou might have an error in your structured data. Check the\nlist of structured data errors\nand the\nUnparsable structured data report\n.\nIf you received a structured data manual action against your page, the structured data on\nthe page will be ignored (although the page can still appear in Google Search results). To fix\nstructured data issues\n, use the\nManual Actions report\n.\nReview the\nguidelines\nagain to identify if your content isn't compliant\nwith the guidelines. The problem can be caused by either spammy content or spammy markup usage.\nHowever, the issue may not be a syntax issue, and so the Rich Results Test won't be able to\nidentify these issues.\nTroubleshoot missing rich results / drop in total rich results\n.\nAllow time for re-crawling and re-indexing. Remember that it may take several days after\npublishing a page for Google to find and crawl it. For general questions about crawling and indexing, check the\nGoogle Search crawling and indexing FAQ\n.\nPost a question in the\nGoogle Search Central forum\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-12 UTC.", "engine": "google", "topic": "structured_data", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15894, "chunk_char_end": 17694}
{"id": "40379826-868f-4772-9f0b-79fbab4ee342", "url": "https://developers.google.com/search/blog/2022/11/introducing-guide-to-ranking-systems", "source_domain": "developers.google.com", "title": "Introducing our new guide to Google Search ranking systems  |  Google Search Central Blog  |  Google for Developers", "section_path": [], "text": "Introducing our new guide to Google Search ranking systems  |  Google Search Central Blog  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nGoogle Search Central Blog\nHome\nSearch Central\nGoogle Search Central Blog\nSend feedback\nIntroducing our new guide to Google Search ranking systems\nStay organized with collections\nSave and categorize content based on your preferences.\nMonday, November 21, 2022\nOver the years, through blog posts and other public communications, Google has regularly shared\ninformation about our automated ranking systems and how they operate. Now we've created a centralized\npage called\nA guide to Google Search ranking systems\nto make it easier for creators and others to learn about our more notable systems. This new page\nwill also help us as we communicate how our systems work and when we update those systems.\nRanking systems versus updates to those systems\nIn the past, the term \"update\" has often been used as the name of a particular ranking system,\nespecially when a new system has been introduced. For example, our system to measure page\nexperience was\ncalled the \"page experience update\"\nand more recently, our system that evaluates the helpfulness of content was\ncalled the \"helpful content update\n\".\nThis can be confusing when we make improvements to systems, especially if a system is constantly\noperating like our\nhelpful content system\n. A\n\"helpful content update update\" or an \"update to our helpful content update\" is difficult to say,\nmuch less understand!\nGoing forward, we'll be more precise with our wording when differentiating systems from updates.\nYes, we'll still have things like a \"helpful content update\" or a \"product reviews update\", but\nwhen possible we will explain those as updates to the respective systems, such as the \"helpful", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1994}
{"id": "ad065693-4dd8-47a3-b172-593870e13192", "url": "https://developers.google.com/search/blog/2022/11/introducing-guide-to-ranking-systems", "source_domain": "developers.google.com", "title": "Introducing our new guide to Google Search ranking systems  |  Google Search Central Blog  |  Google for Developers", "section_path": [], "text": "content system\" and the \"product reviews system.\" We'll also be refreshing our help pages to\nreflect this terminology change, over time.\nOther useful pages about ranking systems\nThis new page joins others that we've added this year that are related to our ranking systems.\nAs a recap:\nList of Google Search ranking updates\n:\nLists the latest ranking updates made to Google Search that are relevant to website owners.\nGoogle Search's core updates and your website\n:\nExplains how updates to our core ranking systems work and how to assess and perhaps improve\ncontent.\nGoogle Search's helpful content update and your website\n:\nExplains how our helpful content system works and how to assess and perhaps improve content.\nGoogle Search's product reviews update and your website\n:\nExplains how our product reviews system works and how to assess and perhaps improve content.\nGoogle Search spam updates and your site\n:\nExplains how updates to our spam detection systems work and how to assess and perhaps improve\ncontent.\nAlso be sure to check out our new\nGoogle Search Essentials\n:\nfor many people, this is all that's needed to start on the path to success with Google Search. If\nyou have any questions, leave a comment in our\nforum post in the Search Central Help Community\n.\nYou can also send feedback on the documentation page itself by clicking the\nSend Feedback\nbutton.\nPosted by\nDanny Sullivan\n,\nPublic Liaison for Search\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1995, "chunk_char_end": 3726}
{"id": "1a4793b6-6a66-463c-8ca4-10e549a7ec79", "url": "https://blog.google/products/search/google-search-update-march-2024/", "source_domain": "blog.google", "title": "Google Search: New updates to address spam and low-quality results", "section_path": [], "text": "Google Search: New updates to address spam and low-quality results\nNew ways we’re tackling spammy, low-quality content on Search\nMar 05, 2024\n·\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nWe’re enhancing Search so you see more useful information, and fewer results that feel made for search engines.\nElizabeth Tucker\nDirector, Product Management\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nEvery day, people turn to Search to find the best of what the web has to offer. We’ve long had policies and automated systems to fight against spammers, and we work to address emerging tactics that look to game our results with low-quality content. We regularly update those policies and systems to effectively tackle these trends so we can continue delivering useful content and connecting people with high-quality websites.\nToday we’re announcing key changes we’re making to improve the quality of Search and the helpfulness of your results:\nImproved quality ranking\n: We’re making algorithmic enhancements to our core ranking systems to ensure we surface the most helpful information on the web and reduce unoriginal content in search results.\nNew and improved spam policies\n: We’re updating our spam policies to keep the lowest-quality content out of Search, like expired websites repurposed as spam repositories by new owners and obituary spam.\nReducing low-quality, unoriginal results\nIn 2022, we began\ntuning our ranking systems\nto reduce unhelpful, unoriginal content on Search and keep it at very low levels. We're bringing what we learned from that work into the March 2024 core update.\nThis update involves refining some of our core ranking systems to help us better understand if webpages are unhelpful, have a poor user experience or feel like they were created for search engines instead of people. This could include sites created primarily to match very specific search queries.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1890}
{"id": "577b2eb4-c0ee-44cb-ae0e-6514981b6e89", "url": "https://blog.google/products/search/google-search-update-march-2024/", "source_domain": "blog.google", "title": "Google Search: New updates to address spam and low-quality results", "section_path": [], "text": "We believe these updates will reduce the amount of low-quality content on Search and send more traffic to helpful and high-quality sites. Based on our evaluations, we expect that the combination of this update and our previous efforts will collectively reduce low-quality, unoriginal content in search results by 40%.\nUpdate April 26, 2024:\nAs of April 19, we’ve completed the rollout of these changes. You’ll now see 45% less low-quality, unoriginal content in search results versus the 40% improvement we expected across this work.\nKeeping more spam out of your results\nFor decades, we’ve relied on advanced\nspam-fighting systems\nand\nspam policies\nto prevent the lowest-quality content from appearing in search results — and that work continues.\nWe’re making several\nupdates\nto our spam policies to better address new and evolving abusive practices that lead to unoriginal, low-quality content showing up on Search. We’ll take action on more types of these manipulative behaviors starting today. While our ranking systems keep many types of low-quality content from ranking highly on Search, these updates allow us to take more\ntargeted action\nunder our spam policies.\nScaled content abuse\nWe’ve long had a policy against using automation to generate low-quality or unoriginal content at scale with the goal of manipulating search rankings. This policy was originally designed to address instances of content being generated at scale where it was clear that automation was involved.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1891, "chunk_char_end": 3375}
{"id": "b7b76484-e4e9-4cc1-ba61-3c5fe9d15d0e", "url": "https://blog.google/products/search/google-search-update-march-2024/", "source_domain": "blog.google", "title": "Google Search: New updates to address spam and low-quality results", "section_path": [], "text": "Today, scaled content creation methods are more sophisticated, and whether content is created purely through automation isn't always as clear. To better address these techniques, we’re strengthening our policy to focus on this abusive behavior — producing content at scale to boost search ranking — whether automation, humans or a combination are involved. This will allow us to take action on more types of content with little to no value created at scale, like pages that pretend to have answers to popular searches but fail to deliver helpful content.\nSite reputation abuse\nSometimes, websites that have their own great content may also host low-quality content provided by third parties with the goal of capitalizing on the hosting site's strong reputation. For example, a third party might publish payday loan reviews on a trusted educational website to gain ranking benefits from the site. Such content ranking highly on Search can confuse or mislead visitors who may have vastly different expectations for the content on a given website.\nWe’ll now consider very low-value, third-party content produced primarily for ranking purposes and without close oversight of a website owner to be spam. We're publishing this policy two months in advance of enforcement on May 5, to give site owners time to make any needed changes.\nExpired domain abuse\nOccasionally, expired domains are purchased and repurposed with the primary intention of boosting search ranking of low-quality or unoriginal content. This can mislead users into thinking the new content is part of the older site, which may not be the case. Expired domains that are purchased and repurposed with the intention of boosting the search ranking of low-quality content are now considered spam.", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3376, "chunk_char_end": 5130}
{"id": "7926a8ca-c2d0-4a6b-bd21-73d0bc51aafb", "url": "https://blog.google/products/search/google-search-update-march-2024/", "source_domain": "blog.google", "title": "Google Search: New updates to address spam and low-quality results", "section_path": [], "text": "Search helps people with billions of questions every day, but there will always be areas where we can improve. We’ll continue to work hard at keeping low-quality content on Search to low levels, and showing more information created to help people.\nPOSTED IN:\nRelated stories\nAI\nThe latest AI news we announced in November\nBy\nKeyword Team\nDec 05, 2025\nGemini\n15 examples of what Gemini 3 can do\nBy\nZahra Thompson\nDec 05, 2025\nSearch\nYear in Search 2025: What and how we searched this year\nBy\nSimon Rogers\nDec 04, 2025\nSearch\nUse Circle to Search and Google Lens to spot scam messages.\nDec 02, 2025\nSearch\nGemini 3 and Nano Banana Pro in Search are coming to more countries around the world.\nDec 01, 2025\nGemini\n48 tips and prompts for holiday planning, travel and more\nBy\nMolly McHugh-Johnson\nNov 21, 2025\n.\nJump to position 1\nJump to position 2\nJump to position 3\nJump to position 4\nJump to position 5\nJump to position 6\nLet’s stay in touch. Get the latest news from Google in your inbox.\nSubscribe\nNo thanks", "engine": "google", "topic": "google_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5131, "chunk_char_end": 6139}
{"id": "e8e1d9b7-e4af-4289-9f5b-a8562709b63e", "url": "https://yandex.com/support/webmaster/en/", "source_domain": "yandex.com", "title": "Help for webmasters | Webmaster", "section_path": [], "text": "Help for webmasters | Webmaster\nHelp for webmasters\nHelp for webmasters\nGetting started\nPopular\nGetting started\nYandex recommendations\nThis guide\ntells you how to make a site that is ranked correctly by the Yandex search engine.\nUse\nYandex Wordstat\nto view search query statistics.\nPopular\nHow the robot works\nHow Yandex indexes sites\nYandex robots\nMeta tags that Yandex takes into account\nIndexing options\nHow do I make my site appear in search results?\nUsing the Sitemap file\nUsing robots.txt\nAppearance in search results\nCreating Sitelinks\nFavicon\nSnippet\nYandex Webmaster\nAbout the service\nSite management rights\nCheck page status\nYou can also go to\nUseful tools\nReindex pages\nRemoving pages from search results\nReport spam\nMy sites\nServices\nYandex Webmaster\nSite search\nYandex Metrica\nYandex Direct\nYandex Advertising Network\nAll services\nWas the article helpful?\nYes\nNo\nNext\nWorking with Yandex Webmaster", "engine": "yandex", "topic": "other_engines", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 910}
{"id": "bafae456-d8fe-4ba3-9629-5bee1838e487", "url": "https://webmaster.yandex.com/site/optimization/seo-guide/", "source_domain": "webmaster.yandex.com", "title": "Optimisation advice â Yandex Webmaster", "section_path": [], "text": "Optimisation advice â Yandex Webmaster\nYou are using an old browser version. We recommend installing a modern browser.\nYandex Browser\nOpera\nMozilla Firefox", "engine": "yandex", "topic": "other_engines", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 157}
{"id": "4de7f3a6-f0b7-4ad1-bac5-5cc724353d17", "url": "https://www.bing.com/webmasters/help/refreshed-webmaster-tools-7c7d2533", "source_domain": "www.bing.com", "title": "Bing Webmaster Tools - Help Documentation", "section_path": [], "text": "Bing Webmaster Tools - Help Documentation", "engine": "bing", "topic": "bing_core", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 41}
{"id": "6ec2d54a-60d9-45dc-a10e-99f7e0295c5b", "url": "https://schema.org/docs/developers.html", "source_domain": "schema.org", "title": "Developers - Schema.org", "section_path": [], "text": "Developers - Schema.org\nNote\n: You are viewing the development\nversion of\nSchema.org\n.\nSee\nhow we work\nfor more details.\nSchema.org\nDocs\nSchemas\nValidate\nAbout\nSchema.org for Developers\nThis page provides basic developer-oriented information about Schema.org. In particular it gives access to machine-readable representations of our schemas.\nMachine Readable Term Definitions\nThere are various ways to access machine-readable definitions of Schema.org terms.\nThe underlying source files are all available in our\nGithub repository\n. Editorial work is conducted in Turtle format, and defined using an approach\nbased on\nRDF Schema\n.\nPeriod releases are published on the Schema.org site, after staging on staging.schema.org. The corresponding data files are linked below, and are also available\nat Github\n.\nMachine-readable definitions of individual terms are also available as JSON-LD, embedded into the term page HTML. Note that in many cases, it is more useful to consider schema.org as a whole.\nAs noted in the\nFAQ\n,\nthe Schema.org URI is sometimes written with 'http', and sometimes with 'https'. Consuming applications are encouraged to accept both representations, and we now generate downloads in both \"flavours\" as a convenience. In editorial work \"on disk\" we use the 'https' form. However\nother sites and applications are likely to be using 'http', especially in\nRDF representations. Although we encourage a move towards 'https', note in\nparticular that the JSON-LD context file for Schema.org uses 'http', and we are unlikely to change this without a long warning period due to the impact it would have on deployed installations. Feedback on this is welcome, e.g.\nin this\ngithub issue\n.", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1694}
{"id": "aabb1c25-1439-4928-bfe1-621eea90425e", "url": "https://schema.org/docs/developers.html", "source_domain": "schema.org", "title": "Developers - Schema.org", "section_path": [], "text": "The JSON-LD context is available from https://schema.org/docs/jsonldcontext.json. Schema.org is typically deployed in JSON-LD 1.0, but the location of our context file is exposed via JSON-LD 1.1 conventions, as an HTTP link header (the commandline tool 'curl' can be useful, e.g. used with  -I -i arguments). We serve the same context description regardless of whether\nthe context is retrieved with http or https.\nVocabulary Definition Files\nTo assist developers, files containing the definition of the current version of the Schema.org vocabulary is available for download in common RDF formats. In addition to the links below, older release versions can be found (under\ndata/releases/\n) in\nGitHub\n.\nSelect the file and format required and click Download.  The CSV format downloads are split across two files:\nTypes\nincludes definitions of Types and Enumeration Values, including lists of associated properties;\nProperties\ncontains property definitions.\nFile\nschemaorg-current-https\ncontains the definition of all terms in, all sections of, the vocabulary.  The file\nschemaorg-all-https\ncontains the definition of all terms in, all sections of, the vocabulary,\nplus\nterms retired from the vocabulary (\nSee the\nattic section\nfor details\n).\nFor those preferring to use http based definitions of Schema.org terms, these equivalent definitions are available in the\nschemaorg-current-http\nand\nschemaorg-all-http\nfiles. For more information on using https or http based terms see the\nFAQ\nfor details.\nFile:\nschemaorg-current-https\nschemaorg-all-https\nschemaorg-current-http\nschemaorg-all-http\nFormat:\nJSON-LD\nTurtle\nTriples\nQuads\nRDF/XML\nCSV\nFor:\nTypes\nProperties\nExperimental\nThe following representations are\nexperimental\nand may change or be removed in future releases.\nD3 RDFS in JSON-LD\nA simplification of the Schema.org type hierarchy, in which each type has at most one super-type, represented\nin a hybrid format that combines JSON-LD,\nRDFS\nand\nD3\n:\ntree.jsonld\n.", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1695, "chunk_char_end": 3661}
{"id": "4b8ccf1b-bccb-4540-b006-2906bd4a35a9", "url": "https://schema.org/docs/developers.html", "source_domain": "schema.org", "title": "Developers - Schema.org", "section_path": [], "text": "This file is made available to support developers using the\nD3\nJavaScript library for manipulating documents based on data.\nIt uses JSON-LD to declare that D3's default \"children\" JSON field represents \"subClassOf\" relationships, but expressed in the\nreverse direction (\nexample usage\n).\nOWL\nAs an experimental feature, an\nOWL\ndefinition file\nschemaorg.owl\nis available. It includes\nthe core and all current extensions to the vocabulary.\nThe structure of the file differs from the above vocabulary definition files, in that\nschema:domainIncludes\nand\nschema:rangeIncludes\nvalues are converted into\nrdfs:domain\nand\nrdfs:range\nvalues using\nowl:unionOf\nto capture the multiplicity of values.\nIncluded in the range values are the, implicit within the vocabulary, default values of\nText\n,\nURL\n,\nand\nRole\n.\nThis file has been made available to enable the representation of the vocabulary in some OWL-based modeling tools.\nThe mapping into OWL is an approximation, and should not be considered an authoritative definition for Schema.org's terms; see\ndatamodel page\nfor details.\nAs an experimental feature, there are no expectations as to its interpretation by any third party tools.\nTerms and conditions\nâ¢\nSchema.org\nâ¢\nV29.3\n|\n2025-09-04", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3662, "chunk_char_end": 4895}
{"id": "5953af7e-fabb-45c6-8d28-6d8bc4e19792", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "Getting Started - schema.org\nNote\n: You are viewing the development\nversion of\nSchema.org\n.\nSee\nhow we work\nfor more details.\nSchema.org\nDocs\nSchemas\nValidate\nAbout\nGetting started with schema.org using Microdata\nMost webmasters are familiar with HTML tags on their pages. Usually, HTML tags tell the browser how to display the information included in the tag. For example,\n<h1>Avatar</h1>\ntells the browser to display the text string \"Avatar\" in a heading 1 format. However, the HTML tag doesn't give any information about what that text string means—\"Avatar\" could refer to the hugely successful 3D movie, or it could refer to a type of profile picture—and this can make it more difficult for search engines to intelligently display relevant content to a user.\nSchema.org provides a collection of shared vocabularies webmasters can use to mark up their pages in ways that can be understood by the major search engines: Google, Microsoft, Yandex and Yahoo!\nYou use the\nschema.org\nvocabulary along with the\nMicrodata\n,\nRDFa\n, or\nJSON-LD\nformats to add information to your Web content.\nThis guide will help get you up to speed with Microdata and schema.org so that you can start adding markup to your web pages.\nAlthough this guide focuses on\nMicrodata\n,\nmost examples on the\nschema.org\nsite show examples in RDFa and JSON-LD too.\nThe basic ideas (types, properties etc.) introduced here are relevant beyond Microdata - take a look at the\nexamples to see how the details compare.\nHow to mark up your content using microdata\nWhy use microdata?\nitemscope and itemtype\nitemprop\nEmbedded items\nUsing the schema.org vocabulary\nschema.org types and properties\nExpected types, text, and URLs\nTesting your markup\nAdvanced topic: Machine-understandable versions of information\nDates, times, and durations\nEnumerations and canonical references\nMissing/implicit information\nExtending schema.org\n1. How to mark up your content using microdata\n1a. Why use microdata?", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1952}
{"id": "1eaea698-d27c-4ea8-9e8c-da409751ebaf", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "Your web pages have an underlying meaning that people understand when they read the web pages. But search engines have a limited understanding of what is being discussed on those pages. By adding additional tags to the HTML of your web pages—tags that say, \"Hey search engine, this information describes this specific movie, or place, or person, or video\"—you can help search engines and other applications better understand your content and display it in a useful, relevant way. Microdata is a set of tags, introduced with HTML5, that allows you to do this.\n1b. itemscope and itemtype\nLet's start with a concrete example. Imagine you have a page about the movie Avatar—a page with a link to a movie trailer, information about the director, and so on. Your HTML code might look something like this:\n<div>\n<h1>Avatar</h1>\n<span>Director: James Cameron (born August 16, 1954)</span>\n<span>Science fiction</span>\n<a href=\"../movies/avatar-theatrical-trailer.html\">Trailer</a>\n</div>\nTo begin, identify the section of the page that is \"about\" the movie Avatar. To do this, add the\nitemscope\nattribute to the HTML tag that encloses information about the item, like this:\n<div\nitemscope\n>\n<h1>Avatar</h1>\n<span>Director: James Cameron (born August 16, 1954) </span>\n<span>Science fiction</span>\n<a href=\"../movies/avatar-theatrical-trailer.html\">Trailer</a>\n</div>\nBy adding\nitemscope\n, you are specifying that the HTML contained in the\n<div>...</div>\nblock is about a particular item.\nBut it's not all that helpful to specify that there is an item being discussed without specifying what kind of an item it is. You can specify the type of item using the\nitemtype\nattribute immediately after the\nitemscope\n.\n<div itemscope\nitemtype=\"https://schema.org/Movie\"\n>\n<h1>Avatar</h1>\n<span>Director: James Cameron (born August 16, 1954)</span>\n<span>Science fiction</span>\n<a href=\"../movies/avatar-theatrical-trailer.html\">Trailer</a>\n</div>", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1953, "chunk_char_end": 3882}
{"id": "209be9f9-370a-4941-b463-d504927bfb38", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "This specifies that the item contained in the div is in fact a Movie, as defined in the schema.org type hierarchy. Item types are provided as URLs, in this case\nhttps://schema.org/Movie\n.\nBack to top\n1c. itemprop\nWhat additional information can we give search engines about the movie Avatar? Movies have interesting properties such as actors, director, ratings. To label properties of an item, use the\nitemprop\nattribute. For example, to identify the director of a movie, add\nitemprop=\"director\"\nto the element enclosing the director's name. (There's a full list of all the properties you can associate with a movie at https://schema.org/Movie.)\n<div itemscope itemtype =\"https://schema.org/Movie\">\n<h1\nitemprop=\"name\"\n>Avatar</h1>\n<span>Director: <span\nitemprop=\"director\"\n>James Cameron</span> (born August 16, 1954)</span>\n<span\nitemprop=\"genre\"\n>Science fiction</span>\n<a href=\"../movies/avatar-theatrical-trailer.html\"\nitemprop=\"trailer\"\n>Trailer</a>\n</div>\nNote that we have added additional\n<span>...</span>\ntags to attach the\nitemprop\nattributes to the appropriate text on the page.\n<span>\ntags don't change the way pages are rendered by a web browser, so they are a convenient HTML element to use with\nitemprop\n.\nSearch engines can now understand not just that http://www.avatarmovie.com is a URL, but also that it's the URL for the trailer for the science-fiction movie Avatar, which was directed by James Cameron.\nBack to top\n1d. Embedded items\nSometimes the value of an item property can itself be another item with its own set of properties. For example, we can specify that the director of the movie is an item of type Person and the Person has the properties\nname\nand\nbirthDate\n. To specify that the value of a property is another item, you begin a new\nitemscope\nimmediately after the corresponding\nitemprop\n.\n<div itemscope itemtype =\"https://schema.org/Movie\">\n<h1 itemprop=\"name\">Avatar</h1>\n<div\nitemprop=\"director\" itemscope itemtype=\"https://schema.org/Person\"\n>", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3883, "chunk_char_end": 5866}
{"id": "74a0ff5a-b3a5-4a01-98b2-1ea29d947349", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "Director: <span itemprop=\"name\">James Cameron</span> (born <span\nitemprop=\"birthDate\"\n>August 16, 1954</span>)\n</div>\n<span itemprop=\"genre\">Science fiction</span>\n<a href=\"../movies/avatar-theatrical-trailer.html\" itemprop=\"trailer\">Trailer</a>\n</div>\nBack to top\n2. Using the schema.org vocabulary\n2a. schema.org types and properties\nNot all web pages are about movies and people—in addition to the Movie and Person types described in section 1, schema.org describes a variety of other item types, each of which has its own set of properties that can be used to describe the item.\nThe broadest item type is\nThing\n, which has four properties:\nname\n,\ndescription\n,\nurl\n, and\nimage\n. More specific types share properties with broader types. For example, a\nPlace\nis a more specific type of Thing, and a\nLocalBusiness\nis a more specific type of Place. More specific items inherit the properties of their parent. (Actually, a LocalBusiness is a more specific type of Place\nand\na more specific type of Organization, so it inherits properties from both parent types.)\nHere's a set of commonly used item types:\nCreative works:\nCreativeWork\n,\nBook\n,\nMovie\n,\nMusicRecording\n,\nRecipe\n,\nTVSeries\n...\nEmbedded non-text objects:\nAudioObject\n,\nImageObject\n,\nVideoObject\nEvent\nOrganization\nPerson\nPlace\n,\nLocalBusiness\n,\nRestaurant\n...\nProduct\n,\nOffer\n,\nAggregateOffer\nReview\n,\nAggregateRating\nYou can also see a\nfull list of all item types\n, listed on a single page.\nBack to top\n2b. Expected types, text, and URLs\nHere are a few notes to keep in mind when adding schema.org markup to your web pages.\nMore is better, except for hidden text.\nIn general, the more content you mark up, the better. However, as a general rule, you should mark up only the content that is visible to people who visit the web page and not content in hidden div's or other hidden page elements.\nExpected types vs text.", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5867, "chunk_char_end": 7746}
{"id": "84d4d552-c9ba-4e2d-a0f8-52fe6be434f7", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "When browsing the schema.org types, you will notice that many properties have \"expected types\". This means that the value of the property can itself be an embedded item (see section 1d: embedded items). But this is not a requirement—it's fine to include just regular text or a URL. In addition, whenever an expected type is specified, it is also fine to embed an item that is a child type of the expected type. For example, if the expected type is Place, it's also OK to embed a LocalBusiness.\nUsing the url property.\nSome web pages are about a specific item. For example, you may have a web page about a single person, which you could mark up using the Person item type. Other pages have a collection of items described on them. For example, your company site could have a page listing employees, with a link to a profile page for each person. For pages like this with a collection of items, you should mark up each item separately (in this case as a series of Persons) and add the url property to the link to the corresponding page for each item, like this:\n<div itemscope itemtype=\"https://schema.org/Person\">\n<a href=\"alice.html\"\nitemprop=\"url\"\n>Alice Jones</a>\n</div>\n<div itemscope itemtype=\"https://schema.org/Person\">\n<a href=\"bob.html\"\nitemprop=\"url\"\n>Bob Smith</a>\n</div>\nBack to top\n2c. Testing your markup\nMuch like a web browser is important for testing changes to your web page layout, and a code compiler is important for testing code you write, you should also test your schema.org markup to make sure it is implemented correctly. Google provides a rich snippets testing tool, which you can use to test your markup and identify any errors.\n3. Advanced topic: Machine-understandable versions of information\nMany pages can be described using only the\nitemscope\n,\nitemtype\n, and\nitemprop\nattributes (described in section 1) along with the types and properties defined on schema.org (described in section 2).", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7747, "chunk_char_end": 9667}
{"id": "cc35b6ec-8feb-48fc-83ad-55de3ddaa290", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "However, sometimes an item property is difficult for a machine to understand without additional disambiguation. This section describes how you can provide machine-understandable versions of information when marking up your pages.\nDates, times, and durations: use the\ntime\ntag with\ndatetime\nEnumerations and canonical references: use the\nlink\ntag with\nhref\nMissing/implicit information: use the\nmeta\ntag with\ncontent\n.\nBack to top\n3a. Dates, times, and durations: use the time tag with datetime\nDates and times can be difficult for machines to understand. Consider the date \"04/01/11\". Does it mean January 11, 2004? January 4, 2011? Or April 1, 2011? To make dates unambiguous, use the\ntime\ntag along with the\ndatetime\nattribute. The value of the\ndatetime\nattribute is the date specified using\nYYYY-MM-DD\nformat. The HTML code below specifies the date unambiguously as April 1, 2011.\n<time datetime=\"2011-04-01\">\n04/01/11\n</time>\nYou can also specify a time within a day, using the\nhh:mm\nor\nhh:mm:ss\nformat. Times are prefixed with the letter\nT\nand can be provided along with a date, like this:\n<time datetime=\"2011-05-08T19:30\">\nMay 8, 7:30pm\n</time>\nLet's see this in context. Here's some HTML describing a concert taking place on May 8, 2011. The Event markup includes the name of the event, a description, and the date of the event.\n<div\nitemscope itemtype=\"https://schema.org/Event\"\n>\n<div\nitemprop=\"name\"\n>Spinal Tap</div>\n<span\nitemprop=\"description\"\n>One of the loudest bands ever\nreunites for an unforgettable two-day show.</span>\nEvent date:\n<time itemprop=\"startDate\" datetime=\"2011-05-08T19:30\">\nMay 8, 7:30pm\n</time>\n</div>\nDurations can be specified in an analogous way using the\ntime\ntag with the\ndatetime\nattribute. Durations are prefixed with the letter\nP\n(stands for \"period\"). Here's how you can specify a recipe cook time of 1 Â½ hours:\n<time\nitemprop=\"cookTime\"\ndatetime=\"PT1H30M\"\n>1 1/2 hrs\n</time>\nH\nis used to designate the number of hours, and\nM", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9668, "chunk_char_end": 11638}
{"id": "5528ac61-93b4-4702-af82-7896531cd2b5", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "is used to designate the number of minutes.\nThe date, time, and duration standards are specified by the\nISO 8601 date/time standard\n.\nBack to top\n3b. Enumerations and canonical references: use link with href\nEnumerations\nSome properties can take only a limited set of possible values. Programmers often call these \"enumerations.\" For example, an online store with an item for sale could use the\nOffer\nitem type to specify the details of the offer. The\navailability\nproperty can typically have one of only a few possible values—\nIn stock\n,\nOut of stock\n,\nPre-order\n, and so on. Much like item types are specified as URLs, possible values for an enumeration on schema.org can also be specified as URLs.\nHere is an item for sale, marked up with the Offer type and relevant properties:\n<div itemscope itemtype=\"https://schema.org/Offer\">\n<span itemprop=\"name\">Blend-O-Matic</span>\n<span itemprop=\"price\">$19.95</span>\n<span itemprop=\"availability\">Available today!</span>\n</div>\nAnd here is the same item, but using\nlink\nand\nhref\nto unambiguously specify the availability as one of the permitted values:\n<div itemscope itemtype=\"https://schema.org/Offer\">\n<span itemprop=\"name\">Blend-O-Matic</span>\n<span itemprop=\"price\">$19.95</span>\n<link itemprop=\"availability\" href=\"https://schema.org/InStock\"/>\nAvailable today!\n</div>\nSchema.org provides enumerations for a handful of properties—typically wherever there are a limited number of typical values for a property, there is a corresponding enumeration specified in schema.org. In this case, the possible values for\navailability\nare specified in\nItemAvailability\n.\nCanonical references\nTypically, links are specified using the\n<a>\nelement. For example, the following HTML links to the Wikipedia page for the book Catcher in the Rye.\n<div itemscope itemtype=\"https://schema.org/Book\">\n<span itemprop=\"name\">The Catcher in the Rye</span>—\nby <span itemprop=\"author\">J.D. Salinger</span>\nHere is the book's <a\nitemprop=\"url\"", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11639, "chunk_char_end": 13607}
{"id": "26511e89-a2d2-49c3-aff7-dd0212a1b8d7", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": "href=\"http://en.wikipedia.org/wiki/The_Catcher_in_the_Rye\">Wikipedia page</a>.\n</div>\nAs you can see,\nitemprop=\"url\"\ncan be used to specify a link to a page on another site (in this case Wikipedia) discussing the same item. Links to 3rd party sites can help search engines to better understand the item you are describing on your web page.\nHowever, you might not want to add a visible link on your page. In this case, you can use a\nlink\nelement instead, as follows:\n<div itemscope itemtype=\"https://schema.org/Book\">\n<span itemprop=\"name\">The Catcher in the Rye</span>—\n<link itemprop=\"url\" href=\"http://en.wikipedia.org/wiki/The_Catcher_in_the_Rye\" />\nby <span itemprop=\"author\">J.D. Salinger</span>\n</div>\nBack to top\n3c. Missing/implicit information: use the meta tag with content\nSometimes, a web page has information that would be valuable to mark up, but the information can't be marked up because of the way it appears on the page. The information may be conveyed in an image (for example, an image used to represent a rating of 4 out of 5) or a Flash object (for example, the duration of a video clip), or it may be implied but not stated explicitly on the page (for example, the currency of a price).\nIn these cases, use the\nmeta\ntag along with the\ncontent\nattribute to specify the information. Consider this example—the image shows users a 4 out of 5 star rating:\n<div itemscope itemtype=\"https://schema.org/Offer\">\n<span itemprop=\"name\">Blend-O-Matic</span>\n<span itemprop=\"price\">$19.95</span>\n<img src=\"four-stars.jpg\" />\nBased on 25 user ratings\n</div>\nHere is the example again with the rating information marked up.\n<div itemscope itemtype=\"https://schema.org/Offer\">\n<span itemprop=\"name\">Blend-O-Matic</span>\n<span itemprop=\"price\">$19.95</span>\n<div\nitemprop=\"reviews\" itemscope itemtype=\"https://schema.org/AggregateRating\"\n>\n<img src=\"four-stars.jpg\" />\n<meta itemprop=\"ratingValue\" content=\"4\" />\n<meta itemprop=\"bestRating\" content=\"5\" />\nBased on <span\nitemprop=\"ratingCount\"", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13608, "chunk_char_end": 15607}
{"id": "177458cf-4ab5-4416-8f88-bb5192c5c4c6", "url": "https://schema.org/docs/gs.html", "source_domain": "schema.org", "title": "Getting Started - schema.org", "section_path": [], "text": ">25</span> user ratings\n</div>\n</div>\nThis technique should be used sparingly. Only use\nmeta\nwith content for information that cannot otherwise be marked up.\nBack to top\n3d. Extending schema.org\nMost sites and organizations will not have a reason to extend schema.org. However, schema.org offers the ability to specify additional properties or sub-types to existing types. If you are interested in doing this, read more about the\nschema.org extension mechanism\n.\nTerms and conditions\nâ¢\nSchema.org\nâ¢\nV29.3\n|\n2025-09-04", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15608, "chunk_char_end": 16129}
{"id": "6e4b31ae-95c8-48f1-81ba-bdadf727f5df", "url": "https://schema.org/docs/schemas.html", "source_domain": "schema.org", "title": "Schemas - Schema.org", "section_path": [], "text": "Schemas - Schema.org\nNote\n: You are viewing the development\nversion of\nSchema.org\n.\nSee\nhow we work\nfor more details.\nSchema.org\nDocs\nSchemas\nValidate\nAbout\nSchemas\nOrganization of Schemas\nThe schemas are a set of 'types', each associated with a set of properties. The types are arranged in a hierarchy.\nThe vocabulary currently consists of 817 Types, 1518 Properties 14 Datatypes, 94 Enumerations and 521 Enumeration members.\nBrowse the full hierarchy in HTML:\nOne page per type\nFull list of types, shown on one page\nLook up a term using the Term\nFinder\n:\nOr you can jump directly to a commonly used type:\nCreative works:\nCreativeWork\n,\nBook\n,\nMovie\n,\nMusicRecording\n,\nRecipe\n,\nTVSeries\n...\nEmbedded non-text objects:\nAudioObject\n,\nImageObject\n,\nVideoObject\nEvent\nHealth and medical types\n: notes on the health and medical types under\nMedicalEntity\n.\nOrganization\nPerson\nPlace\n,\nLocalBusiness\n,\nRestaurant\n...\nProduct\n,\nOffer\n,\nAggregateOffer\nReview\n,\nAggregateRating\nAction\nSee also the\nreleases\npage for recent updates and project history.\nWe also have a small set of\nprimitive data types\nfor numbers, text, etc. More details about the data model, etc. are available\nhere\n.\nDeveloper information / Download Machine Readable files (RDF, JSON-LD, etc):\nSchema.org for Developers\nExtensions\nAs schema.org has grown, we have explored various mechanisms for\ncommunity extension\nas\na way of adding more detailed descriptive vocabulary that builds on the schema.org core. Some areas of Schema.org were\ndeveloped as \"named extensions\", and have dedicated entry pages. We previously called these \"hosted\" extensions, but\nthey are best considered simply as views into a single collection of schema definitions.\nHosted Sections\nFor example, via the\nauto\nsection there is a property for\nemissionsCO2\n,\nand via the\nbib\nsection we have a property\npublisherImprint\n.\nHowever, from the perspective of a publisher, these are simply schema.org properties.\nWe have a few of these sections:\nauto\nbib\nhealth-lifesci", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1997}
{"id": "db07de8e-c4f5-41f8-a2fa-352bd1709623", "url": "https://schema.org/docs/schemas.html", "source_domain": "schema.org", "title": "Schemas - Schema.org", "section_path": [], "text": "meta\npending\nNote\n: the 'pending' and 'meta' hosted sections are part of schema.org's schema development process.\nWe use the '\npending\n' section as a staging area for new schema.org terms that are under discussion and review.\nImplementors and publishers are cautioned that terms in the\npending\nsection\nmay lack consensus and that terminology and definitions could still change significantly after community and\nsteering group\nreview.\nConsumers of schema.org data who encourage use of such terms are\nstrongly encouraged\nto update implementations and documentation to track any evolving changes, and to share early implementation feedback with the\nwider community\n.\nThe '\nmeta\n' section is primarily for vocabulary used internally within schema.org to support technical definitions and\nschema.org site functionality. These terms are not intended for general usage in the public Web.\nAttic\nis a special area where terms are archived when deprecated from the core and other sections, or removed from\npending\nas not accepted into the full vocabulary. References to terms in the attic area are not normally displayed unless accessed via the term identifier or via the  home page. Implementors and data publishers are cautioned not to use terms in the attic area.\nUnlike other core and section terms, these areas may be updated at any time without the need for a full\nrelease\n.\nExternal Extensions\nThe schema.org\nsteering group\ndoes not officially approve external extensions - they are fully independent.\nWe list here some notable extensions that extend schema.org in interesting and useful ways.\nGS1 Web Vocabulary\n(\nblog post\n)\nCroissant\nis an open community-built standardized metadata vocabulary for ML datasets.\nTerms and conditions\nâ¢\nSchema.org\nâ¢\nV29.3\n|\n2025-09-04", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1998, "chunk_char_end": 3767}
{"id": "a0f7b984-d76f-4d02-9790-8cc1b80cb094", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "Data model - Schema.org\nNote\n: You are viewing the development\nversion of\nSchema.org\n.\nSee\nhow we work\nfor more details.\nSchema.org\nDocs\nSchemas\nValidate\nAbout\nData model\nThe following is a discussion about the data model used by\nschema.org.\nThe data model used is very generic and derived from\nRDF Schema\n(which in turn was\nderived from\nCycL\n, see\nHistory\nsection for details ...).\nWe have a set of\ntypes\n, arranged in a\nmultiple inheritance hierarchy\nwhere each type may be a sub-class of multiple types.\nWe have a set of\nproperties\n:\neach property may have one or more types as its domains. The property may be used\nfor instances of any of these types.\neach property may have one or more types as its ranges. The value(s) of the property should\nbe instances of at least one of these types.\nThe decision to allow multiple domains and ranges was purely pragmatic. While the\ncomputational properties of systems with a single domain and range are easier to\nunderstand, in practice, this forces the creation of a lot of artificial types,\nwhich are there purely to act as the domain/range of some properties.\nLike many other systems, the schema presented here can be extended (with\na few types like\nClass\nand\nProperty\nand a few properties like\ndomainIncludes\nand\nrangeIncludes\n)\nto allow for reflection, i.e., for the schema to be represented in terms of itself.\nThe canonical machine representation of schema.org is in RDF/Turtle. See the \"\ndevelopers\n\" page for more information on machine-readable views of schema.org.\nThe type hierarchy presented on this site is not intended to be a 'global ontology' of the world.\nWhen founded in 2011 it was strictly focused around the types of entities\nfor which the project's founders (Microsoft, Yahoo!, Google and Yandex), could reasonably expect to\nprovide some special treatment for via search engines. As the project has\nevolved\n,\nintroducing more community collaboration and extension mechanisms, its scope has expanded gradually.", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1975}
{"id": "c54159b0-7e89-45b0-b7e4-f9155d6868f3", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "However it is still the case that schema.org is not intended as a universal ontology. We expect it to be used\nalongside other vocabulary that shares our basic datamodel and our use of underlying standards like JSON-LD, Microdata\nand RDFa.\nConformance\nAlthough it might be helpful for search applications if structured data markup always followed schema.org very\nstrictly, in practice this is unrealistic. Our schemas also continue to evolve in response to\nfeedback, discussion and new applications of the data. Where possible we\namend\nexisting definitions incrementally\nrather than introducing lots of new properties for similar use cases. We have consequently based schema.org on a\nvery flexible datamodel, and take a pragmatic view of conformance.\nWe expect schema.org properties to be used with new types, both from schema.org and from external extensions.\nWe also expect that often, where we expect a property value of type Person, Place, Organization or some other\nsubClassOf Thing, we will get a text string, even if our schemas don't formally document that expectation.\nIn the spirit of \"some data is better than none\", search engines will often accept this markup and do the best we can.\nSimilarly, some types such as\nRole\nand\nURL\ncan be used with all properties,\nand we encourage this kind of experimentation amongst data consumers.\nNotes for toolmakers and schema authors\nThis section is oriented towards extension authors and tool makers, i.e. creators of applications that\nconsume, check or transform schema.org-based data. Most publishers and webmasters needn't worry about these details\n.\nApplications of schema.org can address conformance in several ways. Tools such as validators can check for\napplication-specific patterns, such as the data structures required for some specific functionality.\nThey may also check compliance with underlying formats (JSON-LD, Microdata, RDFa etc.), or offer additional", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1976, "chunk_char_end": 3894}
{"id": "ac4ae55e-8ca7-47c9-a63c-f1cf121d0bbd", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "hints that go beyond formal conformance (e.g. checking for readability issues or implausible data).\nWhile it is appropriate and useful for such checkers to warn about published data that may be difficult or ambiguous\nfor consumers, they are not obliged to treat unexpected structures as errors. Schema.org's underlying datamodel\nis naturally flexible, and provides an\nextensible\nbasis for rich structured data.\nWe encourage both publishers and consumers to continue to explore and\nshare\nnew vocabulary ideas for\nevolving\nschema.org.\nIt is not an error for a schema.org entity description to include properties from several independent types, e.g. something\nmight simultaneously be both a\nBook\nand a\nProduct\nand be usefully described with\nproperties from both types. It is useful but not required for the relevant types to be included in such a description. This\nflexibility allows schema.org types to be developed with some decentralization, and for vocabulary to be re-used and combined\nin useful ways. When we list the expected types associated with a property (or vice-versa) we aim to indicate the main ways\nin which these terms will be combined in practice. This aspect of schema.org is naturally imperfect. For example the\nschemas for\nVolcano\nsuggest that since volcanoes are places, they may have fax numbers. Similarly,\nwe list the unlikely (but not infeasible) possibility of a\nCountry\nhaving \"opening hours\".\nWe do not attempt to perfect this aspect of schema.org's structure, and instead rely heavily on an extensive collection of\nillustrative examples that capture common and useful combinations of schema.org terms. The type/properties associations of\nschema.org are closer to \"guidelines\" than to formal rules, and improvements to the guidelines are\nalways\nwelcome\n.\nSee also:\nPostel's Law\nMapping to RDFa Lite\nOur use of Microdata maps easily into\nRDFa Lite\n, and\nmany of our examples now show both variations (alongside the newer\nJSON-LD\nsyntax).", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3895, "chunk_char_end": 5857}
{"id": "a515c8ad-275f-411b-8de1-2cb0ae654b48", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "All of Schema.org can be used with the RDFa Lite syntax. The RDFa Lite version of the markup looks almost isomorphic\nto the Microdata version. The following sample demonstrates the use of RDFa Lite to\nmark up a\nProduct\ntype example:\n<div vocab=\"https://schema.org/\" typeof=\"Product\">\n<img property=\"image\" src=\"dell-30in-lcd.jpg\" />\n<span property=\"name\">Dell UltraSharp 30\" LCD Monitor</span>\n<div property=\"aggregateRating\"\ntypeof=\"AggregateRating\">\n<span property=\"ratingValue\">87</span>\nout of <span property=\"bestRating\">100</span>\nbased on <span property=\"ratingCount\">24</span> user ratings\n</div>\n<div property=\"offers\" typeof=\"AggregateOffer\">\n<span property=\"lowPrice\">$1250</span>\nto <span property=\"highPrice\">$1495</span>\nfrom <span property=\"offerCount\">8</span> sellers\n</div>\nSellers:\n<div property=\"offers\" typeof=\"Offer\">\n<a property=\"url\" href=\"save-a-lot-monitors.com/dell-30.html\">\nSave A Lot Monitors - $1250</a>\n</div>\n<div property=\"offers\" typeof=\"Offer\">\n<a property=\"url\" href=\"jondoe-gadgets.com/dell-30.html\">\nJon Doe's Gadgets - $1350</a>\n</div>\n...\n</div>\nMore specifically:\nitemprop\nis replaced with\nproperty\n.\nitemscope\nis dropped.\nitemtype\nis replaced with\ntypeof\n.\nIn addition, the attribute value\npair\nvocab=\"https://schema.org/\"\nis added to the body or\nsome other enclosing tag.\nBackground notes\nThe following sections provide additional information for some of schema.org's more general/abstract terms.\nmainEntity(OfPage) property\nBackground information on\nmainEntityOfPage\n/\nmainEntity\nproperties.\nmainEntityOfPage\n\"Indicates a page (or other CreativeWork) for which this thing is the main entity being described.\"", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5858, "chunk_char_end": 7511}
{"id": "a76b20b2-81dc-4f69-9b75-e3e2f5dc0590", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "Many (but not all) pages have a fairly clear primary topic, some entity or thing that the page describes. For example a restaurant's home page might be primarily about that Restaurant, or an event listing page might represent a single event. The mainEntity and mainEntityOfPage properties allow you to explicitly express the relationship between the page and the primary entity.\nRelated properties include\nsameAs\n,\nabout\n, and\nurl\n.\nThe\nsameAs\nand\nurl\nproperties are both similar to\nmainEntityOfPage\n. The url property should be reserved to refer to more official or authoritative web pages, such as the itemâs official website. The sameAs property also relates a thing to a page that indirectly identifies it. Whereas sameAs emphasises well known pages, the mainEntityOfPage property serves more to clarify which of several entities is the main one for that page.\nmainEntityOfPage can be used for any page, including those not recognized as authoritative for that entity. For example, for a product, sameAs might refer to a page on the manufacturerâs official site with specs for the product, while mainEntityOfPage might be used on pages within various retailersâ sites giving details for the same product.\nabout\nis similar to mainEntity, with two key differences. First, about can refer to multiple entities/topics, while mainEntity should be used for only the primary one. Second, some pages have a primary entity that itself describes some other entity. For example, one web page may display a news article about a particular person. Another page may display a product review for a particular product. In these cases, mainEntity for the pages should refer to the news article or review, respectively, while about would more properly refer to the person or product.\n\"identifier\" property\nBackground information on the 'identifier' property and its sub-properties.\nThe\nidentifier", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7512, "chunk_char_end": 9400}
{"id": "c1d579ea-dcda-4c0e-97ba-37d018dbf44e", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "property and its sub-properties are primarily useful in cases where the content is expressed as a textual string. Increasingly there are canonical URL/URI representations for each of these. All schema.org syntaxes already have built-in representation for URIs and URLs, e.g. in Microdata 'itemid', in RDFa 1.1, 'resource', in JSON-LD, '@id'. Generally it is preferable to use these unless there is a specific requirement to explicitly state the kind of identifier, or to provide additional / alternative identifiers (e.g., DOIs). Such requirements are common e.g. for scientific dataset description.\nIn some cases the values of the\nidentifier\nproperty indicate a set of (somewhat) interchangeable entities rather than a single distinct real world entity.\nSuch sets could be viewed as corresponding to classes, but we do not explore that possibility here. For example\nsku\nand the various product-related GTIN codes.\nHowever\nidentifier\nis not intended to cover broader categorization and classification mechanisms. For example, although the\nisicV4\nproperty has values that are in some sense identifiers,\nwe do not treat\nisicV4\nas a subproperty of\nidentifier\nsince it serves to identify a category not an individual Thing (specifically,\nPerson\n). Similarly, very many\nschema.org properties can have values that are written as URLs, but we do not treat those properties as specializations of\nidentifier\n.\nIn the most complex case, there is sometimes a need to represent the type of an identifier. In this case, a\nPropertyValue\npair ('name', 'identifier') pair can be used\nwhen a standard URI form of the identifier is unavailable. We do not currently have a recommended identifier scheme for identifier schemes, but in most cases there is a\nconventional short name for most identifier schemes (which should be used in lowercase form).\nHistory\nPrevious related work:\nRDF Schema\nMeta Content Framework (MCF) Using XML\n(and\nTutorial\n).\nMCF whitepaper\n,\nspec\nand\nbasic vocabulary\n.\nSee also\nSemantic network", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9401, "chunk_char_end": 11400}
{"id": "3e6ce8f0-0ed8-49c5-b23b-c60fac8a5589", "url": "https://schema.org/docs/datamodel.html", "source_domain": "schema.org", "title": "Data model - Schema.org", "section_path": [], "text": "article on Wikipedia.\nTerms and conditions\nâ¢\nSchema.org\nâ¢\nV29.3\n|\n2025-09-04", "engine": "generic", "topic": "schema_vocabulary", "doc_type": "vocabulary", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11401, "chunk_char_end": 11481}
{"id": "4a4d3ca1-1239-49bc-9128-59242a069899", "url": "https://moz.com/beginners-guide-to-seo", "source_domain": "moz.com", "title": "Beginner's Guide to SEO (Search Engine Optimization) - Moz", "section_path": [], "text": "Beginner's Guide to SEO (Search Engine Optimization) - Moz\nSkip to content\nLast chance: Save up to 40% on Moz Products - ends Sunday!\nThe Beginner's Guide to SEO\nRankings and traffic through search engine optimization\nTable of Contents:\n1. Introduction\n2. Quick Start Guide to SEO\n3. SEO 101\n4. How Search Engines Work: Crawling, Indexing, and Ranking\n5. Keyword Research\n6. On-Page SEO\n7. Technical SEO\n8. Link Building & Establishing Authority\n9. Measuring, Prioritizing, & Executing SEO Success\n10. SEO Glossary of Terms\nIntroduction\nWelcome to your SEO learning journey!\nYou'll get the most out of this guide if your desire to learn\nsearch engine optimization\n(SEO) is exceeded only by your willingness to execute and test concepts.\nThis guide is designed to describe all major aspects of SEO, from finding the terms and phrases (\nkeywords\n) that can generate qualified traffic to your website, to making your site\nfriendly to search engines\n, to\nbuilding links\nand marketing the unique value of your site.\nThe world of search engine optimization is complex and ever-changing, but you can easily understand the basics, and even a small amount of SEO knowledge can make a big difference. Free SEO education is also widely available on the web, including in guides like this! (Woohoo!)\nCombine this information with some practice and you are well on your way to becoming a savvy SEO.\nThe basics of search engine optimization\nEver heard of Maslow's hierarchy of needs? It's a theory of psychology that prioritizes the most fundamental human needs (like air, water, and physical safety) over more advanced needs (like esteem and social belonging). The theory is that you can't achieve the needs at the top without ensuring the more fundamental needs are met first. Love doesn't matter if you don't have food.\nOur founder, Rand Fishkin, made a similar pyramid to explain the way folks should go about SEO, and we've affectionately dubbed it \"\nMoz\nlow's hierarchy of SEO needs.\"", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1976}
{"id": "14bad1e9-f859-4237-a607-df3855d02af0", "url": "https://moz.com/beginners-guide-to-seo", "source_domain": "moz.com", "title": "Beginner's Guide to SEO (Search Engine Optimization) - Moz", "section_path": [], "text": "Here's what it looks like:\nAs you can see, the foundation of good SEO begins with ensuring crawl accessibility, and moves up from there.\nUsing this beginner's guide, we can follow these seven steps to successful SEO:\nCrawl accessibility so engines can read your website\nCompelling content that answers the searcher’s query\nKeyword optimized to attract searchers & engines\nGreat user experience including a fast load speed and compelling UX\nShare-worthy content that earns links, citations, and amplification\nTitle, URL, & description to draw high click-through-rate (CTR) in the rankings\nSnippet/schema markup to stand out in SERPs\nWe'll spend time on each of these areas throughout this guide, but we wanted to introduce it here because it offers a look at how we structured the guide as a whole.\nExplore the chapters:\nChapter 0: Quick Start Guide to SEO\nSpeed up to catch up\nGo from zero to hero with our step-by-step refresh on the core fundamentals for setting your site up for SEO success.\nChapter 1: SEO 101\nWhat is it, and why is it important?\nFor true beginners. Learn what search engine optimization is, why it matters, and all the need-to-know basics to start yourself off right. Download your free Quick Start Worksheet.\nChapter 2: How Search Engines Work – Crawling, Indexing, and Ranking\nFirst, you need to show up.\nIf search engines literally can't find you, none of the rest of your work matters. This chapter shows you how their robots crawl the Internet to find your site and add it to their indexes.\nChapter 3: Keyword Research\nUnderstand what your audience wants to find.\nOur approach targets users first because that's what search engines reward. This chapter covers keyword research and other methods to determine what your audience is seeking.\nChapter 4: On-Page SEO\nUse your research to craft your message.", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1977, "chunk_char_end": 3806}
{"id": "a27c97bd-a885-4145-b2b3-5faad3e0ae74", "url": "https://moz.com/beginners-guide-to-seo", "source_domain": "moz.com", "title": "Beginner's Guide to SEO (Search Engine Optimization) - Moz", "section_path": [], "text": "This is a hefty chapter, covering optimized design, user experience, information architecture, and all the ways you can adjust how you publish content to maximize its visibility and resonance with your audience.\nChapter 5: Technical SEO\nBasic technical knowledge will help you optimize your site for search engines and establish credibility with developers.\nBy implementing responsive design, robot directives, and other technical elements like structured data and meta tags, you can tell Google (a robot itself) what your site is all about. This helps it rank for the right things.\nChapter 6: Link Building & Establishing Authority\nTurn up the volume.\nOnce you've got everything in place, it's time to expand your influence by earning attention and links from other sites and influencers.\nChapter 7: Measuring, Prioritizing, & Executing SEO Success\nSet yourself up for success.\nAn essential part of SEO is knowing what's working (and what isn't), adjusting your approach as you go along.\nChapter 8: SEO Glossary of Terms\nUnderstand key terms and phrases.\nLearning SEO can sometimes feel like learning another language, with all the jargon and industry terms you're expected to know. This chapter-by-chapter glossary will help you get a handle on all the new words.\nMoz Academy SEO Essentials Certification\nOrganize your learning with Moz's SEO Essentials Certification. Learn with on-demand videos and task lessons, test your knowledge with exams at the end of each section, and top it all off with shiny new credentials to share with your professional network.\nGet Certified\nHow much of this guide do I need to read?\nIf you're serious about improving search traffic we do recommend reading the Beginner's Guide to SEO front-to-back. We've tried to make it as concise and easy to understand as possible, and learning the basics of SEO is a vital first step in achieving your online business goals.", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3807, "chunk_char_end": 5705}
{"id": "76034fa0-ae8e-4077-a908-b868b6239276", "url": "https://moz.com/beginners-guide-to-seo", "source_domain": "moz.com", "title": "Beginner's Guide to SEO (Search Engine Optimization) - Moz", "section_path": [], "text": "Go through at the pace that suits you best, and be sure to take note of the dozens of resources we link to throughout the chapters — they're also worthy of your attention.\nIf you want to take a more guided approach to learning SEO or training your entire team, check out the\nMoz Academy SEO Essentials Certification\n. We've consolidated all the resources you need to learn SEO fundamentals alongside unique learning methods, task lessons and quizzes to test your knowledge. You can also display your knowledge with your Linkedin Moz SEO Essentials certification badge.\nGetting excited yet? You should be! Without further ado, let's launch into\nChapter 0: Quick Start Guide\n.\nScale revenue from SEO with Moz Pro\nGet started with the all-in-one SEO tool marketers trust\nStart your free trial\nGet the latest SEO tips and strategies in your inbox\nBack to Top\nRead Next\nQuick Start Guide to SEO\nLooking to get started with SEO right away? The Quick Start Guide from the Beginner's Guide to SEO provides a checklist to jumpstart your SEO strategy.\nSEO 101\nFor true beginners, this chapter gives you a baseline for what SEO is, why it matters, and how you should frame your thinking around it going forward.\nHow Search Engines Work: Crawling, Indexing, and Ranking\nIf search engines literally can't find you, none of the rest of your work matters. This chapter shows you how their robots crawl the Internet to find your site and put it in their indexes.", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5706, "chunk_char_end": 7152}
{"id": "39e0b0ef-8159-4eca-8515-bb7b5c23d221", "url": "https://moz.com/learn/seo", "source_domain": "moz.com", "title": "SEO Learning Center - Moz", "section_path": [], "text": "SEO Learning Center - Moz\nSkip to content\nLast chance: Save up to 40% on Moz Products - ends Sunday!\nSEO Learning Center\nLearn SEO from Moz experts — for free.\nReady to dive in? Explore by SEO topic.\nAnalytics and Reporting\nCompetitive Research\nContent Marketing\nCrawling and Site Audits\nInternational SEO\nKeyword Research\nLink Building\nLocal SEO\nMobile SEO\nOn-Site SEO\nRanking and Visibility\nSocial Media Marketing\nFeatured Resources\nProfessional's SEO Guide\nTake it to the next level.\nLevel-up your search strategy with our guide for practicing SEO professionals.\nRead the guide\nOne-Hour Guide to SEO\nShort on time? Watch the Video Series.\nWe've condensed all the need-to-know SEO essentials into a handy hour-long video series.\nWatch the video series\nExplore SEO by Learning Pathway\nLearn SEO fundamentals\nLearn everything you need to know about SEO fundamentals to grow your business and advance your career.\nThe Beginner's Guide to SEO\nBegin your SEO education with the Beginner’s Guide to SEO and unleash the power of SEO on your web marketing campaigns.\nRead the guide\nThe SEO Keyword Research Master Guide\nThe ultimate keyword research guide! Understand exactly what content to create to best help your business goals and target relevant traffic.\nRead the guide\nSEO Basics on the Moz Blog\nRead all about the fundamentals of SEO with articles from industry experts.\nDiscover blog posts\nMoz Academy SEO Essentials Certification\nRamp up your SEO knowledge and show off your skills on LinkedIn with Moz's SEO Essentials Certification.\nGet certified\nBeat your competition\nLearn everything you need to know about SEO competitive analysis in order to beat your competition.\nGuide to SEO Competitor Analysis\nDiscover who your online competitors are, and engineer the most successful elements of these tactics into your SEO strategy.\nRead the guide\nCompetitive Research Tactics on the Moz Blog", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1892}
{"id": "ffb13c7b-bb0a-415f-a835-8d696429c240", "url": "https://moz.com/learn/seo", "source_domain": "moz.com", "title": "SEO Learning Center - Moz", "section_path": [], "text": "Dive into techniques for reverse-engineering what's working for your competitors, and use it to your advantage.\nExplore the blog\nHighly Competitive Niches\nWith the Professional's Guide to SEO, you'll discover how to rank in a SERP saturated with the most authoritative websites.\nRead the guide\nSEO Competitive Analysis Certification\nLearn how to confidently build an SEO competitive analysis plan and get certified with Moz Academy.\nGet certified\nMaster Technical SEO\nLearn all you need to know about technical SEO to build better websites for visitors and search engines.\nTechnical SEO Audit Checklist\nDownload your free technical SEO (and beyond) site audit checklist.\nDownload your free checklist\nTechnical SEO Tactics on the Moz Blog\nLearn all about technical SEO with articles from industry experts.\nExplore articles on the blog\nProfessional's Guide to Technical SEO\nGet the definitive answer to\nWhat is Technical SEO?\nRead all about the technical foundations of SEO.\nRead the guide\nTechnical SEO Certification\nLearn technical SEO comprehensively and get certified with Moz Academy.\nGet certified\nThe Latest From The Blog\nWhiteboard Friday\nAI and SEO\nSearch Engines\nDid We Pass Peak AIO? — Whiteboard Friday\nHas Google reached \"Peak AI Overview\"? In this episode of Whiteboard Friday, Tom analyzes late 2025 data, revealing a drop in AIO frequency for informational queries and a new trend of AI results appearing below position one.\nWhiteboard Friday\nAI and SEO\nContent Marketing\nNavigating Content Marketing Amidst the Rise of AI — Whiteboard Friday\nIn this episode of Whiteboard Friday, Ross talks about how search has changed and how we, as marketers, can navigate that change to produce quality content. With the increased use of AI and LLMs, content creation may seem more complex, but it’s still based on the fundamentals we use every day.\nAI and SEO\nSEO Analytics\nSEO Tools\n13 Best AI Automation Tools to Increase Productivity & Efficiency", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1893, "chunk_char_end": 3845}
{"id": "0317c14d-aa16-41c1-a3da-51581da881bc", "url": "https://moz.com/learn/seo", "source_domain": "moz.com", "title": "SEO Learning Center - Moz", "section_path": [], "text": "Overwhelmed by AI tools for automation? Discover 13 powerful tools to streamline workflows, boost productivity, and automate repetitive tasks effortlessly.\nRaring to go with SEO?\nWe know the feeling. Set yourself up for success with top-notch data and analytics — have a free month of Moz Pro, on us.\nStart my Free Trial\nLearn more about using our products\nVisit the Help Hub\nConnect and learn with marketers across the world\nJoin the Moz Community\nCheck out our free SEO tools\nGet free tools", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3846, "chunk_char_end": 4338}
{"id": "275b458e-232b-46b3-956c-e0ea17890239", "url": "https://ahrefs.com/seo", "source_domain": "ahrefs.com", "title": "SEO: The Complete Guide for Beginners", "section_path": [], "text": "SEO: The Complete Guide for Beginners\nThe Beginner’s\nGuide to SEO\nYour SEO journey starts here!\nIf you want to learn SEO, you’ve come to the right place.\nThis beginner’s guide, consisting of seven chapters, has everything you need to understand the basics of SEO and start ranking higher. You’ll also find links to useful resources from our\nSEO Blog\nand\nYouTube channel\nthroughout, so you can forge your own path to SEO mastery.\n1/ How Search Engines Work\nBefore you start learning SEO, you need to understand how search engines work.\n2/ SEO Basics\nLearn how to set your website up for SEO success, and get to grips with the four main facets of SEO.\n3/ Keyword Research\nThe starting point in SEO is to understand what your target customers are searching for.\n4/ SEO Content\nLearn how to create content that ranks in search engines.\n5/ On-Page SEO\nThis is where you optimize your pages to help search engines understand them.\n6/ Link Building\nLinks are how search engines discover new pages and judge their \"authority.\" It's hard to rank for competitive terms without links.\n7/ Technical SEO\nIt's important to make sure there are no technical faux pas that prevent Google from accessing and understanding your website.\nSEO Book for Beginners\nWe turned the seven chapters of our beginner’s guide to SEO into a beautiful hardcover book.\nTake a look →\nSEO Glossary\nIf you’ve read anything about SEO before, you’ll know there’s lots of industry jargon. But to keep things simple for now, here are a few key terms and concepts you should know.\nBacklinks:\nLinks from a page on one website to another.\nKeywords:\nThe words and phrases that people type into search engines to find what they’re looking for.\nLong-tail keywords:\nKeywords with low search volumes.\nSERPs:\nThis stands for Search Engine Results Pages. They’re the pages that Google and other search engines show in response to a user’s search query.\nSERP features:", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1915}
{"id": "e6772cb8-99a4-4ca9-961d-956d6d55631c", "url": "https://ahrefs.com/seo", "source_domain": "ahrefs.com", "title": "SEO: The Complete Guide for Beginners", "section_path": [], "text": "Non-traditional search results (not a ’blue link’) like videos, tweets boxes, or featured snippets.\nFeatured snippets:\nAnswers to search queries pulled from a top-ranking page that show up near the top of the SERP.\nRich snippets:\nSearch results with additional valuable information displayed alongside them (e.g., star ratings, cooking times, etc.)\nOutreach:\nEmailing relevant bloggers and journalists to tell them about your content.\nLearn more industry jargon in our\nSEO Glossary\n.\nSEO for Local Businesses\nPeople don’t search for local businesses in the Yellow Pages anymore. They use Google. Learn how to get more business from organic search with our SEO guides for local businesses.\nSEO for Realtors\nSEO for Caterers\nSEO for Gyms\nSEO for Beauty Salons\nSEO for Jewelers\nSEO for Wine Estates\nSEO for Car Dealerships\nSEO for Photographers\nSEO for Landscapers\nSEO for Personal Injury Lawyers\nSEO for Insurance Agents\nSEO for Dentists\nLooking for more resources? Check out our blog and YouTube channel.\nahrefs\nblog↗\nahrefs\ntv↗", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1916, "chunk_char_end": 2943}
{"id": "f9459574-280a-4ef9-8959-bc96f70b4fef", "url": "https://ahrefs.com/academy/seo-training-course", "source_domain": "ahrefs.com", "title": "SEO Course for Beginners", "section_path": [], "text": "SEO Course for Beginners\nAcademy\n/\nSEO Course for Beginners\nIt doesn't matter if you know nothing about SEO. By the end of this free SEO training course, you’ll understand how to apply the basics of SEO like keyword research, on-page SEO, and link building to rank higher in search engines.\nWatch intro\nWho is this course for?\nSEO beginners\nSmall business owners\nMarketing team leaders\nIf your end goal is to rank higher in search engines like Google and get free organic traffic, this SEO training is for you.\n14 lessons\nIn 5 modules\n2h\n\\u00A0\\u00A0\\u00A0\n00m\nTotal length\nStart course\nNo sign-up required, and absolutely free of charge\nWhat you’ll learn\nThe fundamentals of what search engine optimization is and how it works\nWhy SEO is important\nHow to do keyword research\nHow to optimize web pages for search engines\nBeginner-friendly link building strategies to get backlinks to your site\nTechnical SEO best practices for beginners\nCourse breakdown\nSEO Basics: What is SEO and Why is it Important?\n8:20\nModule 1. Keyword Research\nWhat are Keywords and How to Choose Them?\n7:59\nKeyword Research Pt 1: How to Analyze Searcher Intent\n4:32\nKeyword Research Pt 2: How to Find Keywords for Your Website\n11:28\nKeyword Research Pt 3: Understanding Ranking Difficulty\n9:54\nModule 2. On-page SEO\nWhat is On-Page SEO?\n5:00\nOn-Page SEO Pt 2: How to Optimize a Page for a Keyword\n14:35\nModule 3. Link Building\nWhat is Link Building and Why is it Important?\n5:10\n3 Link Building Strategies to Get Backlinks\n4:00\nWhat makes a backlink “Good”?\n8:10\nLink Building Tactics for Beginners\n14:41\nHow to do Blogger Outreach for Backlinks\n12:51\nModule 4. The basics of Technical SEO\nWhat is Technical SEO and Why is it Important?\n7:43\nTechnical SEO Best Practices for Beginners\n6:03\nCourse by\nSam Oh\nSam Oh is VP of Marketing at Ahrefs. He incorporates his commitment to education and love for entrepreneurship into actionable and easy-to-digest tutorials.\n←\n→", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1942}
{"id": "558822dd-dc44-4782-9d9c-985457704d02", "url": "https://ahrefs.com/academy/seo-training-course", "source_domain": "ahrefs.com", "title": "SEO Course for Beginners", "section_path": [], "text": "Amazing work. Thanks for the effort. I really appreciate it. Ahrefs is my favorite SEO tools. You guys are doing a phenomenal job.\nFarris Harbi\n,\nYoutube\nSam will literally make you an SEO specialist :). Thank you\nSourav Panda\n,\nYoutube\nThis feels like so genuinely caring and honest it’s just mind-blowing. It is probably one of the best pieces of content I’ve ever saw on digital marketing\nYevheniy Kushnirenko\n,\nYoutube\nAs always, Sam Oh hits it out of the park! Great video, and great tips. Keep it going!\nKandaman\n,\nYoutube\nThese videos and the content generation course, helped me get a 307% increase in traffic. From 1.3k visitors to 25k/month in 3 months and 40 leads.\ngoing2sleep\n,\nYoutube\nI have learned so many things from this video! Thank you so much Ahrefs and Sam for this amazing and value-loaded video!\nMohsin Ahmad\n,\nYoutube\nSam, you have been a real open book for me and I’m sure for others as well. Tons of highly valuable content for web owners. Thanks a lot!\nDaniel Garino\n,\nYoutube\nReady to start?\nNo sign-up required, and absolutely free of charge\nStart course\nOther courses\nHow to use Ahrefs\nLearn practical ways to use Ahrefs' SEO tools and reports to improve SEO.\nStart course\n62 lessons · 3 hr 18 min\nHow to use Brand Radar\nLearn practical ways to use Ahrefs' Brand Radar to increase your brand's AI visibility.\nStart course\n5 lessons · 0 min\nBlogging for business\nBlogging for Business is a free blogging course where you'll learn how to start and grow a blog past 100,000 visitors and turn those readers into customers.\nStart course\n40 lessons · 4 hr 56 min\nAdvanced Link Building Course\nLearn how to get backlinks at scale without traditional link building tactics.\nStart course\n15 lessons · 1 hr 48 min\nAffiliate Marketing Course for Beginners\nTake our free affiliate marketing course and learn how to start and grow your own revenue-generating affiliate site.\nStart course\n16 lessons · 1 hr 49 min\nThe Best of AhrefsTV", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1943, "chunk_char_end": 3894}
{"id": "2c659245-c017-47ef-af7f-d5183a79c745", "url": "https://ahrefs.com/academy/seo-training-course", "source_domain": "ahrefs.com", "title": "SEO Course for Beginners", "section_path": [], "text": "Learn SEO from a curated list of the best SEO tutorials from Ahrefs’ YouTube channel.\nStart course\n28 lessons · 5 hr 49 min\nLearn Ahrefs API: A beginner course\nThis course teaches you to run requests and unlock the full potential of Ahrefs data with our API.\nStart course\n7 lessons · 11 min", "engine": "generic", "topic": "seo_education", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3895, "chunk_char_end": 4185}
{"id": "5b248024-bdd1-45d6-bfc4-f4055d27b6e3", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Manual actions report - Search Console Help\nSkip to main content\nManual actions report\nSee if your site has any manual actions issued against it and view the site's manual action history.\nIf a site has a manual action, some or all of that site will not be shown in Google search results.\nOpen the Manual actions report\nManual Actions report in Search Console - Google Search Console Training\nWhat is a manual action?\nGoogle issues a manual action against a site when a human reviewer at Google has determined that pages on the site are not compliant with\nGoogle's spam policies\n. Most manual actions address attempts to manipulate our search index. Most issues reported here will result in pages or sites being ranked lower or omitted from search results without any visual indication to the user.\nIf your site is affected by a manual action, we will notify you in the Manual Actions report and in the Search Console message center.\nWhy do manual actions exist?\nEver since there have been search engines, there have been people dedicated to tricking their way to the top of the results page. This is bad for searchers because more relevant pages get buried under irrelevant results, and it's bad for legitimate websites because these sites become harder to find. For these reasons, we've been working since the earliest days of Google to fight spammers, helping people find the answers they're looking for, and helping legitimate websites get traffic from search.\nGoogle is constantly working to improve search. We take a data-driven approach and employ analysts, researchers, and statisticians to evaluate search quality on a full-time basis. Changes to our algorithms undergo extensive quality evaluation before being released.\nMore information about our algorithm.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1767}
{"id": "6cd2d61c-52e8-480c-bc16-280442fc3245", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Our algorithms are extremely good at detecting spam, and in most cases we automatically discover it and remove it from our search results. However, to protect the quality of our index, we're also willing to take manual action to remove spam from our search results.\nDo I have any manual actions against my site?\nYou'll see a count of manual actions against your site at the top of the report. If your site has no manual actions, you'll see a green check mark and an appropriate message.\nBut I just bought this site!\nIf you recently bought a site that violated our spam policies before you owned it, fix the issues listed in this report, then let us know in your reconsideration request that you recently acquired the site and that it now no longer violates the policies.\nWhat pages are affected?\nExpand the manual action description to see a list of patterns of affected pages. This can be a subset of your site, or the entire site. Not all of the pages matching the pattern are necessarily affected.\nExample:\nhttps://example.com/real-estate/*\n– some or all pages under the\nreal-estate/\ndirectory are affected.\nAffects all pages\n– Manual actions with this description affect the entire site.\nHow do I fix the problem?\nTo fix a manual action on your site:\nExpand the manual action description panel on the report for more information.\nSee which pages are affected.\nSee the type and short description of the issue, and follow the \"Learn more\" link to see detailed information and steps to fix the issue. (You can find the detailed information for each action below on this page).\nFix the issue on\nall affected pages\n.\nFixing the issue on just some pages will not earn you a partial return to search results. If you have multiple manual actions on your site, read about and fix all of them.\nBe sure that Google can reach your pages; affected pages should not require a login, be behind a paywall, or be blocked by\nrobots.txt\nor a noindex directive. You can test accessibility by using the", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1768, "chunk_char_end": 3753}
{"id": "ce784dda-c909-46f0-8e17-660729a8634c", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "URL Inspection tool\n.\nWhen\nall issues\nlisted in the report are fixed in\nall pages\n, select\nRequest Review\nin this report. In your reconsideration request, describe your fixes. A good request does three things:\nExplains the exact quality issue on your site.\nDescribes the steps you've taken to fix the issue.\nDocuments the outcome of your efforts.\nReconsideration reviews can take some time (\nsee below)\n. You will be informed of progress by email. You will get a review confirmation message when you send your request, to inform you that the review is in progress; don't resubmit your request before you get a final decision on your outstanding request.\nHow long will my reconsideration review take?\nMost reconsideration reviews can take several days or weeks, although in some cases, such as link-related reconsideration requests, it may take longer than usual to review your request. You will be informed by email when we receive your request, so you'll know it is active. You will also receive an email when the review is complete. Please don't resubmit your request before you get a decision on any outstanding requests.\nList of manual actions\nHere are the manual actions that can be applied by Google and how to fix them.\nSite abused with third-party spam\nGoogle has detected a significant portion of your site being abused with spam that violates Google's spam policies and adds little or no value to the web. The spammy content may appear on your forums, guestbooks, social media platforms, file uploader, free hosting services, or internal search pages.\nThis spammy content is possibly generated by site visitors or other third-parties, and it leverages your site to promote spam rather than the actual subject matter of your site.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3754, "chunk_char_end": 5493}
{"id": "b8e050ba-307a-4c6c-bb9c-69260f6a496e", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "If you receive a notification from Google about this type of spam, the good news is that we generally believe your site is of sufficient quality that we didn't see a need to take manual action on the whole site. This manual action will only affect those pages with spammy content. However, if your site has too much spammy content like this, that may affect our overall assessment of the site, result in a bad user experience, and affect your site's reputation and ranking.\nRecommended actions\nReview Google's Spam policies on\nuser-generated spam\n, documentation on preventing\ncomment spam\nand\nfree host spam\n, and\nhow to fix hacked pages\n, then follow these steps to identify and correct the violation(s) on your site:\nIdentify pages on your site where users, visitors, or other third-parties could add content or interact with: these could be your forums, guestbooks, social media platforms, file uploader, free hosting services, or internal search pages that users can send queries to.\nReview the example URLs in the messages you received in the Search Console Messages center or by email to gain a better sense of where the spammy content appears. In addition, please search your site for unexpected or spammy content using the\n\"site:\" operator\nin Google search, adding commercial or adult keywords that are unrelated to your site's topic. For example, search for [site:\nyour-domain-name\nviagra] or [site:\nyour-domain-name\nfree movie watch online] to detect the irrelevant content on your site. Search for things such as:\nOut-of-context text or off-topic links with the sole purpose of promoting a third-party website or service (for example, \"Free movie download\" or \"watch online”)\nGibberish or text that appears to be auto-generated\nComments or information submitted by users with unrealistic, commercial-sounding keywords; that is, user names like \"Discount Insurance\" that don't sound like real usernames and link to unrelated sites", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5494, "chunk_char_end": 7435}
{"id": "bb1838ea-5b64-4433-bb91-3b61c8dc609d", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Internal search results where the user's query appears geared at promoting a third-party website or service\nMonitor your web server log files for sudden unexpected or unexplained traffic spikes, especially for newly created pages. Look for any URLs with keywords in URL patterns that are completely irrelevant to your website. You can see your top clicked pages in Google Search by using the\nPerformance report for Search\n.\nRemove any inappropriate content and block obviously inappropriate third-party content from being published to your platform with a list of spammy terms (such as streaming, download, adult, gambling, or pharma terms).\nWhere appropriate, use these guides:\nPrevent user-generated spam on your site\nMarking user-generated content links with rel=”ugc”\nFixing hacked pages\nConsider consolidating your interactive content into a concentrated file path for easier maintenance and spam detection.\nWhen you're sure that your site is no longer in violation of our spam policies, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nIn the meanwhile, it is also recommended to continue actively monitoring and cleaning up such spammy content, improve your site's system and fix system vulnerability to prevent such spam in the future.\nUser-generated spam\nGoogle has detected spam on your pages submitted by site visitors. Typically, this kind of spam is found on forum pages, guestbook pages, or in user profiles.\nMatt Cutts explains the \"User-Generated Spam\" manual action\nRecommended actions\nReview the Google's policies on\nuser-generated spam\n, then follow these steps to identify and correct the violation(s) on your site:", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7436, "chunk_char_end": 9366}
{"id": "b2b78f12-3cd4-430b-8537-d89a69144744", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Users commonly add content to sites in forums, blog comments, and user profiles. Identify pages on your site where users could have added content.\nLook for profiles with commercial usernames like \"Discount Insurance” or posts with advertisements, off-topic links, or gibberish text. Check these areas for the following:\nPosts or profiles that look like advertisements\nPosts or profiles with out-of-context or off-topic links\nPosts or profiles with commercial usernames — names like \"Discount Insurance,\" that don't sound like real human names — that link to unrelated sites\nPosts or profiles that appear to be automatically generated (not written by a real user)\nSearch your site for unexpected or spammy content using the\nsite:\noperator in Google search, adding commercial or adult keywords that are unrelated to your site's topic. For example, search for [site:example.com viagra] to search for the term \"viagra\" on your site.\nRemove any inappropriate content.\nConsider\nimplementing measures to prevent user-generated spam\n.\nWhen you're sure that your site is no longer in violation of our spam policies, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nSpammy free host\nA significant fraction of sites hosted on your free web hosting service are spammy.\nGoogle tries to be precise when taking manual action related to spam. However, if a significant fraction of the pages on a given web hosting service are spammy, we may take manual action on the whole service.\nRecommended actions\nGet tips for preventing and identifying abuse of your service.\nRemove any existing spammy accounts from your service.\nContact the technical team at your hosting service and tell them about the manual action.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9367, "chunk_char_end": 11355}
{"id": "7c5f03a4-cb98-498d-89ab-9dd2eb7a9b96", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "When you're satisfied that your site follows our spam policies, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nStructured data issue\nGoogle has detected that some of the markup on your pages may be using techniques that are outside our\nstructured data guidelines\n, for example: marking up content that is invisible to users, marking up irrelevant or misleading content, or other manipulative behavior.\nHere are some of the structured data issues that might apply to your site:\nTitle\nDescription\nContent on page differs from structured data\nJobPosting structured data found on pages that don't have job posting content. This is against our guidelines.\nLearn more\nCan't submit application on job offer page\nContent on pages found to be different than structured data on the page. This is against our guidelines.\nLearn more\nStructured data does not match content\nContent on pages found to be different than structured data on the page. This is against our guidelines.\nLearn more\nJob application requires payment\nPages with JobPosting structured data charge a fee to apply for the job. This is against our guidelines.\nLearn more\nJob request found on job offer pages\nPages with JobPosting structured data are seeking a job, not offering a job. This is against our guidelines.\nLearn more\nMisleading job location\nPages with JobPosting structured data include a misleading job location field. This is against our guidelines.\nLearn more\nJob poster not hiring\nPages with JobPosting structured data are collecting applications but not hiring. This is against our guidelines.\nLearn more\nStructured data issues on list page", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11356, "chunk_char_end": 13263}
{"id": "876a00e8-6d1e-4449-9840-6770208dd183", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Pages with a list of items need to have each item marked up individually. Aggregating data from multiple items into one structured data element is against our spam policies.\nLearn more\nJobPosting structured data on listing page\nA list page should not include structured data for individual jobs.\nLearn more\nJobPosting structured data on expired job\nJobPosting markup used on an expired job without the validThrough Property set in the past.\nLearn more\nContent on page differs from structured data\nClaimReview structured data found on pages that don't contain a claim review. This is against our guidelines.\nLearn more\nClaimReview lacks reference, or reference does not match verdict\nPages with ClaimReview structured data do not include a supporting source or reference. This is against our guidelines.\nLearn more\nStructured data found on hidden content\nStructured data found on elements that are not visible to the user. This is against our guidelines.\nLearn more\nNo mechanism for submitting a new review\nIf a page includes a review, the page must also provide a way to provide a review or clearly show where the reviews come from.\nLearn more\nCompany marked as product\nA company has been labeled as a product in structured data.\nLearn more\nNon-product labeled as product\nA non-product, or generic item, has been marked as a product.\nLearn more\nReview written by the site or person providing the service\nReviews must not be written or provided by the business or content provider unless they are customer, independent, and unpaid editorial reviews.\nLearn more\nEvent structured data is actually a promotion\nThe visible text or structured data description is more concerned with promoting or selling the event than describing it.\nLearn more\nNon-event labeled as an event\nA non-event item has been marked as an event. For example, a holiday or a coupon.\nLearn more\nNon-recipe marked as a recipe", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13264, "chunk_char_end": 15155}
{"id": "a2c28dc5-1a93-4ac7-bfe8-81169bc563b8", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "A non-recipe item has been marked as a recipe. A recipe must be for a food item and include both ingredients and steps.\nLearn more\nStructured data policy violation\nYou have a structured data policy violation on one or more pages.\nLearn more\nIncorrect Employer\nThe employer in the hiringOrganization field should match the employer in the job posting.\nLearn more\nIncomplete Job Description\nThe description field is incomplete or unintelligible.\nLearn more\nRecommended actions\nMake sure that the markup on your site meets Google's\nstructured data guidelines\nif you would like it featured in Google search results. This might require updating existing markup or removing any markup that violates our guidelines.\nWhen you've made these changes, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our guidelines, we'll revoke the manual action.\nUnnatural links\nto\nyour site\nGoogle has detected a pattern of unnatural, artificial, deceptive, or manipulative links pointing to your site.\nBuying links or participating in link schemes\nin order to manipulate ranking in Google Search results is a violation of our spam policies. This can result in some or all of your site getting a manual action.\nMatt Cutts and Alex explain the \"Unnatural links to your site\" manual action\nRecommended actions\nFirst, review\nGoogle's link spam policy\n.\nNext, follow the steps below to identify and correct the violation(s):\nDownload a list of\nlinks to your site\nfrom Search Console. You can download your links arranged either by hostname (\nTop linking sites\n> Export\n) or in chronological order (\nLinks report\n>\nExport external links > Latest links\n).", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15156, "chunk_char_end": 17025}
{"id": "2ef88d6f-5600-452d-bf80-9f389730d7fc", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Check this list for any links that violate our link spam policy. If the list is large, start by looking at the sites that link to you the most, or links that were created recently (in the last few months).\nFor any links that violate our spam policies,\ncontact the owner\nof that site and ask that they either remove the links or prevent them from passing PageRank, such as by adding a\nrel=”nofollow”\nor a more specific attribute.\nUse the\nDisavow links tool\nin Search Console to disavow any links that you could not get removed. We often see the Disavow links tool used incorrectly so\nkeep the following things in mind when using the tool\n:\nIf you can get a backlink removed, make a good-faith effort to remove the link first. Blindly adding all backlinks to the disavow file is not considered a good-faith effort, and will not be enough to make your reconsideration request successful.\nFor multiple links from the same domain to your site, use the \"domain:\" operator in the disavow file for convenience.\nMake sure that you don't disavow organic links to your site.\nSimply disavowing all backlinks without attempting to remove them might lead to rejection of your request.\nWhen you've removed or disavowed the artificial links, select\nRequest Review\non the Manual Actions report. Include documentation about the links you've had removed and an explanation of any links you were unable to remove, to help us process your request.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nUnnatural links\nfrom\nyour site\nGoogle has detected a pattern of unnatural artificial, deceptive, or manipulative outbound links on your site.\nBuying links or participating in link schemes", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17026, "chunk_char_end": 18932}
{"id": "fe5a45ad-cf7f-4181-b423-a7c7972433f3", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "in order to manipulate ranking in Google Search results is a violation of Google's spam policies.\nMatt Cutts and Sandy discuss the \"Unnatural links from your site\" manual action.\nRecommended actions\nFirst, review\nGoogle's link spam policy\n.\nNext, follow the steps below to identify and correct the violation(s) on your site:\nIdentify any links on your site that were paid for or that appear to violate our link spam policy, such as\nexcessive link exchanges\n.\nEither remove these links, or change them so that they no longer pass PageRank, for example by adding a\nrel=”nofollow”\nor a more specific attribute, or by redirecting them through a page\nblocked by robots.txt\n.\nWhen you're sure that your site is no longer in violation of our spam policies, select\nRequest Review\non the Manual Actions report. Provide examples of bad content that you removed and good content that you added.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nThin content with little or no added value\nGoogle has detected low-quality pages or shallow pages on your site. Here are a few common examples of pages that often have thin content with little or no added value:\nThin affiliate pages\nContent from other sources. For example: scraped content or low-quality guest blog posts\nDoorways\nThese techniques don't provide users with substantially unique or valuable content, and are in violation of our\nspam policies\n.\nMatt Cutts explains the \"Thin Content with little or no added value\" manual action.\nRecommended actions\nFirst, review the following sections of our spam policies:\nThin affiliate pages\nScraped content\nDoorways\nNext, follow the steps below to identify and correct the violation(s) on your site:\nCheck for content on your site that duplicates content found elsewhere.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 18933, "chunk_char_end": 20932}
{"id": "bf6d6b56-f60c-49a6-9a26-d634fd1a6957", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Check for thin content pages with affiliate links on your site.\nCheck for doorway pages on your site.\nIf your site contains any of these types of content, think about whether your site\nprovides significant added value for your users\n.\nTip: Consider asking friends or family — real people not affiliated with your site — to use or critique your site to get ideas for improving it.\nImprove your website so that it provides significant value for your users.\nWhen you're sure that your site is no longer in violation of our spam policies, select\nRequest Review\non the Manual Actions report. Provide examples of bad content that you removed and good content that you added.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nCloaking and/or sneaky redirects\nYour site may be showing different pages to users than are shown to Google, or redirecting users to a different page than Google saw.\nCloaking and sneaky redirects\nare a violation of Google's spam policies.\nMatt Cutts explains cloaking\nSubscription and paywalled content:\nPublishers should\nenclose paywalled content with structured data\nin order to help Google differentiate paywalled content from the practice of\ncloaking\n, where the content served to Googlebot is different from the content served to users.\nRecommended actions\nFirst, review Google's spam policies on\ncloaking\nand\nsneaky redirects\n.\nNext, follow the steps below to identify and correct the violation(s) on your site:\nUse the\nURL Inspection tool\nin Search Console to fetch pages from the affected area of your site.\nCompare the content fetched by Google to the content seen by a human user (you!) when visiting the site.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 11, "chunk_char_start": 20933, "chunk_char_end": 22815}
{"id": "859b74f0-c148-4bdc-a17d-9915b98e5567", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "If the content differs, identify and remove the part of your site that's serving different content to Google and users. This will require looking through your site's code on the server.\nCheck for URLs on your site that redirect users to somewhere other than where they expected to go.\nCheck for URLs on your site that redirect conditionally, for example that redirect only users coming from Google search, or only users coming from a particular range of IP addresses.\nIf your site redirects users in any of these ways, identify and remove the part of your site that generates these redirects. This will require looking through your site's code on the server.\nTip: These types of redirects are often written in JavaScript or in your .htaccess file. You might also check your content management system and any plugins.\nWhen you're sure that your site is no longer in violation of our spam policies, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nMajor spam problems\nThe site appears to use aggressive spam techniques such as\nscaled content abuse\n,\ncloaking\n, and/or other repeated or egregious violations of\nGoogle's spam policies\n.\nRecommended actions\nUpdate your site so that it no longer violates\nGoogle's spam policies\n.\nWhen you're sure that your site is no longer in violation of our spam policies, select\nRequest Review\non the Manual Actions report. Provide examples of bad content that you removed and good content that you added.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 12, "chunk_char_start": 22816, "chunk_char_end": 24549}
{"id": "d1894c3d-cb12-4731-a4a6-eff411463487", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "After you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nCloaked images\nSome of your site's images may display differently in Google's search results than when viewed on your site.\nCloaking\nis the practice of presenting different content to human users than to search engines. Cloaking is considered a violation of\nGoogle's spam policies\nbecause it provides our users with different results than they expected. Cloaking images can provide a bad user experience for Google image search results, as obscured images and mismatched thumbnails do not present the user with the image that they are searching for.\nExamples of cloaked images\nHere are some examples of image cloaking behavior:\nServing images to Google that are obscured by another image, for example: a block of text blocking an image.\nServing images to Google that are different than the image served to a page visitor.\nIf you need to block images from Google search results\n, use the method\ndescribed below\n.\nRecommended actions\nEnsure that your site displays exactly the same images to users on your site and in Google search results. Cloaking is acceptable only for opting out of image search inline linking as detailed below.\nWhen you're sure that your site's images are exactly the same whether viewed directly on your site or from Google search results, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account; we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nMinimize or block an image in search results\nTo prevent the\nfull-sized image", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 13, "chunk_char_start": 24550, "chunk_char_end": 26523}
{"id": "e89b8805-460d-4fac-b4a6-822e2971f266", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "from appearing in the Google search results,\nopt out of inline linking\n.\nTo\nprevent the image from appearing at all\nin search results,\nfollow these steps\n.\nTo opt out of inline linking:\nWhen your image is requested, examine the\nHTTP referrer header\nin the request.\nIf the request is coming from\na Google domain\n, reply with HTTP 200 or 204 and no content.\nGoogle will still crawl your page and see the image, but will display a thumbnail image generated at crawl time in search results. This opt-out is possible at any time, and does not require re-processing of a website's images. This behavior is not considered image cloaking and will not result in a manual action.\nHidden text and/or keyword stuffing\nSome of your pages may contain\nhidden text\nor\nkeyword stuffing\n, techniques that are not allowed by\nGoogle's spam policies\n.\nMatt Cutts and Nelson explain what it means if your site has a manual action labeled as \"Hidden text and/or keyword stuffing\" and what you can do to fix it.\nRecommended actions\nFirst, review Google's spam policies on\nhidden text\nand\nkeyword stuffing\n.\nNext, follow the steps below to identify and correct the violation(s) on your site:\nUse the\nURL Inspection tool\nin Search Console to check for content that's visible to our crawler but isn't visible to a human user (you!) when visiting the site.\nCheck for text that's the same, or similar, color as the background of the webpage.\nTip: You can often reveal such text by selecting all the text on the page, for example by pressing Ctrl + A or Command + A.\nCheck for any text hidden using CSS styling or positioning.\nRemove or re-style any hidden text so that it's equally discoverable by search engine crawlers and by human users.\nCheck for lists or paragraphs of repeated words without any context.\nCheck\n<title>\ntags and alt text for strings of repeated words.\nRemove any such words or other instances of keyword stuffing.\nWhen you're sure your site is no longer in violation of our spam policies, select", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 14, "chunk_char_start": 26524, "chunk_char_end": 28511}
{"id": "1c4f22be-77d9-4cb2-be2f-bc27dac0ff26", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Request Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nAMP content mismatch\nThere is a difference in content between the AMP version and its canonical web page.\nThe content of the AMP version and its canonical web page should be essentially the same. The text need not be identical, but the topic should be the same, and users should be able to accomplish the same tasks on both the AMP and the canonical page.\nAMP pages affected by this manual action will not be shown in Google Search: the canonical page will be shown instead.\nRecommended actions\nEnsure that the AMP is referencing the correct canonical web page.\nEnsure that the general content of the AMP and canonical page are the same.\nCheck that Google's view of the page is not different from the user's view of the page by using the\nURL Inspection tool\nfor both the AMP and the canonical page. A mismatch can occur when a robots.txt file blocks significant resources on one or the other. Blocked resources will also be shown in the URL Inspection tool.\nWhen your AMP and canonical pages are essentially the same, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nSneaky mobile redirects\nSome pages on this site appear to be redirecting mobile device users to content not available to search engine crawlers. These sneaky redirects are a violation of\nGoogle spam policies", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 15, "chunk_char_start": 28512, "chunk_char_end": 30418}
{"id": "8b4b39ce-3992-4642-9762-76dfdf15aaf8", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": ". To ensure quality search results for our users, the Google Search Quality team can take action on such sites, including removal of URLs from our index.\nOverview\nIn many cases, it is okay to show slightly different content on different devices. For example, optimizing for the smaller space of a smartphone screen can mean that some content, like images, need to be modified. Similarly, for mobile-only redirects, redirecting mobile users to improve their mobile experience (like redirecting mobile users from example.com/url1 to m.example.com/url1) is often beneficial to them. However, redirecting mobile users sneakily to different content is bad for the user experience.\nA frustrating experience:\nThe same URL shows up in search result pages on desktop and on mobile. However, when a user clicks on this result on their desktop computer, they visit URL A, but users clicking on the same result on a smartphone are redirected to unrelated URL B.\nSneaky mobile redirects can be created intentionally by a site owner, but we've also seen situations where mobile-only sneaky redirects happen without the site owner's knowledge. The following are examples of configurations that can cause sneaky mobile redirects:\nAdding code that creates redirection rules for mobile users\nUsing a script or element to display ads and monetize content that redirect mobile users\nA script or element added by hackers that redirects your mobile users to malicious sites\nRecommended actions\nIf you are not engaging in this behavior intentionally:\nMake sure that your site is not hacked\nCheck the\nSecurity Issues report\nto see if Google thinks you have been hacked.\nAudit third-party scripts/elements on your site\nIf your site is not hacked, then we recommend that you take the time to investigate if third-party scripts or elements are causing the redirects. You can follow these steps:\nRemove any third-party scripts or elements you do not control from the redirecting page(s) one by one.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 16, "chunk_char_start": 30419, "chunk_char_end": 32389}
{"id": "16596fb0-4e3f-4242-884c-3dd6461581cb", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "After removing each script or element, check your site behavior on a mobile device or in\nChrome mobile emulator\n(or any other emulator) to see if the redirection stops.\nIf you think a particular script or element is responsible for the sneaky redirect, consider removing it from your site and possibly debugging the issue with the script or element provider.\nIf you are engaging in this behavior intentionally:\nFix your pages.\nConfirm your fix\nby visiting your pages from Google search results with a smartphone or in a mobile device emulator.\nWhen the issue is fixed on all pages in your site, select\nRequest Review\non the Manual Actions report.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our spam policies, we'll revoke the manual action.\nAvoiding sneaky mobile redirects in the future\nTo diminish the risk of unknowingly redirecting your own users, be sure to choose advertisers who are transparent on how they handle user traffic. If you are interested in building trust in the online advertising space, you should research industry-wide best practices when participating in ad networks. For example, the Trustworthy Accountability Group's (Interactive Advertising Bureau)\nInventory Quality Guidelines\nare a good place to start. There are many ways to monetize your content with mobile solutions that provide a high quality user experience. Be sure to use them.\nTo check for sneaky mobile redirects on your site, walk through the following steps:\nCheck if you are redirected when you navigate to your site on your smartphone", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 17, "chunk_char_start": 32390, "chunk_char_end": 34116}
{"id": "cd6e2529-132b-4f3e-ae68-2a138ef2d023", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "We recommend you check the mobile user experience of your site by visiting your pages from Google search results with a smartphone. When debugging, mobile emulation in desktop browsers is handy because you can test for many different devices. You can, for example, view pages as a mobile device straight from your browser in Chrome, Firefox or Safari (for the latter, make sure you have enabled the \"Show Develop menu in menu bar” feature).\nListen to your users\nYour users can see your site in different ways than you. It's always important to pay attention to user complaints, so you can hear of any issue related to the mobile user experience.\nMonitor your mobile users in your site's analytics data\nUnusual mobile user activity can be detected by looking at some of the data in your website's analytics data. For example, keep an eye on the average time spent on your site by your mobile users: if all of a sudden, only your mobile users start spending much less time on your site than they used to, there might be an issue related to mobile redirections.\nMonitoring for any large changes in your mobile user activity can help you proactively identify sneaky mobile redirects. You can\nset up Google Analytics alerts\nthat will warn you of sharp drops in average time spent on your site by mobile users or drops in mobile users. While these alerts do not necessarily mean that you have mobile sneaky redirects, it's something worth investigating.\nNews and Discover policy violations\nYou have violated content policies for\nGoogle News\nand/or\nDiscover\n. The following sections describe the type of violation.\nDangerous content (News and Discover)\nGoogle has detected content on your site that appears to violate our dangerous content policy because it contains content that could directly facilitate serious and immediate harm to people or animals.\nRecommended actions\nReview and update your pages to comply with the policy by removing the dangerous content. (\nNews policy\n/\nDiscover policy\n)", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 18, "chunk_char_start": 34117, "chunk_char_end": 36108}
{"id": "b591fa7a-6267-4764-a119-1539f12b79f9", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "When you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nDeceptive practices: Coordinated deceptive practices (News and Discover)\nGoogle has detected content on your site that appears to conceal or misrepresent the site ownership or purpose.\nGoogle doesn't allow content or accounts that misrepresent or conceal their ownership or primary purpose, or engage in inauthentic or coordinated behavior to deceive, defraud, or mislead. This includes, but isn't limited to, the misrepresentation or concealment of country of origin, or working together in ways that conceal or misrepresent information about relationships or editorial independence.\nRecommended actions\nReview and update your site so that it clearly discloses any financial or otherwise meaningful relationships with governments and political interest groups. You may want to review who has access to your site's analytic, content management, and ecommerce systems, and decide whether those parties should be mentioned on your site as stakeholders.\nWhen you're sure that your site is no longer in violation of our guidelines, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial guidelines and an editorial board with a history of improved practices.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 19, "chunk_char_start": 36109, "chunk_char_end": 37894}
{"id": "00b9c3a6-e763-4b50-b23d-1a65fcb46d40", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "After you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our guidelines, we'll revoke the manual action.\nDeceptive practices: Good neighbor policy (News and Discover)\nGoogle has detected content on your site that impersonates or conceals the organization that created the content.\nGoogle doesn't allow content or accounts that misrepresent or conceal their ownership or primary purpose, or engage in inauthentic or coordinated behavior to deceive, defraud, or mislead. This includes, but isn't limited to, the misrepresentation or concealment of country of origin, directing content at users in another country under false premises, or working together in ways that conceal or misrepresent information about relationships or editorial independence.\nRecommended actions\nReview and update your site so that it clearly discloses any financial or otherwise meaningful relationships with governments and political interest groups. You may want to review who has access to your site's analytic, content management, and ecommerce systems, and decide whether those parties should be mentioned on your site as stakeholders.\nWhen you're sure that your site is no longer in violation of our guidelines, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial guidelines and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our guidelines, we'll revoke the manual action.\nDeceptive practices: Impersonation (News and Discover)", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 20, "chunk_char_start": 37895, "chunk_char_end": 39813}
{"id": "e9c87154-835f-448d-923d-00327547ab95", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Google has detected content on your site that misrepresents the person or organization that provided the content.\nGoogle doesn't allow content or accounts that mislead or confuse users by pretending to be someone else or pretend to represent an organization you do not.\nRecommended actions\nReview and update your site to clearly disclose the satirical nature of your site or provide evidence that disclosing accurate identities would create a personal security risk.\nWhen you're sure that your site is no longer in violation of our guidelines, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial guidelines and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our guidelines, we'll revoke the manual action.\nDeceptive practices: Misrepresentation of affiliation (News and Discover)\nGoogle has detected content on your site that seems to misrepresent or conceal the financial or editorial relationships of the content producers.\nGoogle doesn't allow content or accounts that misrepresent or conceal their financial and editorial relationships with governments or political interest groups.\nRecommended actions\nReview and update your site so that it clearly discloses any financial and editorial relationships with governments and political interest groups or provide evidence that this disclosure would create a personal security risk.\nWhen you're sure that your site is no longer in violation of our guidelines, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial guidelines and an editorial board with a history of improved practices.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 21, "chunk_char_start": 39814, "chunk_char_end": 41748}
{"id": "42916865-e132-4b1d-85ad-bfcb5058cd2b", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "After you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our guidelines, we'll revoke the manual action.\nDeceptive practices: Misrepresentation of location (News and Discover)\nGoogle has detected content on your site that seems to misrepresent or conceal the country of origin of the website.\nGoogle doesn't allow content or accounts that misrepresent or conceal site ownership or primary purpose, or engagement in inauthentic or coordinated behavior to deceive, defraud, or mislead. This includes, but isn't limited to, the misrepresentation or concealment of country of origin or location.\nRecommended actions\nReview and update your site so that it clearly clearly discloses an accurate location or locations for your organization or provide evidence that this disclosure would create a personal security risk.\nWhen you're sure that your site is no longer in violation of our guidelines, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial guidelines and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our guidelines, we'll revoke the manual action.\nHarassing content (News and Discover)", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 22, "chunk_char_start": 41749, "chunk_char_end": 43348}
{"id": "80decf6e-dd73-4d09-baf4-64e0eef494e0", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Google has detected content on your site that appears to violate our harassing content policy because it contains harassment, bullying, or threatening content, including but not limited to, that which might single someone out for malicious abuse, threaten someone with serious harm, sexualize someone in an unwanted way, expose private information of someone else that could be used to carry out threats, disparage, or belittle victims of violence or tragedy, deny an atrocity, or harass in other ways.\nRecommended actions\nReview and update your pages to comply with the policy by removing the harassing content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nHateful content (News and Discover)\nGoogle has detected content on your site that appears to violate our hateful content policy with content that incites hatred.\nWe do not allow content that promotes or condones violence, or has the primary purpose of inciting hatred against an individual or group, including but not limited to, on the basis of their race or ethnic origin, religion, disability, age, nationality, veteran status, sexual orientation, gender, gender identity, or any other characteristic that is associated with systemic discrimination or marginalization.\nRecommended actions\nReview and update your pages to comply with the policy by removing the hateful content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 23, "chunk_char_start": 43349, "chunk_char_end": 45346}
{"id": "a59f1b3c-d43c-4a3f-8bdc-b430fbed0650", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Request Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nManipulated media (News and Discover)\nGoogle has detected content on your site that appears to violate our manipulated media policy. We do not allow audio, video or image content that has been manipulated to deceive, defraud, or mislead by means of creating a representation of actions or events that verifiably did not take place and would cause a reasonable person to have a fundamentally different understanding or impression thereof - such that it may cause significant harm to groups or individuals, or significantly undermine participation or trust in electoral or civic processes.\nRecommended actions\nReview and update your pages to comply with the policy by identifying media manipulation, disclosing satirical or parody intent, or removing misleading content that significantly undermines participation or trust in civic or electoral processes. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nMedical content (News and Discover)", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 24, "chunk_char_start": 45347, "chunk_char_end": 47306}
{"id": "693bcd5d-a9c5-4f1b-88a4-70d8fbe272a3", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "We don't allow content that contradicts or runs contrary to scientific or medical consensus and evidence-based best practices.\nRecommended actions\nReview and update your pages to comply with the policy by removing the medical content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nMisleading content (News and Discover)\nGoogle has detected content on your site that appears to violate our misleading content policy and misleads users into engaging with it by promising details which are not reflected in the content.\nRecommended actions\nReview and update your pages to comply with the\nDiscover policy\nby removing the misleading content.\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nSexually explicit content (News and Discover)\nGoogle has detected content on your site that appears to violate our sexually explicit content policy because it contains explicit sexual imagery or videos primarily intended to cause sexual arousal.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 25, "chunk_char_start": 47307, "chunk_char_end": 49295}
{"id": "43329318-fa7c-4b21-8b11-6abfa7e90975", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Recommended actions\nReview and update your pages to comply with the policy by removing the sexually explicit content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nTerrorist content (News and Discover)\nGoogle has detected content on your site that appears to violate our terrorist content policy and promotes terrorist or extremist acts, including recruitment, inciting violence, or celebrating terrorist attacks.\nRecommended actions\nReview and update your pages to comply with the policy by removing the terrorist content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nTransparency (News and Discover)\nGoogle has detected content on your site that appears to violate our transparency policy.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 26, "chunk_char_start": 49296, "chunk_char_end": 51082}
{"id": "8b6f16e0-d0cd-4b57-86be-74fbe1d24e12", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "Visitors to your site want to trust and understand who publishes the content they are consuming, and information about those who have written articles. That's why news sources on Google should provide clear dates and bylines, as well as information about authors, the publication, the publisher, company or network behind it, and contact information.\nRecommended actions\nReview and update your pages to comply with the\nNews policy\nby providing clear dates, non-generic information about authors, the publication, the publisher, editorial board, company or network behind it, and non-generic contact information.\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nViolence and gore content (News and Discover)\nGoogle has detected content on your site that appears to violate our violence and gore policy because it contains content that incites or glorifies violence. We also do not allow extremely graphic or violent materials for the sake of disgusting others.\nRecommended actions\nReview and update your pages to comply with the policy by removing the violent or gory content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 27, "chunk_char_start": 51083, "chunk_char_end": 52974}
{"id": "a6d1ea7c-162f-4dde-8f9d-64735e38e13f", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "After you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nVulgar language and profanity (News and Discover)\nGoogle has detected content on your site that appears to violate our vulgar language and profanity policy because it contains gratuitous obscenities or profanities.\nRecommended actions\nReview and update your pages to comply with the policy by removing the offensive content. (\nNews policy\n/\nDiscover policy\n)\nWhen you're sure that your site is no longer in violation of our policies, select\nRequest Review\non the Manual Actions report and provide evidence of changed editorial practices including new editorial policies and an editorial board with a history of improved practices.\nAfter you've submitted a reconsideration request, be patient and watch for review status messages in your Search Console account — we'll let you know when we've reviewed your site. If we determine that your site is no longer in violation of our policies, we'll revoke the manual action.\nSite reputation abuse\nGoogle has detected that a portion of your site is violating our spam policy on\nsite reputation abuse\n.\nThe current manual action affects pages with content that violates our policy. However, if your site repeatedly violates our site reputation abuse policy, that may lead to further manual actions and/or affect your site’s overall ranking.\nRecommended actions:\nReview Google’s\nspam policy on site reputation abuse\n, then follow these steps to identify and correct the violation(s) on your site:\nLook for any third-party content on your site that violates the\nsite reputation abuse policy", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 28, "chunk_char_start": 52975, "chunk_char_end": 54791}
{"id": "27fbce2c-4b2b-4e65-acf0-400d55125b4d", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": ". To get a better sense of where the violating content appears, review the list of patterns of affected pages in the message you received from Search Console (either in the Search Console Messages center, Manual Actions report, or by email).\nDecide what to do with the violating content and take action. For example:\nMove the violating content to a new domain.\nIf you link from the old site to the new site, use the\nnofollow\nattribute\nin those links. Avoid\nredirecting URLs\nfrom the old site to the new site, as redirecting may introduce the site reputation abuse issue again.\nUse\nnoindex\nto exclude the violating content from Search indexing\n. To make sure your noindex rule is effective, don't block that content with your robots.txt file.\nRedo the violating conten\nt as first-party content.\nRemove the violating content\nfrom your site.\nCaution\n: Moving the content to another established site or to a subdomain/subdirectory within your site may not resolve the underlying issue:\nMoving to a subdomain/subdirectory\nmay be viewed as an\nattempt to circumvent\nour spam policy, which could lead to broader actions against your site in Google Search.\nMoving to an established site\nmay introduce a site reputation abuse issue to the established site if that site has its own reputation and the third-party nature is unchanged.\nWhen you're sure that your site is no longer in violation of our spam policies, select “Request Review” on the\nManual Actions report\n.\nAfter you’ve submitted a reconsideration request, look for review status messages in your Search Console account — we’ll let you know when we’ve reviewed your site. If we determine that your site is no longer in violation of our spam policies, we’ll revoke the manual action.\nWhat's the difference between the Manual Actions report and the Security Issues report?\nThere is some conceptual overlap between the Manual Actions report and the Security Issues report, so it is useful to know the difference between them:\nThe Manual Actions report", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 29, "chunk_char_start": 54792, "chunk_char_end": 56791}
{"id": "c493377d-a2fb-42b6-9f7c-27624f9862a3", "url": "https://support.google.com/webmasters/answer/9044175?hl=en", "source_domain": "support.google.com", "title": "Manual actions report - Search Console Help", "section_path": [], "text": "lists manually detected issues with a page or site that are mostly attempts to manipulate our search index, but are not necessarily dangerous for users. Most issues reported here will result in pages or site being ranked lower or omitted from search results without any visual indication to the user.\nThe Security Issues report\nlists indications that your site was hacked, or behavior on your site that could potentially harm a visitor or their computer: for example, phishing attacks or installing malware or unwanted software on the user's computer. These pages can appear with a warning label in search results, or a browser can display an interstitial warning page when a user tries to visit them.\nWas this helpful?\nHow can we improve it?\nYes\nNo\nSubmit\nNeed more help?\nTry these next steps:\nPost to the help community\nGet answers from community members\ntrue\nNew to Search Console?\nNever used Search Console before? Start here, whether you're a complete beginner, an SEO expert, or a website developer.\nSearch\nClear search\nClose search\nGoogle apps\nMain menu\n4398134607553084735\ntrue\nSearch Help Center\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\n83844\nfalse\nfalse\nfalse\nfalse", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 30, "chunk_char_start": 56792, "chunk_char_end": 57957}
{"id": "8c77b2e1-938d-4de7-bb77-c62adab02ad8", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nTiếng Việt\nTürkçe\nРусский\nالعربيّة\nहिंदी\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nDocumentation\nSearch Console\nHome\nSearch Central\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nSpam policies for Google web search\nIn the context of Google Search, spam refers to techniques used to deceive users or manipulate\nour Search systems into ranking content highly. Our spam policies help protect users and improve the\nquality of Search results. To be eligible to appear in Google web search results (web pages,\nimages, videos, news content or other material that Google finds from across the web), content\nshouldn't violate\nGoogle Search's overall policies\nor the spam policies listed on this page. These policies apply to all web search results,\nincluding those from Google's own properties.\nWe detect policy-violating practices both through automated systems and, as needed,\nhuman review that can result in a\nmanual action\n.\nSites that violate our policies may rank lower in results or not appear in results at all.\nIf you believe that a site is violating Google's spam policies, let us know by\nfiling a search quality user report\n.\nWe're focused on developing scalable and automated solutions to problems, and we'll use these\nreports to further improve our spam detection systems.\nOur policies cover common spam practices, but Google may act against any type of spam practices\nwe detect.\nCloaking\nCloaking refers to the practice of presenting different content to users and search engines\nwith the intent to manipulate search rankings and mislead users. Examples of cloaking include:\nShowing a page about travel destinations to search engines while showing a page about", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1985}
{"id": "31aa2f44-c2ce-48e3-b1bf-dfa213fce3a5", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "discount drugs to users\nInserting text or keywords into a page only when the user agent that is requesting the\npage is a search engine, not a human visitor\nIf your site uses technologies that search engines have difficulty accessing, like\nJavaScript\nor\nimages\n,\nsee our recommendations for making that content accessible to search engines and users without cloaking.\nIf a site is hacked, it's not uncommon for the hacker to use cloaking to make the hack harder\nfor the site owner to detect. Read more about\nfixing hacked sites\nand avoiding being hacked.\nIf you operate a paywall or a content-gating mechanism, we don't consider this to be cloaking\nif Google can see the full content of what's behind the paywall just like any person who has\naccess to the gated material and if you follow our\nFlexible Sampling general guidance\n.\nDoorway abuse\nDoorway abuse is when sites or pages are created to rank for specific, similar search queries. They lead\nusers to intermediate pages that are not as useful as the final destination. Examples of\ndoorway abuse include:\nHaving multiple websites with slight variations to the URL and home page to maximize their reach for any specific query\nHaving multiple domain names or pages targeted at specific regions or cities that funnel users to one page\nGenerating pages to funnel visitors into the actual usable or relevant portion of a site\nCreating substantially similar pages that are closer to search results than a clearly defined, browseable hierarchy\nExpired domain abuse\nExpired domain abuse is where an expired domain name is purchased and repurposed primarily to\nmanipulate search rankings by hosting content that provides little to no value to users. Illustrative\nexamples include, but are not limited to:\nAffiliate content on a site previously used by a government agency\nCommercial medical products being sold on a site previously used by a non-profit medical charity\nCasino-related content on a former elementary school site\nHacked content", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1986, "chunk_char_end": 3973}
{"id": "b67d9429-3603-4dbb-a79c-216aa32e6159", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Hacked content is any content placed on a site without permission, due to vulnerabilities in a\nsite's security. Hacked content gives poor search results to our users and can potentially\ninstall malicious content on their machines. Examples of hacking include:\nCode injection\n: When hackers gain access to your website, they might try\nto inject malicious code into existing pages on your site. This often takes the form of\nmalicious JavaScript injected directly into the site, or into iframes.\nPage injection\n: Sometimes, due to security flaws, hackers are able to add new\npages to your site that contain spammy or malicious content. These pages are often meant to\nmanipulate search engines or to\nattempt phishing\n.\nYour existing pages might not show signs of hacking, but these newly-created pages could\nharm your site's visitors or your site's performance in search results.\nContent injection\n: Hackers might also try to subtly manipulate existing pages on\nyour site. Their goal is to add content to your site that search engines can see but which\nmay be harder for you and your users to spot. This can involve adding\nhidden links or hidden text\nto a page by using CSS or HTML, or it can involve more complex changes like\ncloaking\n.\nRedirects\n: Hackers might inject malicious code to your website that redirects some\nusers to harmful or spammy pages. The kind of redirect sometimes depends on the referrer,\nuser agent, or device. For example, clicking a URL in Google Search results could redirect\nyou to a suspicious page, but there is no redirect when you visit the same URL directly\nfrom a browser.\nHere are our tips on\nfixing hacked sites\nand avoiding being hacked.\nHidden text and link abuse\nHidden text or link abuse is the practice of placing content on a page in a way solely to manipulate\nsearch engines and not to be easily viewable by human visitors. Examples of hidden text or\nlink abuse include:\nUsing white text on a white background\nHiding text behind an image", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3974, "chunk_char_end": 5950}
{"id": "88b45ef7-53f7-49d1-a259-40dfa930b228", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Using CSS to position text off-screen\nSetting the font size or opacity to 0\nHiding a link by only linking one small character (for example, a hyphen in the middle of a paragraph)\nThere are many web design elements today that utilize showing and hiding content in a dynamic\nway to improve user experience; these elements don't violate our policies:\nAccordion or tabbed content that toggle between hiding and showing additional content\nSlideshow or slider that cycles between several images or text paragraphs\nTooltip or similar text that displays additional content when users interact with over an element\nText that's only accessible to screen readers and is intended to improve the experience\nfor those using screen readers\nKeyword stuffing\nKeyword stuffing refers to the practice of filling a web page with keywords or numbers in an\nattempt to manipulate rankings in Google Search results. Often these keywords appear in a list\nor group, unnaturally, or out of context. Examples of keyword stuffing include:\nLists of phone numbers without substantial added value\nBlocks of text that list cities and regions that a web page is trying to rank for\nRepeating the same words or phrases so often that it sounds unnatural. For example:\nUnlimited app store credit. There are so many sites that claim to offer app store\ncredit for $0 but they're all fake and always mess up with users looking for unlimited app\nstore credits. You can get limitless credits for app store right here on this website.\nVisit our unlimited app store credit page and get it today!\nLink spam\nLink spam is the practice of creating links to or from a site primarily for the purpose of\nmanipulating search rankings. The following are examples of link spam:\nBuying or selling links for ranking purposes. This includes:\nExchanging money for links, or posts that contain links\nExchanging goods or services for links\nSending someone a product in exchange for them writing about it and including a link", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5951, "chunk_char_end": 7914}
{"id": "654a03da-bef4-411f-999d-37f9148a99a8", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Excessive link exchanges (\"Link to me and I'll link to you\") or partner pages exclusively for the sake of cross-linking\nUsing automated programs or services to create links to your site\nRequiring a link as part of a Terms of Service, contract, or similar arrangement without\nallowing a third-party content owner the choice of\nqualifying the outbound link\nText advertisements or text links that don't block ranking credit\nAdvertorials or native advertising where payment is received for articles that include\nlinks that pass ranking credit, or links with optimized anchor text in articles, guest posts,\nor press releases distributed on other sites. For example:\nThere are many\nwedding rings\non the market.\nIf you want to have a\nwedding\n, you will have to pick\nthe\nbest ring\n. You will also need to\nbuy flowers\nand a\nwedding dress\n.\nLow-quality directory or bookmark site links\nKeyword-rich, hidden, or low-quality links embedded in widgets that are distributed across various sites\nWidely distributed links in the footers or templates of various sites\nForum comments with optimized links in the post or signature, for example:\nThanks, that's great info!\n- Paul\npaul's pizza\nsan diego pizza\nbest pizza san diego\nCreating low-value content primarily for the purposes of manipulating linking and ranking signals\nGoogle does understand that buying and selling links is a normal part of the economy of the\nweb for advertising and sponsorship purposes. It's not a violation of our policies to have\nsuch links as long as they are\nqualified\nwith a\nrel=\"nofollow\"\nor\nrel=\"sponsored\"\nattribute value to the\n<a>\ntag.\nMachine-generated traffic\nMachine-generated traffic (also called\nautomated traffic\n)\nrefers to the practice of sending automated queries to Google. This includes scraping results for\nrank-checking purposes or other types of automated access to Google Search conducted without\nexpress permission. Machine-generated traffic consumes resources and interferes with our ability", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7915, "chunk_char_end": 9892}
{"id": "efe4fd7f-cae6-40e5-809f-1d3fcd2957ad", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "to best serve users. Such activities violate our spam policies and the\nGoogle Terms of Service\n.\nMalware and malicious practices\nGoogle checks to see whether websites host malware or unwanted software that negatively\naffects the user experience.\nMalware\nis any\nsoftware or mobile application specifically designed to harm a computer, a mobile device, the\nsoftware it's running, or its users. Malware exhibits malicious behavior that can include\ninstalling software without user consent and installing harmful software such as viruses. Site\nowners sometimes don't realize that their downloadable files are considered malware, so these\nbinaries might be hosted inadvertently.\nUnwanted software\nis an executable file or mobile application that engages in behavior that is deceptive, unexpected,\nor that negatively affects the user's browsing or computing experience. Examples include\nsoftware that switches your home page or other browser settings to ones you don't want, or\napps that leak private and personal information without proper disclosure.\nSite owners should make sure they don't violate the\nUnwanted Software Policy\nand\nfollow our guidelines\n.\nMisleading functionality\nMisleading functionality refers to the practice of intentionally creating sites that trick users\ninto thinking they would be able to access some content or services but in reality can't. Examples\nof misleading functionality include:\nA site with a fake generator that claims to provide app store credit but doesn't actually provide the credit\nA site that claims to provide certain functionality (for example, PDF merge, countdown\ntimer, online dictionary service), but intentionally leads users to deceptive ads rather\nthan providing the claimed services\nScaled content abuse\nScaled content abuse is when many pages are generated for the primary purpose of manipulating\nsearch rankings and not helping users. This abusive practice is typically focused on creating large", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9893, "chunk_char_end": 11838}
{"id": "4fe4bd99-9d97-44cc-b674-4add7be1ab43", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "amounts of unoriginal content that provides little to no value to users, no matter how it's created.\nExamples of scaled content abuse include, but are not limited to:\nUsing generative AI tools or other similar tools to generate many pages without adding value\nfor users\nScraping feeds, search results, or other content to generate many pages (including through\nautomated transformations like synonymizing, translating, or other obfuscation techniques), where\nlittle value is provided to users\nStitching or combining content from different web pages without adding value\nCreating multiple sites with the intent of hiding the scaled nature of the content\nCreating many pages where the content makes little or no sense to a reader but contains search\nkeywords\nIf you're hosting such content on your site,\nexclude it from Search\n.\nScraping\nScraping refers to the practice of taking content from other sites, often through automated means,\nand hosting it with the purpose of manipulating search rankings. Examples of abusive scraping include:\nRepublishing content from other sites without adding any original content\nor value, or even citing the original source\nCopying content from other sites, modify it only slightly (for example, by\nsubstituting synonyms or using\nautomated techniques\n),\nand republish it\nReproducing content feeds from other sites without providing some type of unique\nbenefit to the user\nCreating sites dedicated to embedding or compiling content, such as videos, images, or other media\nfrom other sites, without substantial added value to the user\nSite reputation abuse\nSite reputation abuse is a tactic where third-party content is published on a host site mainly\nbecause of that host's already-established ranking signals, which it has earned primarily from its\nfirst-party content. The goal of this tactic is for the content to rank better than it could\notherwise on its own.\nHaving third-party content alone isn't a violation of the site reputation abuse policy; it's only", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11839, "chunk_char_end": 13833}
{"id": "f59cb75d-8b83-46ff-927d-f36f4da578fc", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "a violation if the third-party content is published on a host site mainly because of that host\nsite's already-established ranking signals. Examples of site reputation abuse include, but are not\nlimited to:\nAn educational site hosting a page about  sponsored reviews of payday loans written by a third-party that\ndistributes the same page to other sites across the web\nA medical site hosting a third-party advertising page about \"best casinos\" that readers\nwouldn't expect and that's being placed on the site to rank better due to the established site's\nranking signals\nA movie review site hosting third-party pages about topics that would be confusing to users to\nfind on a movie review site (such as \"ways to buy followers on social media sites\", the \"best\nfortune teller sites\", and the \"best essay writing services\")\nA news site hosting coupons provided by a third-party white-label service where the main\nreason for publishing the coupons on the news site is to capitalize on the news\nsite's reputation\nAn established first party site branches out into a new area primarily using freelance content\nbecause this content will rank better on the first-party site than it would have otherwise\nIf you're hosting pages that violate this policy, learn how to\ncorrect this issue\n.\nExamples that are\nNOT\nconsidered site reputation abuse include:\nWire service or press release service sites\nNews publications that have syndicated news content from other news publications\nSites designed to allow user-generated content, such as a forum website or comment sections\nColumns, opinion pieces, articles, and other work of an editorial nature\nThird-party content (for example, \"advertorial\" or \"native advertising\" type pages)\nwhere the purpose is to share content directly\nto readers (such as through promotion within the publication itself), rather than hosting the\ncontent to manipulate search rankings\nUsing affiliate links throughout a page, with\nlinks treated appropriately\n,", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13834, "chunk_char_end": 15803}
{"id": "bdf29797-8b93-41ef-a43f-8b8265a79864", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "or embedding third-party ad units throughout a page\nCoupons that are sourced directly from merchants and other businesses that serve consumers\nSneaky redirects\nRedirecting is the act of sending a visitor to a different URL than the one they initially\nrequested. Sneaky redirecting is the practice of doing this maliciously in order to either show users and search\nengines different content or show users unexpected content that does not fulfill their\noriginal needs. Examples of sneaky redirects include:\nShowing search engines one type of content while redirecting users to something significantly\ndifferent\nShowing desktop users a normal page while redirecting mobile users to a completely\ndifferent spam domain\nWhile sneaky redirection is a type of spam,  there are many legitimate, non-spam reasons to\nredirect one URL to another. Examples of legitimate redirects include:\nMoving your site to a new address\nConsolidating several pages into one\nRedirecting users to an internal page once they are logged in\nWhen examining if a redirect is sneaky, consider whether or not the redirect is intended to\ndeceive either the users or search engines. Learn more about how to appropriately\nemploy redirects on your site\n.\nThin affiliation\nThin affiliation is the practice of publishing content with product affiliate links where the\nproduct descriptions and reviews are copied directly from the original merchant without any\noriginal content or added value.\nAffiliate pages can be considered thin if they are a part of a program that distributes its\ncontent across a network of affiliates without providing additional value. These sites often\nappear to be cookie-cutter sites or templates with the same or similar content replicated\nwithin the same site or across multiple domains or languages. If a Search results page\nreturned several of these sites, all with the same content, thin affiliate pages would create\na frustrating user experience.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15804, "chunk_char_end": 17742}
{"id": "9d337be0-7aa8-4dad-83fd-d1775bdde4df", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "Not every site that participates in an affiliate program is a thin affiliate. Good affiliate\nsites add value by offering meaningful content or features. Examples of good affiliate pages include offering\nadditional information about price, original product reviews, rigorous testing and ratings,\nnavigation of products or categories, and product comparisons.\nUser-generated spam\nUser-generated spam is spammy content added to a site by users through a channel intended\nfor user content. Often site owners are unaware of the spammy content. Examples of spammy\nuser-generated content include:\nSpammy accounts on hosting services that anyone can register for\nSpammy posts on forum threads\nComment spam on blogs\nSpammy files uploaded to file hosting platforms\nHere are several tips on how to\nprevent abuse of your site's public areas\n.\nHere are our tips on\nfixing hacked sites\nand avoiding being hacked.\nOther practices that can lead to demotion or removal\nLegal removals\nWhen we receive a significant volume of\nvalid copyright removal requests\ninvolving a given site,\nwe are able to use that\nto demote other content from the site in our results. This way, if there is other\ninfringing content, people are less likely to encounter it versus the original content. We\napply similar demotion signals to complaints involving defamation, counterfeit goods, and\ncourt-ordered removals. In the case of child sexual abuse material (CSAM), we always remove\nsuch content when it is identified and we demote all content from sites with a significant\nproportion of CSAM content.\nPersonal information removals\nIf we process a significant volume of personal information removals involving a site with\nexploitative removal practices\n,\nwe demote other content from the site in our results.\nWe also look to see\nif the same pattern of behavior is happening with other sites and, if so, apply demotions to\ncontent on those sites. We may apply similar demotion practices for sites that receive a significant", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17743, "chunk_char_end": 19725}
{"id": "e0d54d2e-dbea-439c-89fa-8f411cb0f98b", "url": "https://developers.google.com/search/docs/essentials/spam-policies", "source_domain": "developers.google.com", "title": "Spam Policies for Google Web Search | Google Search Central  |  Documentation  |  Google for Developers", "section_path": [], "text": "volume of removals of content involving\ndoxxing content\n,\nexplicit\npersonal imagery created or shared without consent\n, or\nexplicit\nnon-consensual fake content\n.\nPolicy circumvention\nIf a site continues to engage in actions intended to bypass our spam policies or\ncontent policies for Google Search\n,\nwe may take appropriate action which may include restricting or removing eligibility for\nsome of our search features (for example, Top Stories, Discover) and taking broader action in\nGoogle Search (for example, removing more sections of a site from Search results). Circumvention\nincludes but is not limited to:\nUsing existing or creating new subdomains, subdirectories, or sites with the intention of\ncontinuing to violate our policies\nUsing other methods intended to continue distributing content or engaging in a behavior\nthat aims to violate our policies\nScam and fraud\nScam and fraud come in many forms, including but not limited to impersonating an official\nbusiness or service through imposter sites, intentionally displaying false information about\na business or service, or otherwise attracting users to a site on false pretenses. Using\nautomated systems, Google seeks to identify pages with scammy or fraudulent content and\nprevent them from showing up in Google Search results. Examples of online scams and fraud include:\nImpersonating a well-known business or service provider to trick users into paying money\nto the wrong party\nCreating deceptive sites pretending to provide official customer support on behalf of a\nlegitimate business or provide fake contact information of such business\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-10 UTC.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19726, "chunk_char_end": 21668}
{"id": "3cda836f-173a-44a8-aab0-57b0c7516e7e", "url": "https://developers.google.com/search/blog/2014/04/webmaster-guidelines-for-sneaky", "source_domain": "developers.google.com", "title": "Webmaster Guidelines for sneaky redirects updated  |  Google Search Central Blog  |  Google for Developers", "section_path": [], "text": "Webmaster Guidelines for sneaky redirects updated  |  Google Search Central Blog  |  Google for Developers\nSkip to main content\nGoogle\nSearch Central\n/\nEnglish\nDeutsch\nEspañol\nFrançais\nIndonesia\nPortuguês – Brasil\nРусский\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nSign in\nGoogle Search Central Blog\nHome\nSearch Central\nGoogle Search Central Blog\nSend feedback\nWebmaster Guidelines for sneaky redirects updated\nStay organized with collections\nSave and categorize content based on your preferences.\nWednesday, April 30, 2014\nRedirects are often used by webmasters to help forward visitors from one page to another. They are\na normal part of how the web operates, and are very valuable when well used. However, some\nredirects are designed to manipulate or deceive search engines or to display different content to\nhuman users than to search engines. Our\nquality guidelines\nstrictly forbid these kinds of redirects.\nFor example, desktop users might receive a normal page, while hackers might redirect all mobile\nusers to a completely different spam domain. To help webmasters better recognize problematic\nredirects, we have updated our quality guidelines for\nsneaky redirects\nwith examples\nthat illustrate redirect-related violations.\nWe have also updated the\nhacked content guidelines\nto include\nredirects on compromised websites. If you believe your site has been compromised, follow these\ninstructions to\nidentify the issues on your site and fix them\n.\nAs with any violation of our quality guidelines, we may take manual action, including removal from\nour index, in order to maintain the quality of the search results. If you have any questions about\nour guidelines, you can ask in our\nWebmaster Help Forum\n.\nPosted by\nAaseesh Marina\n, Search Quality Team\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1981}
{"id": "62909fd8-397b-4211-8222-e4f102c00960", "url": "https://developers.google.com/search/blog/2014/04/webmaster-guidelines-for-sneaky", "source_domain": "developers.google.com", "title": "Webmaster Guidelines for sneaky redirects updated  |  Google Search Central Blog  |  Google for Developers", "section_path": [], "text": ". Java is a registered trademark of Oracle and/or its affiliates.", "engine": "google", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1982, "chunk_char_end": 2047}
{"id": "d0e182d1-d8d6-4162-b057-e3014db12174", "url": "https://digitalmarketinginstitute.com/blog/the-ultimate-guide-to-bad-seo-practices", "source_domain": "digitalmarketinginstitute.com", "title": "The Ultimate Guide to Bad SEO Practices | Digital Marketing Institute", "section_path": [], "text": "The Ultimate Guide to Bad SEO Practices | Digital Marketing Institute\nCourses\nResources\nSign In\nView\nCourses\nLogin\nView Courses\n- - -\nCourses\nResources\n← Back to\nArticles\nFeb 26, 2018\nThe Ultimate Guide to Bad SEO Practices\nArticles\n•\nsearch engine optimization\nby\nDigital Marketing Institute\nShare via:\nSearch Engine Optimization (SEO) is an integral part of any business with an online presence. Like most elements of digital, SEO is constantly evolving and it's important to keep up with these changes to ensure you're in line with best practices.\nBelow, we’ll give you an overview of the good, the bad and the ugly of today’s SEO practices.\nWhat is “White Hat” SEO?\nWhite hat SEO is the good kind that plays by the rules and follows\nGoogle’s guidelines\n. Generally, this means that the content produced on any given site is going to be genuinely high quality and useful for a given user. This means it will also rank well. But you don’t have to be breaking any rules to be low quality and low ranking; something as simple as a page loading slowly or a non-responsive site can put you behind in the rankings simply because it creates a less favorable experience for the user.\nExamples of White Hat principles that businesses should be sticking to include:\nHigh-quality content that adds value to the customer experience, such as how-to videos and articles\nDescriptions and keyword clusters that are on-brand and sensible\nA visually appealing website that is easy to navigate, fast and mobile-friendly\nWhat is “Black Hat” SEO?\nBlack Hat SEO basically refers to SEO strategies that equate to cheating or hacking Google’s terms in order to boost a website’s ranking. When there’s so much competition out there, it’s understandable that people will find ways to cheat, however, when they cheat, it only makes Google come down on them hard. A search engine (Google) will then change the rules again in order to prevent that kind of action from happening again in the future.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1972}
{"id": "2ac8f3d0-78ce-424a-9ae6-c082ecd48f2c", "url": "https://digitalmarketinginstitute.com/blog/the-ultimate-guide-to-bad-seo-practices", "source_domain": "digitalmarketinginstitute.com", "title": "The Ultimate Guide to Bad SEO Practices | Digital Marketing Institute", "section_path": [], "text": "All in all, the aim is that consumers are delivered quality content and aren’t spammed or scammed in any way. Search indexes have a responsibility and a focus on making sure this happens.\nHere are a few examples of some common Black Hat SEO techniques.\nKeyword Stuffing\nEffective content needs to be smooth and readable in order to ensure an optimal user experience. Google knows this too – which is why it punishes those who try to cram too many keywords into a piece of content. Keyword stuffing is just like it sounds: adding a ton of keywords to a piece of content in the interest of getting more page views and increasing Google ranking. Boosting your SEO on a website relies heavily on keyword placement and weight.\nWhen you have the right keyword density – many people recommend a maximum of 3 keywords for a short piece of content, but there are no hard and fast rules – your content will be more organic and readable, thus offering better value to the user.\nIrrelevant Keywords\nIf your keywords are unrelated to your actual business, you’ll likely get higher bounce rates, and this can backfire when it comes to your SEO ranking. You aim to build authority, so anything you can do to genuinely attract and keep customers should essentially be rewarded via a higher ranking.\nBlog Spam\nBlog comment sections can be excellent spaces for audience engagement and brand building for pretty much any type of business. Spammers love to place links, illegible and irrelevant comments in these areas in order to try to get more traffic back to their own sites. It’s important that spam comments be controlled so as not to damage the reputation of your site in the eyes of search engines.\nSuch comments may be viewed as irrelevant content by search engine crawlers, and links may also damage your ranking. There are plenty of free or built-in tools to help manage spam on blogs such as\nAkismet\n(for WordPress) and\nDisqus\n.\nCloaking", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1973, "chunk_char_end": 3902}
{"id": "cb52f361-a6b9-4476-b2bd-ef3f65b51902", "url": "https://digitalmarketinginstitute.com/blog/the-ultimate-guide-to-bad-seo-practices", "source_domain": "digitalmarketinginstitute.com", "title": "The Ultimate Guide to Bad SEO Practices | Digital Marketing Institute", "section_path": [], "text": "Just like it sounds, cloaking is a trick of deception that consists of creating a different page to show the search engine and hiding the main page that you’re showing to viewers. An example of cloaking is when a web page is programmed to show an HTML page to search engines when viewers actually see a set of images. Like link farms, this is another form of spamdexing.\nThe penalty for this can be as harsh as a permanent ban, so it’s just really not a good idea to risk it.\nDuplicated Content\nAlso known in the publishing world as straight up plagiarism, duplicate content between different domains is theft. In the wild, wild world of the internet, people do it anyway to try to get traffic. But it does not generally make for a positive user experience.\nNote that what we’re talking about is directly stealing content and not scraping, which is essentially an automated process whereby a bot extracts information from websites. There are also other instances where duplicate content is acceptable, such as discussion forums where content might be duplicated across pages in order to facilitate conversation.\nSpinning Content\nContent or article “spinning” essentially means grabbing high-ranking pieces of content and rewriting it using the same keywords for your own benefit. Not only is it a bad idea because it will be punished by Google, it’s also just not good for the reputation of your business – for instance, if you get a robot to do it (or even Google translate), you won’t get any engagement because it will just be bad writing.\nIf you’re ever tempted to use this technique, it’s much more beneficial to consider something similar to taking an article you love and re-creating it with your own ideas in a new way. It’s not the worst idea to use the same keywords, just be sure to do it in a way that adds specific value to your own customer base and you’ll be in the clear.\nClickbait", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3903, "chunk_char_end": 5800}
{"id": "021bc557-d97a-4a6a-b838-6728454fe6b6", "url": "https://digitalmarketinginstitute.com/blog/the-ultimate-guide-to-bad-seo-practices", "source_domain": "digitalmarketinginstitute.com", "title": "The Ultimate Guide to Bad SEO Practices | Digital Marketing Institute", "section_path": [], "text": "Hyperbolic blog titles such as those that use the words “shocking” or “terrifying” in order to lure people onto a page in a misleading fashion are clickbait. And while there may not be a specific punishment, it’s just a tacky marketing approach. If you want long-term loyalty, your customers have to respect and engage with your content on an authentic level – clickbait just doesn’t facilitate this.\nLink Farms\nLink farms are a form of “spamdexing” which means that a website is “spamming” a search engine. This is a black hat SEO approach that you will definitely want to avoid at all costs. A link farm is a cluster of websites that hyperlink to each other in order to collectively build rank in search indexes. Most of these are built via automated services.\nHidden Text and Links\nProgrammers have the capacity to design pages that search engines can see but something different is shown to the readers. This may be a cloaking type strategy where a whole page is hidden, or it may be as simple as “hiding” certain pieces of text or links by using a white font or making the font very small.\nFinal Thoughts\nThere are plenty of SEO tactics that you should be avoiding and only a few are listed here. It’s important that digital experts across all spectrums stay on top of these rules and act accordingly – not just to avoid punishment, but because they want to remain clear and transparent in their long-term business behavior.\nLearn the tools and technologies needed to meet the challenges of tomorrow with a\nPostgraduate Diploma in Digital Marketing\n. Download a brochure today!\nRelated Free Video Lessons\n▶\nOptimizing SEO\nBuilding Backlinks\n▶\nSEO (Search Engine Optimization)\nSEO and Keywords\n▶\nContent Marketing For Search\nE-A-T and Quality Content\n▶\nSEO Setup\nTypes of Keywords\nView Courses\nShare via:\nCategories:\narticles\n•\nsearch engine optimization\n•", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5801, "chunk_char_end": 7661}
{"id": "aa537f64-69da-4205-99e1-07fa9634c359", "url": "https://searchengineland.com/guide/seo/violations-search-engine-spam-penalties", "source_domain": "searchengineland.com", "title": "SEO Guide: Toxins & Search Engine Spam Penalties", "section_path": [], "text": "SEO Guide: Toxins & Search Engine Spam Penalties\nIs your organic traffic disappearing?\nSee why competitors are outranking you and take back your visibility with Semrush.\nIdentify traffic-stealing competitors\nFind untapped keyword opportunities\nCreate content that ranks higher\nInput\nInstant analysis, no credit card needed.\nSEO\n»\nSEO, PPC & AIO Guides\n»\nEssential Guide to SEO: Master the science of SEO\n»\nChapter 8: Toxins & search engine spam penalties\nChapter 8: Toxins & search engine spam penalties\nChat with SearchBot\nChat with SearchBot\nConversations\nNew Chat\nDownload TXT\nPowered by Search Engine Land\nClose\nAnyone entering the realm of search engine optimization is likely to encounter some questionable (aka “black hat”) tactics, or Toxins, as we call them in our\nPeriodic Table of SEO Factors\n.\nThese are shortcuts, or tricks, that may have been sufficient to guarantee a high ranking back in the day when the engines’ methods were much less sophisticated. (They might even work now, at least until you’re caught.) We recommend staying far away from these tactics, because employing them could result in a penalty or ban.\nRest assured, It’s hard to accidentally spam a search engine, and the engines look at a variety of signals before deciding if someone deserves a harsh penalty. That said, let’s talk about things not to do.\nCl: Cloaking\nShowing search engine crawlers something different than what you present to users is called “\ncloaking\n,” and it can potentially be used to trick users into visiting irrelevant or harmful pages.\nUnlike some of the other Toxins, cloaking is not something that can happen by accident — it’s a deliberate attempt to manipulate search results, and if you’re caught doing it, you can expect a very heavy penalty.\nWhat about JavaScript issues? “Cloaking is specifically against\nGoogle guidelines", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1841}
{"id": "13e3b612-6b6a-472f-bd41-d910e11b20e9", "url": "https://searchengineland.com/guide/seo/violations-search-engine-spam-penalties", "source_domain": "searchengineland.com", "title": "SEO Guide: Toxins & Search Engine Spam Penalties", "section_path": [], "text": ", but those guidelines are murky at the moment because of JavaScript,” says Search Engine Land’s Detlef Johnson. “One server side rendering (SSR) solution offers it only dynamically in concert with spider spotting. When you handle a request from Googlebot, you can opt to do SSR whereas all other requests are handled normally, delivering scripts for rendering in the browser. That, technically speaking, is cloaking, but Google looks the other way because they’re aware of the intent.”\n“As long as your intent is not suspicious, you can do this and expect to not get banned. It’s when you reserve some content for spiders that you don’t display to users that things start to cross the line,” explains Johnson.\nFor more, see our articles on\nSEO: Cloaking and Doorway Pages\n.\nSf: Stuffing\nYou might assume that the more times a keyword shows up on a page, the more relevant search engines will consider the page to be to the query. Nope. Inserting keywords more often than is natural or useful to users is called “keyword stuffing.” It’s one of the oldest spam tactics out there and it can still get you penalized.\nDon’t repeat keywords over and over again in your headings, copy, footers — anywhere — to try to improve your rankings. There is no magic formula for keyword frequency, and\nkeyword density is a myth\n.\nInstead, focus on addressing the user’s intent. Whether that results in a keyword occurring only a couple of times or over a dozen times is far less important than the quality of your content and the value it provides to your audience.\nAr: Piracy\nRipping off someone else’s intellectual property — an article, song, graphic, photo, video, etc. — and passing it off as your own is illegal. That’s not the only reason why it’s bad for SEO, though: users generally want the original source of the content, and search engines want to provide it for them.\nGoogle’s\n2012 “Pirate” update", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1842, "chunk_char_end": 3737}
{"id": "2d246e2a-49b8-4cf6-942c-432a5d385ae3", "url": "https://searchengineland.com/guide/seo/violations-search-engine-spam-penalties", "source_domain": "searchengineland.com", "title": "SEO Guide: Toxins & Search Engine Spam Penalties", "section_path": [], "text": "took aim at sites infringing on copyright law. Sites are subject to Digital Millennium Copyright Act (DCMA) takedown requests. Plagiarizing or hosting plagiarized or illegal content can get you delisted from search results. Check your Google Search Console notifications if you suspect that a DMCA takedown request has been filed against you.\nSc: Schemes\nSeeking backlinks is an essential aspect of SEO, but the rules change when money is involved. Paying for links that pass link equity violates both\nGoogle\nand\nBing\n’s guidelines, and doing so can have dire consequences for your organic visibility.\n“You could be penalized or banned by Google, and neither is a good situation,” says Julie Joyce, director of operations at Link Fish Media. “Depending upon how bad the problem is, it can take anywhere from a few months to a few years to get back to where you were.”\nTo be clear, you can pay to have a backlink placed on another entity’s website (as would be the case with ads), but those links cannot pass link equity. Paid links should be indicated with either a\nrel=“nofollow” or rel=“sponsored”\nlink attribute.\nSchemes aren’t just limited to buying links, either: large-scale guest posting services with keyword-laden anchors, link exchanges, blog spamming and other illicit practices may also result in penalties from search engines. There are\nnumerous examples\nof brands getting busted for attempting to manipulate search algorithms using these methods — even\ninvolving Google itself\n. If you choose to ignore Google’s rules, be prepared for little mercy if caught. And don’t believe programs that tell you their paid links are undetectable. They’re not, especially when so many of the cold-call ones are\nrun by idiots\n.\nIt’s far better to see your rankings gradually rise over time than take shortcuts and have to claw your way back after a penalty.\nFor more, see our articles on\nLink Building: Paid Links\nand\nSEO Spamming\n.\nHi: Hiding", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3738, "chunk_char_end": 5681}
{"id": "8e9797ea-2c78-4343-8e9e-123ba88df35f", "url": "https://searchengineland.com/guide/seo/violations-search-engine-spam-penalties", "source_domain": "searchengineland.com", "title": "SEO Guide: Toxins & Search Engine Spam Penalties", "section_path": [], "text": "Site owners who stuff keywords into their pages may also try to obscure those attempts by hiding the text. Whether it’s by matching the font color to the background, positioning text off screen, decreasing font size to zero or any other method of concealment, hiding text is a\nviolation\nof Google’s Webmaster Guidelines and can result in a penalty.\nLinks may also be styled in a way to make them invisible to users, which some site owners might do to visually obscure paid links while attempting to pass link equity. Whatever reason you may have, hiding elements isn’t something that users benefit from and is unlikely to\nimprove your SEO\n.\nThere is, however, the case of expandable content that reveals itself when the user interacts with it; for example, mousing over a link within a Wikipedia article may reveal more information.\nExpandable content is not against search engine policies.\nIv: Intrusive\nWhether the obstacle is an interstitial, a deluge of ads or some other intrusive element, making visitors jump through hoops to find what they’re looking for can hurt your user experience as well as your organic visibility.\nOften used in attempts to extract revenue or manipulate site metrics, these types of bad practices are what Google’s\nPage Layout algorithm\n, also known as the\nTop Heavy Update\n, was created to address.\nFor better or worse, interstitials are now a common part of the mobile user experience. In 2017, Google rolled out the\nmobile intrusive interstitial penalty\nto discourage site owners from abusing such elements.\nMore recently, Google updated its\nSearch Quality Evaluator Guidelines\nto address this trend, stating, “A single pop-over Ad or interstitial page with a clear and easy-to-use close button is not terribly distracting, though may not be a great user experience. However, difficult-to-close Ads that follow page scrolls, or interstitial pages that require an app download, can be truly distracting and make the MC [main content] difficult to use.”\nSource:\nGoogle", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5682, "chunk_char_end": 7682}
{"id": "9e3db2f9-54d0-429d-92b6-6291f31c3a6e", "url": "https://searchengineland.com/guide/seo/violations-search-engine-spam-penalties", "source_domain": "searchengineland.com", "title": "SEO Guide: Toxins & Search Engine Spam Penalties", "section_path": [], "text": "Not all interstitials are liabilities. If “used responsibly,” interstitials pertaining to legal obligations (such as privacy or age verification), login dialogs and other banners that use a “reasonable amount of screen space and are easily dismissible” would not be affected by\nGoogle’s mobile intrusive interstitial penalty\n.\n[Pro Tip]\n“There is a lot of chatter around ‘is CTR a ranking factor? Is dwell time a ranking factor?’ And it leads some people to try to artificially increase the time people spend on the page, but for bad reasons . . . Interstitials, that’s one way, essentially, to waste the time of your users. Sometimes, we’ll see some pages that are not going to load the content fully and then you click on the button and it’s going to say, ‘Oh, wait a second, time for us to load the content,’ as if it took 10 seconds to call a database and build the content.\nThese are all tactics that clearly are made to artificially increase dwell time. It sounds very petty because that’s just wasting time of the users for the sake of fulfilling an SEO urban legend. But also, it is harmful to our [search engine] users. So, this is something that we definitely recommend against and that we reserve the right of taking action if it is really abusive.”\n–Frédéric Dubut, senior program manager lead for Bing\nSEO Guide\nchapters:\nHome\n–\n1: Factors\n–\n2: Content\n–\n3: Architecture\n–\n4: HTML\n–\n5: Trust\n–\n6: Links\n–\n7: User\n–\n8: Toxins\n–\n9: Emerging\nNew on Search Engine Land\nNils Rooijmans speaks on when ignoring Google emails can cost you\nThe latest jobs in search marketing\nGoogle tests regional member pricing in Shopping Ads\nCloudflare: 416 billion AI bot requests blocked since July\nEcommerce PPC: 4 takeaways that shape how campaigns perform", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7683, "chunk_char_end": 9434}
{"id": "5967bd62-848d-408f-9212-39d14532c75b", "url": "https://www.seocasestudy.com/seo-examples/black-hat-seo", "source_domain": "www.seocasestudy.com", "title": "14 Black Hat SEO Examples & Case Studies", "section_path": [], "text": "14 Black Hat SEO Examples & Case Studies\nSEOcasestudy.com\n14 Black Hat SEO Examples & Case Studies\nâ\nMore SEO Examples\nTable of Contents\nThis is also a heading\nThis is a heading\nWritten by Ben Goodey\nWhat is black hat SEO?\nBlack Hat SEO is an attempt to improve search rankings using techniques search engines disapprove of in their guidelines.\nTypically black hat SEO is an attempt to âcheatâ the algorithm.\nWhen Googleâs algorithm was less advanced, this was easy.\nYet, many black hat SEO techniques still work today (yep, paid link building is black hat)\nAs Google gets smarter they aim to penalize anyone using these techniques. Your short-term wins may become long-term disaster (so, it's best to avoid the majority of them).\nIn this article:\nBlack hat SEO techniques to avoid\nReal world examples showing you why\nâAnother useful test is to ask, \"Does this help my users? Would I do this if search engines didn't exist?\"âGoogle\n10 black hat techniques to avoid\nCloaking\nWebsites deploying cloaking look at the IP address of incoming visitors to determine if theyâre a real human or a robot (typically the ârobotâ is Googleâs crawlers)\nThey then show different content depending on which the visitor is.\nA website could show Google that a web page is about one topic, but the human visitors land on a page thatâs more nefarious (like one about gambling or discount drugs for sale).\nDoorways\nâDoorways are sites or pages created to rank for specific, similar search queries.ââ\nGoogleâs Spam Policies\nExamples:\nMultiple eCommerce pages with tiny variations (e.g. /camera-accessories and /camera-accessories-kids and /camera-accessories-adults) that when you visit them, they simply link to the main category ages (e.g. /camera-accessories). All the pages do is funnel the visitor to the actual usable portion of the site.\nHaving multiple websites with slight variations to the URL and home page to maximize their reach for any specific query", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1972}
{"id": "e6864f8b-7f85-412d-98c4-4140a1f044b7", "url": "https://www.seocasestudy.com/seo-examples/black-hat-seo", "source_domain": "www.seocasestudy.com", "title": "14 Black Hat SEO Examples & Case Studies", "section_path": [], "text": "Having multiple domain names or pages targeted at specific regions or cities that funnel users to one page\nIrrelevant keywords\nKeyword stuffing\ninvolves filling your page with keywords with the aim of manipulating Googleâs algorithm.Â\nExamples:\nSentences and paragraphs that hardly make sense because of the overuse of keywords.\nRandom lists of keywords and keyword variations on the page.\nGoogle includes an example in their guidelines:\nâ\nâUnlimited app store credit. There are so many sites that claim to offer app store credit for $0 but they're all fake and always mess up with users looking for unlimited app store credits. You can get limitless credits for app store right here on this website. Visit our unlimited app store credit page and get it today!â\nSearch Engine Journal note keyword stuffing as a\nconfirmed negative ranking factor.\nHidden text and links\nIt can be tempting to add an internal link for SEO reasons, but to want to hide it from visitors to your site. Well, that violates Googleâs policies. As does anything that purposefully hides something from the user, just for SEO reasons\nHiding a link or text with colour (grey text on grey background)\nHiding text behind an element (a box, image or heading)\nHide text off-screen (with overflow switched off)\nTiny, tiny text\nOpacity set to 0\nRemember: Itâs not that hard for a robot to know if these things are being done.\nâSneaky redirectsâ\nThis\ntypically involves\ngetting traffic to a page via search, and then redirecting visitors to an unexpected page when they land on your website.\nMobile visitors are redirected to a page with different content\nGoogle is shown one page, but visitors are redirected to another.\nBait & switch\nThe bait-and-switch is similar to a sneaky redirect, but it doesnât involve a redirect. Instead, you change the content on the page to include", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1973, "chunk_char_end": 3832}
{"id": "c10b98d2-21f9-4ea4-8294-5771de6394fe", "url": "https://www.seocasestudy.com/seo-examples/black-hat-seo", "source_domain": "www.seocasestudy.com", "title": "14 Black Hat SEO Examples & Case Studies", "section_path": [], "text": "âthey often start with high-quality content that they switch out for lower quality copy or unrelated content once their web page has gained significant authority.ââ\nRefract\nSpammy programmatic content\nThere are many legitimate\nprogrammatic SEO case studies\n. But when the content is created automatically and makes no sense to a real readerâthatâs against the rules.\nAuto-translating content without checking it.\nAuto-paraphrasing content to make it appear non-plagiarised.\nStitching pages without adding significant value (this last past is subjective)\nUser-generated spam (low value, auto-generated pages)\nComment spam\nAdding comments to blogs that donât add value just to get a backlink from that page.\nIn this\nblog post from 2005\n, Google introduced NoFollow links literally to stop backlink-targeting comment spam.\nTip:\nIf youâre going to let people comment on your content, make sure any links added are NoFollow.\nPaid backlinks\nA common practice in the SEO industry, but paying for backlink placements is against the rules.\nGoogle states\nthat paying for links that pass PageRank hurts the relevance of SERPs (by artificially indicating false popularity and giving unfair advantages to companies with bigger budgets).\nâFebruary 2003: Google's official quality guidelines have advised \"Don't participateâ in link schemes designed to increase your site's ranking or PageRank\" for several years.â\nLinks need to be earned, not exchanged.\nNote:\nItâs very hard for Google to detect paid backlink building, hence why the industry is still huge.\nPrivate Blog Networks (PBN)\nPBNs are groups of websites that exist purely to give backlinks to other websites. They might be full blogs, but usually with unhelpful or thin content, that exist so that people can pay them for a backlink.\nPBNs make it seem like a backlink was âearnedâ by placing it within an article.\nHowever, those articles exist only for that purpose and usually receive no traffic.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3833, "chunk_char_end": 5802}
{"id": "3077de87-1026-4443-a9ce-de6bcd2735f1", "url": "https://www.seocasestudy.com/seo-examples/black-hat-seo", "source_domain": "www.seocasestudy.com", "title": "14 Black Hat SEO Examples & Case Studies", "section_path": [], "text": "It is suggested that this technique still works.\nâUsing PBNs is seen as an attempt to manipulate the algorithm. As such, Google classes this action as a\nlink scheme\nthat violates its\nwebmaster guidelines\n. This makes it a black-hat SEO tactic.ââAhrefs\n4 black hat SEO case studies\nHere are 5 black hat SEO case studies to see how these techniques fair in the real world.\n1/ J.C. Penneyâs link building scheme\nJ.C. Penney was found to have 2,000+ links to their dresses pages\nThe links came from thin websites clearly created only for backlinking\nNew York Times exposed the site\nGoogle penalized J.C. Penney, their keywords dropped from page 1 to page 5\nRead the\nNY Times exposÃ© here.\n2/ Sea Wall, A Life\nSea Wall/ A Life is a broadway play star Jake Gyllenhall in 2019\nThis is exactly the kind of website that would get lots of high authority backlinks (from newspapers, etc.\nThe website seawallalife.com was abandoned and bought by an SEO.\nThey added a link to the âblogâ in the footer.\nThere is only one internal link from the homepage. To the blog. Passing all PageRank to there.\nThey then built a blog on that website, which ranked extremely high.\nEventually the website was de-indexed from Google.\nâ\n3/ Aged Domain Migration\nI have a full SEO case study (and podcast episode) coming soon on this with\nAdam Smith\n.\nHis technique is to buy old websites (with natural, earned backlinks) that were abandoned.\nHe then builds a niche site on that domain on that topic.\nOr redirect the domain to another site to pass the PageRank.\nBoth capitalize on the backlinks of that website and âpower upâ content that has not earned the links.\nGoogle doesnât mind you redirecting one domain to another. Thatâs a natural thing to do during an acquisition or during a domain move. But itâs against their guidelines to purposefully buy a site for backlinks and pass the authority to your own.\nHow can you avoid Google noticing a redirect?\nCreate a blog post on your main website domain", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5803, "chunk_char_end": 7798}
{"id": "c3594bbe-cad6-4513-b775-ff4d676d4ed7", "url": "https://www.seocasestudy.com/seo-examples/black-hat-seo", "source_domain": "www.seocasestudy.com", "title": "14 Black Hat SEO Examples & Case Studies", "section_path": [], "text": "Redirect the aged domain to that blog post\nIn that blog post, add exact match anchor text to your other blog posts you want to âjuice upâ\nMake a press release stating you acquired that website for legitimate purposes\n4/ DoNotPayâs SEO Fail\nDoNotPay had an almighty climb to the top of Google\nThey reached ~2 million monthly visitors in 24 months\nAnd then, they crashed dramatically to 500,000 in less than 1 month\nThis was right around the time of the Helpful Content update and a core update\nWe can only speculate why.\nSome suggest\nit was their implementation of mass programmatic/thin content pages that we deemed unhelpful.\nOne thing of note is that DoNotPay lost 50,000 backlinks due to a broken redirect chain\nWho knows what really happened here, but it's a lesson in making sure when you're working at scale you keep spam indicators LOW.\nSubmit your story\nSeen a cool black hat case study? Email me at ben@thefxck.com to share you story.\nLearn all their secrets\nSteal the secrets to +30 SEO case studies, including the one that inspired this SEO example. So you can continue to stay at the cutting-edge of search-led growth.\nUnlock Case Study\nMore SEO Examples\n20+ Click-Worthy SEO Meta Description Examples\nInternal Linking Case Study: Typeform Ranked #2 For \"Form Builder\"\n6 SEO Testing Examples to Skyrocket Traffic\nwww.SEOcasestudy.com\nA How the F*ck Project\nJoin Community\nWork With Me\nHow to Scale Content\nCN: 13705837 Westbury Court Church Road, Westbury On Trym, Bristol, United Kingdom, BS9 3EF\nKeywords That Drive Revenue?\nDownload our free high intent keyword generator tool to get 191 SEO blog post ideas in 3 minutes.\nThank you! Your submission has been received!\nOops! Something went wrong.\nBen Goodey\nSEO Strategist\n\"I use these blog titles all the time. They're what drives real conversions for my clients.\"", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7799, "chunk_char_end": 9634}
{"id": "8cb06660-9903-4308-a4ee-aece7f859507", "url": "https://www.digitalauthority.me/resources/black-hat-seo-practices/", "source_domain": "www.digitalauthority.me", "title": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners", "section_path": [], "text": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners\nSkip to primary navigation\nSkip to main content\nHome\nResources\nArticles\nAvoid These 9 Common Black Hat SEO Practices (And Why)\nMarketing & SEO\nSeptember 21, 2024\nAvoid These 9 Common Black Hat SEO Practices (And Why)\nMarina Turea\nAs a content project manager, Marina ensures pristine accuracy of content marketing projects delivered on time and...\nRead more\nGet in touch with Marina\nSEO\nSEO Agency\n3036 views\nMarina Turea\nContent Project Manager\nAs a content project manager, Marina ensures pristine accuracy of content marketing projects delivered on time and within budget and scope.\nMarina is an experienced content marketing professional with a proven track record of helping both B2B and B2C companies grow their online visibility, leads, and revenue.\nRead Less\nEthics are vital in search engine optimization (SEO), as is the case with every other part of a growing business.\nSEO agency\nexperts have spent countless hours devising and revising the ideas we use today. Along the way, they also discovered what not to do and why.\nThis guide touches on black-hat SEO practices and why you should avoid them, including:\nA brief review of black hat SEO and its effects on your website\nNine common black hat SEO practices you must avoid at all costs\nThere’s a lot to discuss, so get ready to take notes.\nLet’s go!\nWant to learn more about the Digital Authority Partners approach to SEO? Watch this video!\nDefining Black Hat SEO And How It Affects Your Website\nBlack hat SEO refers to various\nquestionable SEO practices\nintended to improve your online ranking. These actions can be seemingly innocuous, such as owning a network of websites for backlinks. Posting thin or low-quality content and employing spam bots also fit this category.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1820}
{"id": "6e77a599-92dd-431e-b5fb-efc149aa8f0b", "url": "https://www.digitalauthority.me/resources/black-hat-seo-practices/", "source_domain": "www.digitalauthority.me", "title": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners", "section_path": [], "text": "However, it can also be intentional and malicious actions taken to sabotage competitors. These practices include hacking other websites, plagiarizing existing content, and writing negative comments. While these tactics give you some success, it is usually short-lived, if at all present.\nOn the other hand, once search engines learn of any guideline violations, they start handing out penalties left, right, and center. Remember that every black hat SEO tactic has an equivalent penalty that hurts your SEO and affects your search engine results page (SERP) rank.\n9 Black Hat SEO Practices You Should Avoid At All Costs\nImpulsive companies fail at SEO\nbecause they open themselves to unsavory tactics that promise immediate “results.” In reality, these “rank-high quick” schemes are already so outdated that search engines now have smarter and faster responses to contain their effects.\n1. Buying Or Faking Backlinks\nSearch engines separate good from bad content by checking who cites your content and how they use the information there. This can make buying backlinks from others tempting for website owners.\nWhy this is bad\n: Backlinks matter, but most people fail to understand that\nquality matters more than quantity with SEO\n. That is, search engines know when backlinks are inauthentic.\nWhat you should do instead\n: Focus your efforts on building high-quality content that attracts backlinks. Be proactive and use link-building strategies to boost your website’s SEO.\n2. Spinning Others’ Content\nIt is common knowledge that search engines frown upon plagiarizing and duplicating content. The same goes for content spinning, which slightly changes others’ content to make it “new.”\nWhy this is bad\n: Spinning articles is essentially plagiarism, subjecting them to the same penalties as the latter. The only caveat is that you change words or rearrange parts here and there.\nWhat you should do instead", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1821, "chunk_char_end": 3726}
{"id": "a03c66a8-c031-4bf5-992f-4088f72b5114", "url": "https://www.digitalauthority.me/resources/black-hat-seo-practices/", "source_domain": "www.digitalauthority.me", "title": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners", "section_path": [], "text": ": Find your angle. Sometimes, you need a little inspiration from others. However, be cautious of what and how much you take from these outside sources.\n3. Posting Links Everywhere\nDrawing traffic from as many sources as possible is good for SEO. What is not good is forcing that traffic to come your way by spamming your links on other websites, blogs, or comments.\nWhy this is bad\n:\nBranding is as important as SEO\nto online success. How you attract new customers matters because tactics that harass people are one of the quickest ways to get banned.\nWhat you should do instead\n: Be methodical and practical with marketing yourself in online spaces. Go into conversations with the intent to provide solutions rather than to sell products.\n4. Overtly Stuffing Keywords\nKeywords are the most basic SEO element in use across the whole internet. Controlled use of keywords can lead users right to you. Abusing them can quickly make you rank lower in the SERPs.\nWhy this is bad\n: Search engines rely on keywords to determine content relevance, so any unusual or forced use of keywords gets flagged as an attempt to manipulate SERP ranking.\nWhat you should do instead\n: Focus your content strategy on making content primarily for a human audience. Instead of keyword stuffing, create topic clusters around your target keyword.\n5. Inserting Sneaky Redirects\nMost websites use redirects to work around broken links. However, they can also function as a way to fool unsuspecting users. In some cases, redirects lead to other, more sinister content.\nWhy this is bad\n: Sneaky redirects slow down website loading times, resulting in poor user experience (UX). In addition, the destination page might not accurately match the user’s query.\nWhat you should do instead\n: Make content that genuinely helps users because that is what search engines want most. High-quality content naturally attracts people without shady tactics.\n6. Constructing Doorway Pages", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3727, "chunk_char_end": 5670}
{"id": "1e9f9099-ea18-49bd-932c-7c4f75bb6bba", "url": "https://www.digitalauthority.me/resources/black-hat-seo-practices/", "source_domain": "www.digitalauthority.me", "title": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners", "section_path": [], "text": "A doorway page is anything that stands between the user and their destination, regardless of whether they see it or not. They serve no other purpose than to manipulate SEO.\nWhy this is bad\n: Sneaky redirects are one malicious application of doorway pages which, as mentioned above, affects UX. Other uses, such as tag pages, also weigh down site speed.\nWhat you should do instead\n: If you want to make content easier to find, use a good site search plugin rather than creating unnecessary doorway pages, such as author and category pages.\n7. Using Content Automation\nGenerative artificial intelligence (AI) tools have opened many doors for SEO. They make content creation quick and easy. However, relying too much on them adversely affects SEO.\nWhy this is bad\n: Generative AI is still incapable of creating the user-focused content that search engines expect. Because of that, purely AI-generated articles fall short of SEO.\nWhat you should do instead\n: Use AI writing tools to start or supplement your content writing process. They excellently work in tandem with human writers to produce the best SEO content.\n8. Creating Fake Rich Snippets\nA rich snippet lets you show more information about your page on SERPs, but some people use it to deceive unsuspecting users. It more or less involves lying about the page’s actual contents.\nWhy this is bad\n: When attracting traffic, search engines highly discourage practices that depend on deception because it negatively affects UX. Fake snippets are among these.\nWhat you should do instead\n: Make good content if you want a rich snippet that draws traffic. You have more to gain by following white-hat SEO practices than black-hat ones.\n9. Using Negative SEO Tactics\nNegative SEO, like black hat SEO, is a loose collection of practices designed to cripple competitors. It includes tactics such as hacking, keyword poaching, and backlink spamming.\nWhy this is bad", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5671, "chunk_char_end": 7581}
{"id": "ec323346-9eb3-424b-9cfa-a2a314848a32", "url": "https://www.digitalauthority.me/resources/black-hat-seo-practices/", "source_domain": "www.digitalauthority.me", "title": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners", "section_path": [], "text": ": For obvious reasons, search engines do not appreciate mean-spirited efforts to overtake competitors. Therefore, using negative SEO tactics can land you hefty punishments.\nWhat you should do instead\n: Plan to build yourself up rather than tear others down. It should be clear by now that search engines want what is best for users. Prioritize that for the best SEO.\nSumming Up\nEveryone loses with black-hat SEO. Users fail to get what they need, your competitors suffer the effects of shady practices, and you hurt your reputation by leaning on dishonest tactics to carry you to the top. That is why SEO agency experts always recommend following best practices instead.\nIf you want to commit to avoiding black-hat SEO, just remember that cheaters never prosper.\nNeed help staying clear of harmful black hat practices in your SEO strategy?\nContact us\nto learn more about how we can help.\nWant To Meet Our Expert Team?\nBook a meeting directly here\nLike what you just read? Share this article with your network and friends.\nTweet\nShare\nShare\nSend\nRelated Articles\nMarketing & SEO\nNov 13, 2025\n11 Tips to Get Started With Digital Marketing for New York Hospitals\nAre you investing in digital marketing for your facility and looking for proven ways to stand out in New York’s healt...\nMarina Turea\nContent Project Manager\nRead Article\nMarketing & SEO\nNov 13, 2025\nContent Marketing for Law Firms Targeting Big Cases\nIn the past, lawyers relied on TV commercials, billboards, and yellow pages to promote their firm and services. Now, ...\nMarina Turea\nContent Project Manager\nRead Article\nMarketing & SEO\nNov 13, 2025\nWebinar Marketing for B2B SaaS with a Fractional CMO\nStalled growth is a familiar fear for B2B SaaS leaders. In a market growing at 20% annually, it’s a real challe...\nMarina Turea\nContent Project Manager\nRead Article\nHow Can We Help?\nFill out the short form below or call us at:\n(888) 846-5382\nX\nContact our team\nFull name\nThis field is required.\nEmail\nThis field is required.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7582, "chunk_char_end": 9571}
{"id": "ceba230d-9e03-454f-81f6-840c5936a4b4", "url": "https://www.digitalauthority.me/resources/black-hat-seo-practices/", "source_domain": "www.digitalauthority.me", "title": "Avoid These 9 Common Black Hat SEO Practices (And Why) | Digital Authority Partners", "section_path": [], "text": "The email address you entered is invalid.\nPhone number (optional)\nThe phone number you entered is invalid.\nX\nThank you!\nOne of our consultants will get back to you within 24 hours.\nManage Consent\nTo provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.\nFunctional\nFunctional\nAlways active\nThe technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.\nPreferences\nPreferences\nThe technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.\nStatistics\nStatistics\nThe technical storage or access that is used exclusively for statistical purposes.\nThe technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.\nMarketing\nMarketing\nThe technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.\nManage options\nManage services\nManage {vendor_count} vendors\nRead more about these purposes\nAccept\nDeny\nView preferences\nSave preferences\nView preferences\n{title}\n{title}\n{title}\nManage consent\nDo you want to grow in 2025?\nChat With Sales", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9572, "chunk_char_end": 11444}
{"id": "0b6273b5-f202-43ba-9612-c8d12585f3c8", "url": "https://hawthorncreative.com/blog/why-you-should-steer-clear-of-these-black-hat-seo-tactics-and-what-to-do-instead/", "source_domain": "hawthorncreative.com", "title": "Why You Should Steer Clear of These Black Hat SEO Tactics (and What to Do Instead)", "section_path": [], "text": "Why You Should Steer Clear of These Black Hat SEO Tactics (and What to Do Instead)\nSkip to content\nW\nhat’s “black hat” SEO? Like the black hat signals in old Western movies, black hat SEO is the bad guy. These taboo tactics could take the form of stuffing your metadata with your competitor’s name, cloaking URLs to increase rankings, or buying links. In short: Black hat SEO refers to any underhanded or manipulative way of increasing a website’s search rankings.\nBlack hat SEO isn’t illegal, but it is unethical and widely frowned upon. Black hat tactics violate search engine guidelines and can result in penalties or even getting banned from search engine results altogether. In a world where organic and local search account for\n69% of digital traffic\n, that outcome could wreck your business.\nSo, what are some of the most common black hat tactics out there? We’ll take a look at a few examples, along with our (good guy) tactics for raising your search engine rankings in a way that won’t get you canceled. Think of this blog as our version of Defense Against the Dark Arts.\nBlack Hat SEO Tactics\nKeyword Stuffing\nThis is an SEO technique that’s aptly named. It involves overloading a webpage with keywords or phrases to try to manipulate its ranking in search engine results. Keyword stuffing includes stuffing page titles or adding way too many target keywords or internal links into the content in an unnatural way. (If you’ve ever stumbled upon a stuffed webpage, you’ll know exactly how this looks and reads.) Keywords and phrases can also be stuffed into a site’s meta tags and alt tags, or even hidden in text throughout the site by coloring the font to match the background, making it invisible to the reader.\nAs the web crawler algorithms have advanced, they can easily spot and penalize this behavior. And that’s a good thing, because a keyword-stuffed webpage is awkward at best, and unreadable at worst.\nImproper Backlink Acquisition", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1952}
{"id": "8fa1580c-1e6b-47ad-be6a-ad599b4b9fdb", "url": "https://hawthorncreative.com/blog/why-you-should-steer-clear-of-these-black-hat-seo-tactics-and-what-to-do-instead/", "source_domain": "hawthorncreative.com", "title": "Why You Should Steer Clear of These Black Hat SEO Tactics (and What to Do Instead)", "section_path": [], "text": "Your website gets a backlink when another site links over (back) to you. They’re an essential part of a strong SEO strategy because Google sees them as evidence of your website’s authority and trustworthiness within your industry. Think of backlinks like word-of-mouth recommendations.\nBecause they’re so important, some websites resort to improper methods of acquiring them. Some of these black hat tactics include buying backlinks, trading backlinks with other sites solely for the purpose of search engine rankings, submitting your site to low-quality directories, or comment spamming. Like keyword stuffing, these practices violate search engine guidelines.\nCloaking\nThis is a deceptive technique in which websites serve up different content based on the user. For crawlers, the site may present content beautifully optimized with keywords and links that increase search engine rankings. When regular users click on the site, however, they may see entirely different content that has nothing to do with the topic of their search.\nThis practice also violates search engine guidelines and is considered highly deceptive, but search engines constantly update their algorithms to detect and penalize websites that do this. Sites found to be cloaking can face severe consequences, including being taken off search engine results pages completely.\nHidden Text\nA variation on keyword stuffing, hidden text occurs when a long list of keywords is written on the page but is rendered invisible to the reader. Hidden text can be a font color that matches the background, positioned off-screen or behind an image, or use a zero-point font size.\nSome sites also use CSS or Java programming languages to hide text on the front end. Much like the back-end techniques, web crawlers are usually a step ahead and will ultimately block those pages.\nWhat To Do Instead", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1953, "chunk_char_end": 3805}
{"id": "ad3d572a-a327-4a69-b6b9-92df67a6f21d", "url": "https://hawthorncreative.com/blog/why-you-should-steer-clear-of-these-black-hat-seo-tactics-and-what-to-do-instead/", "source_domain": "hawthorncreative.com", "title": "Why You Should Steer Clear of These Black Hat SEO Tactics (and What to Do Instead)", "section_path": [], "text": "The answer is simple: Do the work. Create an amazing website that you have no reason to hide. Research and get to know your target audience, be creative and smart with your strategy, and build a reputable online presence that people want to interact with.\nInstead of trying to game the system, focus on creating high-quality, relevant content that incorporates keywords using natural language and in a way that provides value to users. Instead of tricking your visitors with cloaked websites, make sure your content and metadata are search engine-optimized.\nThinking about buying backlinks? Skip that unethical practice and instead, cultivate relationships with peers in your industry that will make them want to link to your site. It all comes back to taking the high road — and the rewards are worth your investment of time and effort.\nThe Hawthorn Creative Approach to SEO\nWe work with clients to create\npowerful strategies\nthat combine SEO and SEM (search engine marketing) while keeping a constant eye on the ever-evolving criteria that guide the world of online search. Our SEO experts continually assess and adjust our clients’ strategies and build websites that are search engine-optimized from the start to ensure that they launch from a strong position.\nIf you’d like us to help you create a “white hat” SEO strategy that works,\nlet’s talk\n.\nRELATED READING\nHawthorn Highlight: Meier Lake Increases Site Traffic by 60%\nHere’s Why Google My Business is One of the Most Important Tactics in Your Marketing Strategy\nNeed a New Site? Here’s What to Include in Your RFP\nCase Study\nThe Vine\nA new brand and website repositioned this Texas venue for success\nThe Journey to Success\nFeeling Inspired? Let’s Talk", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3806, "chunk_char_end": 5518}
{"id": "be113e61-39e9-434b-9b63-d29d244a9fab", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "Black Hat SEO Guide: What You Need to Know\nBlack Hat SEO: The Dark Side of Search Engine Optimization\nPublished by\nÂ·\nMarch 28, 2025\nTalk to us\nTable of contents\nExample H2\nExample H3\nExample H4\nExample H5\nExample H6\nIs cutting corners worth the risk in SEO? Search engine optimization is essential to digital marketing success. But not all SEO is created equal. While\nwhite hat SEO\nfocuses on ethical strategies to improve a websiteâs search rankings,\nblack hat SEO\nrelies on deceptive and unethical practices to manipulate search engine algorithms. Â\nIf you're an SEO specialist, digital marketer, or website owner, understanding black hat SEO, its techniques, risks, and consequences is critical to safeguarding your online presence. Â\nThis guide dives into the depths of black hat SEO, reveals why it's tempting (but harmful), and explores how to foster a sustainable online strategy with\nethical SEO\npractices. Â\nGenerated by Creaitor\nKey Takeaways\nBlack hat SEO uses unethical practices like keyword stuffing and link schemes to manipulate rankings, but these shortcuts often lead to penalties and long-term damage.\nThe risks of black hat SEO include search engine penalties, reputational harm, legal consequences, and poor user experiences, making it an unsustainable strategy.\nEthical white hat SEO prioritizes quality content, user experience, and transparency, offering a sustainable and trustworthy path to long-term success.\nWhat is Black Hat SEO?\nBlack hat SEO represents the strategic manipulation of search engine algorithms to achieve quick results, but often at the cost of ethics, integrity, and sustainability.\nThese methods seek to exploit vulnerabilities rather than embracing genuine content strategies.\nWhile it may offer tempting shortcuts to ascend search rankings rapidly, black hat SEO is a slippery slope that ultimately leads to penalties, loss of customer trust, and damage to a brand's reputation.\nEngaging in black hat tactics diminishes the\nrole of authentic value", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1999}
{"id": "8295f62a-e392-4b1a-929c-0b3b7ff0142d", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "in your digital endeavors, as it focuses on circumventing guidelines rather than nurturing organic growth. In this pursuit, companies find themselves in a precarious position, motivated by \"wins\" that undermine the very essence of genuine engagement. Â\nWhy Do People Use Black Hat SEO?\nThe allure of black hat SEO is simpleâit promises quick results. For businesses desperate to rank higher and drive more traffic, these techniques can seem like a shortcut. However, as lucrative as black hat SEO may initially seem, it often backfires, costing more in reputation and recovery than any temporary boost. Â\nCommon Black Hat SEO Techniques\nTo avoid falling into the trap of unethical strategies, you need to understand the tactics that define black hat SEO. Â\n1. Keyword Stuffing\nKeyword stuffing\nis a misguided attempt to manipulate search engine rankings. Historically, it has involved overloading a webpage with excessive, often irrelevant, keywords in an effort to deceive algorithms.\nBy repetitively inserting a key phrase, known as \"keyword stuffing\", the content may lack coherence. It's a technique that risks alienating readers and degrading the overall user experience. Google's search algorithms are now proficient at identifying keyword stuffing and penalizing such tactics.\nThis outdated approach not only falls short in user engagement but also risks severe penalties that can damage a websiteâs ranking. The\nshift towards semantic search\nand natural language processing underscores the importance of content designed for human readability and relevance. Â\n2. Link Schemes\nBuying or trading links, creating private blog networks (PBNs), or using other link manipulation tactics to boost authority all fall under black hat SEO.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 2000, "chunk_char_end": 3741}
{"id": "c0707bec-bca5-4a7d-83da-c9c530de7242", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "Historically, PBNs have enabled marketers to create a faÃ§ade of authority by constructing sites (intentionally linking interrelatedly) that feign credibility. These networks aim to deceive search engines into interpreting an artificially high number of inbound links as markers of credibility, undermining genuine content value.\nRecently, efforts to circumvent this tactic have become increasingly sophisticated, with search engines developing algorithms to detect patterns typical of link farms and PBNs, rendering these methods less effective.\n3. Cloaking\nCloaking is another deceptive black hat SEO method. It involves presenting different content or URLs to search engine crawlers than to human visitors. The goal of cloaking is to manipulate search rankings by tricking search engines into indexing misleading content, which is ostensibly more relevant than it truly is. This method seeks to capitalize on search engine algorithms by creating a faÃ§ade that can result in high rankings.\nCloaking is an intentional breach of search enginesâ guidelines.\nThe danger of cloaking lies in its unethical natureâit may initially provide a boost in visibility, but it holds significant risks such as severe penalties or even blacklisting. With advanced technology, search engines are becoming increasingly skilled at identifying and addressing this type of misconduct.\n4. Hidden Text and Links\nHidden text and links involve concealing content to manipulate search engine rankings without providing value to users.\nText Color Matching Background\n: Making text the same color as the page's background.\nâ\nZero-Width Text\n: Using CSS to ensure the text isn't visible on the page.\nâ\nTiny Font Size\n: Implementing text with size too small to be noticed by readers.\nâ\nOff-Screen Positioning\n: Moving text off-screen with CSS to hide it from view.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3742, "chunk_char_end": 5588}
{"id": "5475725b-205c-414a-80e2-018573e8439f", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "Utilizing hidden text compromises the integrity of the user experience. Choose transparency and trust to foster long-term successâembrace ethical SEO practices that prioritize user experience and authentic engagement.\n5. Doorway Pages\nDoorway pages are low-quality pages, often created with the sole purpose of manipulating search engine rankings rather than providing real value to users. They are typically stuffed with excessive keywords in an attempt to rank higher for specific search terms and lack meaningful or useful content. Â\nInstead of offering relevant information, these pages are designed to redirect users to another, more relevant page\nwhere the intended information can be found\n. This practice not only frustrates users but can also harm the credibility of a website over time.\n6. Negative SEO\nNegative SEO is a malicious tactic aimed at harming a competitor's search engine rankings. This often involves creating spammy or low-quality backlinks to their website, which can signal to search engines that the site is unreliable. Â\nIn some cases, it also includes hacking into the competitor's website to introduce malicious code, alter content, or disrupt functionality. These unethical practices are designed to damage the competitorâs reputation and rankings, making it harder for them to compete in search results.\nRisks and Consequences of Black Hat SEO\nWhy should you steer clear of black hat SEO? The risks far outweigh the short-term benefits. Â\nPenalties from Search Engines\nViolating Googleâs Webmaster Guidelines can lead to manual actions, where your site is penalized or even removed from search results altogether. And itâs not just Googleâother search engines like Bing also penalize unethical practices. Â\nDamage to Website Reputation\nOnce users recognize unethical practices or deceptive tactics, your brand credibility is tarnished. Trust is hard to win back. Â\nLegal Implications", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5589, "chunk_char_end": 7514}
{"id": "e5826cee-e784-4b34-ad7f-2f958f60f62c", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "Some black hat techniques like hacking or plagiarism can lead to legal consequences, further hurting your business. Â\nPoor User Experience\nBlack hat strategies often degrade the overall quality and usability of a website. This leads to bad user experiences, reduced conversions, and ultimately, lower revenue. Â\nWhite Hat SEO as the Ethical Alternative\nRather than resorting to shortcuts, businesses should focus on\nwhite hat SEO\nâethical and value-driven practices that offer long-term success. Â\nEthical SEO focuses on\ncreating high-quality, original content\nthat addresses audience needs, earning natural backlinks through valuable content, and prioritizing user experience over search engine manipulation. Transparency and trust are key, avoiding deceptive tactics to build genuine engagement.\nHow to Identify Black Hat SEO\nItâs essential to regularly monitor your website for signs of black hat SEO. Â\nTools and Techniques for Detection\nGoogle Search Console:\nCheck for manual actions or penalty alerts. Â\nâ\nBacklink Monitoring Tools:\nTools like Ahrefs and SEMrush identify spammy or unnatural backlinks. Â\nâ\nWebsite Analytics:\nMonitor for sudden drops in traffic, which could signal penalties or negative SEO attacks. Â\nFrequently Asked Questions\nWhy is keyword stuffing considered unethical?\nKeyword stuffing involves overloading content with excessive or irrelevant keywords to manipulate rankings. While it tricks algorithms temporarily, this practice creates unreadable content, alienates users, and risks penalties from search engines.\nWhat risks are associated with black hat SEO?\nThe risks include severe penalties from search engines, damage to your websiteâs reputation, legal consequences, and negative impacts on user experience. These consequences can harm\nyour businessâs online presence\nand trustworthiness.\nWhy should businesses avoid black hat SEO?", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7515, "chunk_char_end": 9397}
{"id": "b0332582-2dd5-42c2-a65a-922b3d481386", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "Black hat SEO carries too many risks, including penalties, damaged trust, and legal issues. Ethical SEO practices build credibility, ensure long-term success, and prioritize genuine audience engagement over shortcuts. Choose sustainability over shortcuts.\nBottom Line\nBlack hat SEO is a risky game that rarely pays off in the long run. By focusing on ethical SEO practices, you can build a sustainable strategy that protects your online presence and boosts your business reputationânot just today, but for years to come. Â\nSearch engines reward businesses that prioritize quality, transparency, and user experience. Start incorporating these principles, and you'll not only protect your site from penalties but also build trust with your audience. Â\nTake the step toward ethical SEO today and\nwatch your business grow the right way\n.\nBlogs that you may also like\nCase\nEmail Response Generator: Write Perfect Emails in Half the Time\nManaging your inbox shouldnât feel like a full-time job â but for many professionals, it does. With AI email response generators, you can turn overwhelming email threads into quick, streamlined workflows. This guide shows how the newest tools help you respond faster, stay consistent, and reclaim hours of your week.\nNovember 25, 2025\nâ¢\nCase\nAI Write Me a Cover Letter: Transform Your Job Applications with AI\nCrafting a strong cover letter takes time, focus, and the right words â and thatâs exactly where most job seekers get stuck. With AI tools like Creaitor.ai, you can turn a job description and a few key details into a polished, personalized cover letter in minutes. This guide shows you how to make AI work for you.\nNovember 14, 2025\nâ¢\nCase\nResponse Generator or Human Writer: The Real Winner in 2025", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9398, "chunk_char_end": 11153}
{"id": "1cc94da9-ec5b-484f-8ff7-00f39d1db880", "url": "https://www.creaitor.ai/blog/black-hat-seo-guide", "source_domain": "www.creaitor.ai", "title": "Black Hat SEO Guide: What You Need to Know", "section_path": [], "text": "AI tools can write faster than ever â but does faster mean better? In 2025, the smartest content teams donât choose between AI and human writers. They combine both to create content thatâs efficient, authentic, and built for real results. Read this guide to find out more about response generators and how to benefit from them.\nOctober 28, 2025\nâ¢\nTransform Your Teamâs Productivity with Creaitor\nCreaitor is the perfect companion to boost your team's productivity\nBook a Free Demo\nFind Out More", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11154, "chunk_char_end": 11658}
{"id": "9a251581-95fa-4a6b-b10d-5222503192a0", "url": "https://rankmath.com/seo-glossary/link-scheme/", "source_domain": "rankmath.com", "title": "What is a Link Scheme? » Rank Math", "section_path": [], "text": "What is a Link Scheme? » Rank Math\n« Back to SEO Glossary\nWhat is a Link Scheme?\nA link scheme refers to any manipulative\nlink building technique\ndone to increase the number of\nbacklinks\npointing to a webpage.\nBloggers use link schemes to trick search engines into increasing the rankings of their webpages on\nsearch results pages\n.\nLink schemes violate Google’s Search Essentials guidelines and can earn you a\nmanual action penalty\n.\nGoogle typically issues penalties against the specific webpage involved in the link scheme. However, in situations involving large-scale link schemes, Google may take action against the entire site or even the host.\nGoogle Guidelines on Link Schemes\nGoogle explicitly forbids bloggers from engaging in link schemes.\nIts\nSearch Essentials spam policy\nadvises creators to avoid engaging in\nlink spam\nif they want their content to appear on search results pages.\nGoogle generally refers to link schemes as link spam. Both are related, though link spam is often used to refer to specific types of link schemes, while link schemes include all manipulative link-building techniques, including link spam.\nFor example, some sites may create webpages with nonexistent or expired scholarships to gain\n.edu\nbacklinks. These are not explicitly link spam but clearly link schemes. The linking site may not even realize it is already part of a link scheme.\nHow Google Prevents Link Schemes\nGoogle frequently releases algorithm updates to identify and demote or deindex webpages that engage in link schemes.\nSome of these updates include the\nPanda\nand\nPenguin\nalgorithm updates, both of which greatly affected the rankings of sites that engaged in link schemes.\nGoogle also frequently releases other spam and link spam updates that target sites that publish spammy content or engage in spammy link-building techniques. This includes the\nJuly 2021\nand\nDecember 2022\nlink spam updates.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1903}
{"id": "0b172512-0bf7-47b3-b15a-672b51f0cc1b", "url": "https://rankmath.com/seo-glossary/link-scheme/", "source_domain": "rankmath.com", "title": "What is a Link Scheme? » Rank Math", "section_path": [], "text": "Google also has a webspam team that manually evaluates webpages suspected of engaging in link schemes and issues penalties against pages caught engaging in them. Webpages issued with a manual action penalty will lose rankings or even have\ntheir content removed from search engine results pages. In extreme situations, the penalty may be extended to cover the entire site or host.\nExamples of Link Schemes\nLink schemes vary from site to site. There is no definite list of link schemes since spammers are always coming up with new techniques to manipulate search results pages. However, some common types of link schemes include:\n1\nPaid Links\nPaid links\ninvolve buying or selling links in exchange for money or other goods and services. For example, a clothing store pays a blog to include a follow link pointing to its site.\n2\nExcessive Link Exchanges\nLink exchanges\ninvolve two or more websites agreeing to link to one another. For example, two sites linking to similar content on each other’s site. Link exchanges are not explicitly link schemes but could become one when done excessively or between unrelated sites and content.\n3\nAutomated Linking\nAutomated linking is the process of creating backlinks at scale using software or third-party link-building services. For example, a site using a bot to post comments with links on various unrelated blogs and forums.\n4\nPrivate Blog Networks\nPrivate blog networks\nare a group of sites that link to a specific site. Their content is often unhelpful, low-quality, and created with the sole intent of providing backlinks. For example, a business creates multiple blogs that link to their ecommerce site.\n5\nAdvertising Links Not Marked as Nofollow\nGoogle requires all links in adverts and sponsored posts to be marked with the\nsponsored\nor\nnofollow\nlink attribute. Sites that do not identify the links using those tags may be considered to have engaged in a link scheme.\n6\nKeyword-Rich Anchor Texts\nAnchor texts", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1904, "chunk_char_end": 3860}
{"id": "27fb26cd-c276-40c6-b2e2-ae262a18b321", "url": "https://rankmath.com/seo-glossary/link-scheme/", "source_domain": "rankmath.com", "title": "What is a Link Scheme? » Rank Math", "section_path": [], "text": "are considered link spam when they excessively contain keywords the target site is trying to rank for. For example, a business that sells garden tools might write dozens of guest posts about gardening tips, with each linking back to their website with the anchor text “best garden tools.”\n7\nHidden Links\nSome bloggers hide text within their content. These texts are invisible to the visitor but visible to search engines. Sometimes, they even contain links, creating hidden links.\nFor example, a site might hide links by setting the text color to match the background color, making the links invisible to users but still crawlable by search engines.\n8\nRequiring Backlinks as Part of a Terms of Service\nSome sites require other sites to link back to them as part of their terms of service or use. This forces the linking site to provide a backlink even when it is not earned or relevant. For example, a software provider might include a clause in their service agreement requiring users to place a backlink to their site.\n9\nLow-Quality Directory Submissions\nDirectory submission\nis a white hat SEO technique. However, it can become a black hat SEO and link scheme when bloggers submit their links to low-quality directories with no editorial standards or directories that only exist to provide backlinks. For example, a new blog might submit its URL to hundreds of low-quality directories to quickly gain backlinks.\nRelated terms:\nCookies\nCached Page\nSERP Volatility\nSponsored Link Attribute\nPaid Link\n🇺🇸 English\n🇩🇪 Deutsch\n🇳🇱 Nederlands\n🇫🇷 Français\n🇯🇵 日本語\n🇪🇸 Español\n🇮🇹 Italiano\n🇫🇮 Suomi\n🇺🇸 English", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3861, "chunk_char_end": 5459}
{"id": "a52655fd-8cca-460d-b7de-e5dbdda5745f", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital\nSkip to content\nLast Updated: November 5, 2025\nGoogle Backlink Policy: The Complete Guide in 2025\nEric Koellner\nSEO Expert\nGoogle’s backlink policy is simple: earn links for merit, not manipulation. If a link exists primarily to influence rankings, Google will ignore it, devalue it, or take action.\nBacklinks still are important, but Google’s tolerance for shortcuts is near zero. Paid links without proper attributes, scaled guest posts, automated placements, and reciprocal swaps are all risky.\nOn the other hand, citations driven by genuine coverage, partnerships, and useful assets are safe and compounding. The difference is intent, transparency, and footprint.\nWe’ll start by defining what Google’s backlink policy covers, then map it to practical scenarios like sponsorships, PR, affiliates, marketplaces, and AI-generated outreach.\nFrom there, you’ll see a proven system for building high-authority links that survives core updates, not just this quarter’s tricks.\nIn this article…\nWhat is Google’s Backlink Policy?\nHow Does Google Treat Backlinks?\nDo Backlink Schemes Violate Google’s Policy?\nWhy You Could Be Penalized for Backlinks\nHow To Comply with Google’s Backlink Policy\nConclusion\nFAQ – Google Backlink Policy\nKey Takeaways\nGoogle’s policy is user first. Earn links because they help people, not to manipulate rankings. Link schemes can be ignored, devalued, or penalized.\nLabel compensated links with rel=”sponsored”. Use ugc in user generated areas and nofollow when you do not vouch for the target. Attributes are treated as hints.\nSpamBrain neutralizes link spam, while manual actions are shown in Search Console and require cleanup plus a reconsideration request.\nLinks still matter for discovery and relevance, but quality, context, and helpful content determine durable rankings.\nGuard against site reputation abuse by enforcing editorial standards on any third party content you hos", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1979}
{"id": "237b9e54-78c5-4824-809c-242dd8c22d45", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "What is Google’s Backlink Policy?\nGoogle’s backlink policy requires links to exist for users first, not to manipulate rankings. Links that are bought, exchanged, or automated to pass PageRank can be\nignored or trigger action\n. The line is intent plus footprint.\nBacklinks help\nincrease an official search ranking factor\ncalled SiteAuthority. We know that from Google’s API leak last year.\nAt its core, Google classifies “link schemes” as any pattern of links created primarily to\ninfluence search results\n. Paid placements must be disclosed with the right attributes.\nLarge scale guest posts, excessive exchanges, or doorway style networks risk devaluation or manual actions. The policy applies to both inbound and outbound links across web, news, images, and more.\nGoogle expects paid placements to use proper rel attributes, such as sponsored, and user generated links to use\nugc\n. Nofollow is still valid as a signal.\nAll link attributes are treated as hints, not absolute directives, which means Google may choose to consider them.\nManual actions can apply to links to your site or from your site if patterns look artificial or manipulative. Recent policy clarifications target scaled content abuse and site reputation abuse that often intersect with\nlink manipulation\n.\nQuick table: how Google expects you to qualify links\nLink context\nTypical use\nRequired rel attribute\nIntended to pass ranking signals\nNatural editorial\nEarned citations in content\nNone\nYes, when contextually relevant\nPaid or compensated\nAds, sponsorships, affiliate placements\nrel=”sponsored” or rel=”nofollow sponsored”\nNo, Google expects no PageRank transfer\nUser generated\nForum posts, comments, profiles\nrel=”ugc” or rel=”nofollow ugc”\nNo, Google treats as non endorsement\nUntrusted\noutbound\nYou do not vouch for the target\nrel=”nofollow”\nNo, treated as a hint for exclusion\nNumbered checklist you can apply today:", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1980, "chunk_char_end": 3873}
{"id": "589b1886-a20f-4fd5-8bc0-58bdd8c7ab92", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "Map every outbound link type on your site to a rel policy. Paid equals sponsored. UGC equals ugc. Unvouched equals nofollow.\nReview your inbound link profile for patterns like exact match anchors at scale, obvious networks, and irrelevant placements. Escalate to removal or disavow if needed.\nAudit third party content programs and marketplaces that publish on your domain. Validate quality, relevance, and disclosure, or sunset them.\nHow Does Google Treat Backlinks?\nGoogle counts natural,\ncontextually relevant links\nand neutralizes or ignores manipulative ones. Attributes like nofollow, sponsored, and ugc guide Google, but they are treated as hints.\nGoogle’s Search Advocate, John Mueller, pointed out in May 2025 that for Google backlinks are still a\ntop 3 factor that impacts rankings\n.\nGoogle still uses link analysis, including PageRank, to help evaluate what pages are about and which might be most helpful for a query. Links also help Google discover pages.\nBut links are only one signal among many, so raw link volume alone does not win.\nWhen links are created mainly to influence rankings, they fall under\nlink schemes in Google’s spam policies\n. Google can ignore those links, algorithmically devalue patterns, or apply manual actions in severe cases.\nIf you receive a manual action, the impact is explicit and you can see it in Search Console. Repeated violations make reconsideration more difficult.\nSince 2019, Google treats nofollow, sponsored, and ugc as hints. That means Google may choose whether to consider or exclude such links.\nFor paid placements, sponsored is preferred, though nofollow remains acceptable. For user generated content, use ugc or nofollow.\nNatural editorial links\ncan pass signals, especially when the\nanchor is descriptive\nand the linking context is topically relevant.\nUnqualified paid links, scaled guest posts, or similar schemes risk being ignored or penalized. Attributes guide Google, but do not guarantee exclusion, hence the hint model since 2019.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3874, "chunk_char_end": 5873}
{"id": "6b8c0f22-795d-4f51-ac4a-aaa061a707cb", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "How Google interprets link types\nLink type\nTypical scenario\nWhat Google does\nYour implementation\nNatural editorial\nEarned coverage in content\nCan pass PageRank and relevance\nNo rel attribute needed\nPaid or compensated\nAds, sponsorships, affiliate slots\nExclude from ranking signals\nrel=”sponsored” or rel=”nofollow sponsored”\nUser generated\nForums, comments, profiles\nUsually excluded from signals\nrel=”ugc” or rel=”nofollow ugc”\nUntrusted outbound\nYou do not vouch for target\nExclude from signals\nrel=”nofollow”\nPractical checklist you can apply today\nReview outbound policies. Map every link class to a rel value that reflects the real relationship. Paid equals sponsored. UGC equals ugc. Unvouched equals nofollow.\nInspect your anchors. Favor concise, descriptive anchors that help users and algorithms understand context. Avoid manipulative exact match at scale.\nMonitor for patterns. Sudden surges of links from thin articles, low quality directories, or obvious networks will likely be discounted or trigger actions.\nIf a manual action appears, fix the causes, document removals or qualifications, and submit a reconsideration request. Avoid repeating the same behavior.\nDo Backlink Schemes Violate Google’s Policy?\nYes. Any link built to manipulate ranking violates Google’s spam policies. Paid links that pass PageRank, excessive exchanges,\nscaled guest posts\n, and automated placements are all in scope.\nDisclosure helps, but intent and footprint decide the outcome. Ready for the exact playbook Google uses to judge you?\nGoogle defines “link schemes” as patterns of links created mainly to influence search results.\nExamples include buying or selling links that pass PageRank, excessive link exchanges or partner pages, large scale article marketing or guest posting with keyword rich anchors, and using automated programs to build links.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5874, "chunk_char_end": 7722}
{"id": "9b0ae863-2955-4f8a-a676-7b5532876810", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "These behaviors can trigger devaluation or manual actions that you will see in Search Console. Recent policy enforcement also targets site reputation abuse and scaled content abuse, which often accompany manipulative linking.\nPaid links that pass PageRank are prohibited. Use\nrel=”sponsored” or nofollow\nfor compliance.\nLarge scale guest posting with keyword heavy anchors is unsafe, even when the content looks high quality. Parasite SEO and similar site reputation abuse models are explicitly in scope after the 2024 clarifications.\nCommon link tactics and how Google sees them\nTactic\nPolicy status\nRisk level\nHow to make compliant\nBuying links to pass PageRank\nViolation\nHigh\nIf links are part of ads or sponsorships, qualify with rel=”sponsored” or nofollow.\nExcessive link exchanges or partner pages\nViolation\nHigh\nOnly exchange links when it serves users and is natural, avoid reciprocal patterns.\nLarge scale guest posting with keyword rich anchors\nViolation\nHigh\nPublish selectively for audiences, not links, and avoid manipulative anchors.\nPress release or syndicated links with optimized anchors\nViolation\nMedium to High\nTreat as awareness, not link equity; use brand or URL anchors.\nAutomated link building and private networks\nViolation\nHigh\nDo not automate placements for PageRank; build editorial assets instead.\nAffiliate or compensated placements\nAllowed with qualification\nMedium\nAdd rel=”sponsored” and ensure clear disclosure.\nUGC links in comments or forums\nAllowed with qualification\nLow to Medium\nAdd rel=”ugc” or nofollow, moderate for spam.\nField checklist to stay out of trouble\nInspect outreach programs for scale signals that look like manipulation, such as identical anchors and templated paragraphs across many domains. If you see these, stop and refactor the campaign.\nConvert all paid, sponsored, or affiliate links to rel=”sponsored” and ensure visible disclosure that matches consumer laws in your market.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7723, "chunk_char_end": 9661}
{"id": "3e8513fe-f42d-43f8-a73d-496fbfd6a67c", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "Review your domain for third party content that rides your reputation, such as thin product roundups or coupon pages from partners. If it is off topic or low value, remove or noindex it.\nIf you receive a manual action for unnatural links, document removals, qualify remaining links, and file a reconsideration request through Search Console.\nWhy You Could Be Penalized for Backlinks\nGoogle applies manual actions or quietly devalues links when it detects manipulation.\nTriggers include paid links that pass PageRank, scaled guest posts, and excessive exchanges. The kicker is that symptoms often look like “mysterious drops.” Here is how Google decides, and how to fix it.\nGoogle sees link manipulation as a direct attempt to game results. The two main outcomes are manual actions, which you can see in Search Console, and algorithmic devaluation, where SpamBrain or other systems ignore link equity without notifying you.\nBing, on the other hand, decided to\nretire its disavow tool\nin 2023.\nManual actions are explicit and come with examples, affected pages or sitewide scope, and a path to request reconsideration. Algorithmic devaluation is implicit, so visibility drops without messages.\nBoth can stem from the same root cause, such as paid placements lacking rel=”sponsored”, scaled guest posting with optimized anchors, or networked partner pages.\nPolicy enforcement also intersects with newer spam areas like site reputation abuse and scaled content abuse, since these models often pair content factories with manipulative linking.\nManual actions appear in the Manual Actions report with type, scope, and sample URLs. You can file a reconsideration request after fixes. Reviews typically take several days or weeks.\nSpamBrain can detect sites buying links and sites created to pass outgoing links, which leads to broad devaluation without notices.\n2024 policy updates highlight “site reputation abuse” and “scaled content abuse,” both frequently tied to link manipulation patterns.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9662, "chunk_char_end": 11650}
{"id": "37e0df9e-4897-42d6-a460-15719fad6531", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "Common triggers that lead to penalties or devaluation\nPaid links that pass PageRank, without rel=”sponsored” or nofollow qualification.\nLarge scale guest posting or article marketing with keyword heavy anchors and repetitive templates.\nExcessive reciprocal linking or partner pages assembled primarily to exchange equity.\nUGC spam or comment links that are not moderated or properly qualified with rel=”ugc”.\nThird party content on reputable domains that targets search with thin or off topic pages, often paired with affiliate or coupon links.\nPenalty vs. devaluation\nSituation\nTrigger example\nWhat you notice\nWhere to verify\nResolution path\nTypical review time\nManual action, unnatural links\nPaid placements that pass equity, scaled guest posts\nSharp ranking loss for affected pages or sitewide\nSearch Console > Manual actions\nRemove or qualify links, document cleanup, submit reconsideration\nSeveral days or weeks per Google\nAlgorithmic devaluation\nSpamBrain detects link buying or selling patterns\nGradual loss of benefit from new or existing links, no messages\nNo direct notice, infer from logs and link audits\nStop the tactic, clean up footprints, earn editorial links\nNo formal review, improvement after recrawling\nHow To Comply with Google’s Backlink Policy\nShort answer: qualify paid or compensated links, control UGC, and earn editorial citations through value. Build for users first and document your standards.\nDo that and you avoid most policy traps. Compliance starts with intent and implementation. If money, product, or quid pro quo is involved, the link must not pass PageRank.\nUse rel=”sponsored” for paid placements. Use rel=”ugc” for user generated areas that you moderate. Keep rel=”nofollow” for untrusted outbound links.\nGoogle treats these attributes as hints, not commands, so\nquality and context\nstill matter. Pair technical compliance with editorial standards and a clear outreach policy.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11651, "chunk_char_end": 13566}
{"id": "65477232-3969-4495-99f0-bb1e38b8360e", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "If something goes wrong and a manual action hits, remove or qualify offending links and submit a detailed reconsideration request in Search Console.\nAlthough, Google has recently pointed out the possibility of\nremoving the disavow link tool\nfunction from Google Search Console.\n5 point compliance blueprint\nQualify any paid, sponsored, or affiliate link with rel=”sponsored”. Nofollow is still acceptable but sponsored is preferred.\nModerate UGC at scale. Apply rel=”ugc” on forums, profiles, and comments, and filter obvious spam.\nAvoid link schemes. No excessive exchanges, automated placements, or scaled guest posts with optimized anchors.\nWatch for site reputation abuse. Do not host thin third party content that rides your domain’s authority.\nKeep an audit log. Track outreach sources, compensation, attributes used, and cleanup actions for future reviews.\nPolicy scenarios and the correct rel attribute\nScenario\nWhat is happening\nCorrect attribute\nWhy it is compliant\nSponsored review or paid placement\nYou pay or provide value for inclusion\nrel=”sponsored”\nGoogle expects paid links not to pass PageRank.\nAffiliate links to merchants\nYou earn commission on clicks or sales\nrel=”sponsored” or nofollow\nBoth qualify compensation and reduce abuse signals.\nCommunity forum links\nUsers link out in threads or profiles\nrel=”ugc” (optionally with nofollow)\nIndicates non editorial endorsement and reduces spam risk.\nYou reference a site but do not vouch\nEditorial mentions with low trust\nrel=”nofollow”\nSignals non endorsement so PageRank is unlikely to pass.\nThird party content on your domain\nContributors publish on your site\nAvoid low quality, off topic pages and qualify commercial links\nPrevents site reputation abuse and link manipulation.\nConclusion\nShort answer: build links that help users, qualify anything compensated, and avoid scale that looks manipulative. When in doubt, disclose and focus on editorial value.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13567, "chunk_char_end": 15494}
{"id": "a0332a44-a82f-460c-8c7e-a6705dfdac6a", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "Google rewards relevance and usefulness, then lets good links amplify that work. If you want a crisp action plan, use the scorecard below.\nGoogle’s written policies do not ask you to stop earning links. They ask you to stop manufacturing them for the primary purpose of ranking.\nPaid placements must be qualified, UGC must be controlled, and any third party content that rides your domain’s reputation should meet your editorial bar or be removed.\nWhen violations happen, Google either devalues the links with systems like SpamBrain or issues a manual action that you can see and address in Search Console.\nFAQ – Google Backlink Policy\nWhat is a “link scheme” in Google’s policy?\nAny pattern of links created mainly to influence rankings. Examples include buying or selling links that pass PageRank, excessive exchanges, large scale guest posting with optimized anchors, or automated placements. These can be ignored, devalued, or trigger manual actions.\nHow should I label paid, sponsored, or affiliate links?\nUse rel=”sponsored” for compensated links. nofollow is acceptable, and attributes can be combined where needed. Make the relationship clear and keep disclosures visible to users.\nDo nofollow, sponsored, and ugc links pass PageRank?\nGoogle treats these attributes as hints. Since 2019, Google may choose whether to consider or ignore them. Use them correctly to show intent and reduce risk.\nWhat is “site reputation abuse,” and why does it matter?\nPublishing low quality or off topic third party content to exploit a site’s ranking signals violates policy. Google announced and clarified this policy in 2024, with enforcement to curb “parasite” models.\nWhat is the difference between a manual action and algorithmic devaluation?\nManual actions are explicit penalties listed in Search Console with examples and scope. Algorithmic systems like SpamBrain neutralize link spam automatically, often without notices.\nWhen should I use the disavow tool?", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15495, "chunk_char_end": 17451}
{"id": "55eaaf19-985e-48b5-bb91-0353f211e78c", "url": "https://bluetree.digital/google-backlink-policy/", "source_domain": "bluetree.digital", "title": "Google Backlink Policy: The Complete Guide in 2025 - Blue Tree Digital", "section_path": [], "text": "Use it in limited cases, such as a manual action for unnatural links or a high risk footprint you cannot remove. Try removals first, then disavow remaining bad domains or URLs.\nDo backlinks still help in 2025?\nYes, but they are one signal among many. Links aid discovery and relevance, and their value increases when context and anchor text are strong. Content quality and usefulness remain decisive.\nHow do I recover after a link related manual action?\nRemove or qualify problematic links, document your cleanup, and submit a reconsideration request. Expect review time after submission, and avoid repeating the behavior.\nEric Koellner\nEric Koellner focuses on optimizing crawlability, site speed, and structured data. His audits have helped enterprise websites resolve critical issues and boost organic visibility.\nIs AI recommending you, or your competitors?\nBecome the Brand AI\nRecommends\nOur clients have jumped to\n447\nAI Overview placements and\n+437%\naverage organic traffic in 6 months, with AI clicks converting\n~50%\nbetter than standard SEO.\nBook a Free Audit\nNeed some advice before you decide?\nWe’re here to answer your questions and show you how to get started with building your link portfolio.\nSet up a discovery call\nDoes\nAI recommend\nyour business to people?\nUsing our proprietary technology we will measure your visibility in AI models and send you a report.\nGet Your AI Visibility Report\nGive your brand the exposure it deserves!\nConnect with our sales team now to start reaching new audiences.\nLet's Talk!\nLimited Time Offer\nSteal Our Pitch List!\n200+ sites, editor contacts, and the topics they accept. ⤵️\ndays\nhrs\nmins\nsecs\nSteal Our Links\n×\nTable of Contents\nTable of Contents\nGot Questions?\nChat with our expert sales team\nStart the conversation\nStart the conversation\nTalk to our Sales Team\nBook a Call\nSend an Email", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17452, "chunk_char_end": 19292}
{"id": "b94cf176-70d5-4853-9f6b-078604d31757", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Spam Links: How To Remove Spammy Links\nBack to all posts\nHome\n»\nBlog\n»\nLink Building\n»\nWhat Are Spam Links & How To Remove Them\nWhat Are Spam Links & How To Remove Them\nBy the Rhino Rank team\n1st Jan 2024\nAs we navigate the digital ecosystem, we find ourselves entangled in a web of deceptive tactics and manipulative strategies, making it increasingly challenging to distinguish friend from foe.\nOne of these elusive culprits is spammy links, which wield power to corrode the foundation of credible online content, tarnishing user experience and sabotaging digital integrity.\nBut we need to understand the anatomy of spammy links, and learning to wield the tools of identification is the shield that guards against this onslaught.\nThis article delves into the allure of spam links, exploring their origins and dangers, empowering you to safeguard the sanctity of your online presence.\nShort Summary\nSpammy links are hyperlinks created to manipulate search engine rankings or deceive users rather than provide genuinely valuable content.\nThey are harmful because they can lead to deceptive or malicious websites, compromise user privacy and security, and undermine the credibility and trustworthiness of legitimate online content and platforms.\nTo protect yourself from spammy links, you must adopt practices like only clicking on URLs from trusted sources and being cautious of unsolicited messages or emails containing suspicious links.\nWhat are Spam Links?\nSpam links also referred to as link spam, refer to bothersome and irrelevant hyperlinks you come across on the internet intending to manipulate how browsers rank websites and redirect unsuspecting users to other sites.\nThese hidden links are sprinkled across the online landscape, from websites and forums to comment sections, social media platforms, and emails.\nThey often show up without regard for whether they fit the context or are useful to the users of that platform.\nSchedule An Account Strategy Call", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1968}
{"id": "73a14d3c-cb4c-4729-a1ba-08556099cc4c", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Grow Your Business with Natural, High-Quality Backlinks\nSchedule Your Call\nWhat Is Google Link Spam Update?\nGoogle always works to improve its search results by ensuring that websites providing valuable and relevant content get higher rankings.\nThe link spam update is designed to crack down on websites trying to manipulate these rankings by buying or selling links, participating in link exchange networks, or creating a flood of irrelevant backlinks.\nInterestingly, this update doesn’t just rely on basic criteria. Instead, it uses advanced algorithms to evaluate the quality and relevance of a website’s backlinks.\nIf a website is caught in the act of link spamming, there are consequences. These could range from a drop in search visibility and lower rankings to being completely removed from results, particularly if the link spamming is extreme.\nConversely, websites that follow the rules and build high-quality, genuine backlinks can see positive outcomes. Their rankings could improve, leading to more visibility.\nSo, it’s like a digital quality control mechanism – rewarding the good players and giving the not-so-good ones a run for their money.\nTypes Of Google Penalties\nAs a leader in search engines, Google employs\nvarious strategies\nto maintain the quality of its search results.\nOne of the approaches it employs involves penalizing websites that resort to spammy practices when building their links.\nGoogle enforces two main penalties to discourage such behavior:\nThe Manual Link Penalty\nThe Algorithmic Link Spam Penalty\nLet’s look at both of these penalties to understand what they mean.\nManual Link Penalty\nGoogle’s Manual Link Penalty is a direct response to websites that are caught using unnatural or manipulative techniques to boost their rankings.\nUnlike automated algorithmic penalties, these penalties are decided by real human reviewers at Google. These reviewers examine each case individually, looking into reported violations or suspicious activities.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1969, "chunk_char_end": 3950}
{"id": "4d42c6a4-552e-4468-8848-689f25d964c0", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "When a website is hit with a Manual Link Penalty, its visibility in the results can take a major hit.\nGoogle might take specific actions, such as downplaying the impact of spammy or manipulative links, leading to a noticeable drop in its search rankings.\nThese penalties come in varying degrees of severity, ranging from a decrease in visibility for specific keywords to complete removal from results.\nTo resolve a Manual Link Penalty, website owners must thoroughly review their backlink profiles.\nThey should identify any spammy or unnatural links and then take steps to disavow or remove those links. Once the necessary actions have been taken, the website owner can request reconsideration from Google.\nIn this request, they explain the steps taken to address the issue and show their commitment to following proper link-building practices.\nAlgorithmic Link Spam Penalty\nIn contrast to manual penalties decided by humans, the Algorithmic Link Spam Penalty is automatically applied by Google’s algorithms.\nThese algorithms always evolve to detect spam and link manipulation patterns on a broader scale.\nWebsites that engage in practices like link spamming, which involves creating irrelevant or low-quality backlinks or participating in link schemes, risk triggering an Algorithmic Link Spam Penalty.\nWhen triggered, this penalty can cause a sudden ranking drop or reduced keyword visibility for a site owner.\nRecovering from an Algorithmic Link Spam Penalty requires a comprehensive review and cleanup of the website’s backlink profile.\nThis involves identifying and disavowing spammy links and ensuring that future link-building efforts align with Google’s quality guidelines.\nUnlike manual penalties, there’s no need to submit a reconsideration request for algorithmic penalties. As time passes and the algorithms reassess the website’s link profile, improvements in rankings should become noticeable.\nFactors Contributing to Spam Links", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3951, "chunk_char_end": 5893}
{"id": "b8e60d39-f9c7-4502-84c1-ee4c446ba810", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Understanding the factors contributing to the creation and spread of misleading links gives us a valuable peek into the strategies spammers employ.\nLet’s dive into some of these factors and shed light on how they play a role in the intricate world of spammy link-building practices.\n1. Abnormally Small Site Mark-up\nThe markup, or structure, of a website’s code, holds valuable information browsers use to understand the content and context of a page. Websites with abnormally small or insufficient markup can raise suspicions.\nThese sites often lack the necessary information for browsers to accurately determine their relevance, making them vulnerable to being associated with spammy or low-quality content.\nSpammers might keep their websites’ markup limited to hide their true intentions or escape search engine algorithms’ radar.\nThese tactics mess with search engine results pages’ ability to properly gauge content quality, leading to inaccuracies in results and potential exposure to spam links.\n2. Low Number of Internal Links\nInternal links\nare the connections between different pages within a single website. A website with too few\ninternal links\ncan make us wonder about its credibility and worth.\nInternal links are like signposts that guide users through a website’s content and help browsers understand how the site is structured.\nSpammers could avoid internal links to prevent their poor-quality or irrelevant content from getting noticed. But, this strategy messes with user navigation and disrupts the user experience.\nA genuine website, in contrast, leverages internal links to make things easier to navigate and offers meaningful pathways for users to explore.\n3. Small Portion of Anchor Texts/Branded Links\nAnchor texts\ngive users a clue about what’s on the linked page.\nWebsites that don’t have a variety of anchor texts or rely too heavily on branded ones might be trying to manipulate search rankings.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5894, "chunk_char_end": 7818}
{"id": "6b74b3e5-3228-4ce9-8ea7-025625460eaf", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Spammers often use branded anchor texts to create an illusion of legitimacy, hiding the true nature of the content they’re promoting.\nThis approach goes against the natural diversity of anchor texts that would show up in genuine organic links.\n4. A Large Number of External Links\nHaving extraneous links can add credibility, but if there are too many, it could raise eyebrows about spammy behavior.\nWebsites that throw around these links like confetti, especially to low-credibility or irrelevant sites, might seem like they’re trying to play games with search rankings.\nSpammers might launch aggressive link-building campaigns to pump up their website’s authority artificially.\nThis could mean peppering their content with tons of extrinsic links to trick browsers into thinking their site is a well-connected and reputable one.\n5. The Presence of External Links in Navigation\nWhen websites pepper their navigation menus with extraneous links, they unknowingly set themselves up for spammy risks.\nThis could signal to browsers that the website leads users to low-quality or irrelevant stuff.\nSuch tactics can attract spammy backlinks from similarly disreputable sites, ultimately tarnishing the credibility of the linked site.\nTo avoid this, website administrators must be cautious and ensure that exterior links in navigation add real value to the user experience.\n6. Backlinking Website’s Domain Length\nThe length of a domain name in backlinks can reveal a lot about its authenticity. Short, snappy domain names are usually connected with established and trusted websites.\nConversely, overly long domain names could hint at less trustworthy sources. Spammers might opt for long domain names to mimic real sites, fooling unsuspecting users into clicking on these bad links.\nBeing cautious and assessing the legitimacy of backlinking domains based on their length and overall appearance is key.\n7. Top-level Domains (TLDs) Correlated with Spam Domains", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7819, "chunk_char_end": 9771}
{"id": "dcba4101-b0f6-4e1e-a27e-0adcb32cc413", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Some top-level domains (TLDs) have a higher tendency to be linked with spammy domains.\nTLDs like .info, .biz, and .xyz have often been associated with a greater number of spammy websites. Not all websites with these TLDs are bad, but they get extra attention from browsers.\nWebsite admins should tread carefully when dealing with backlinks from domains with TLDs with a history of spam and ensure the content aligns with their site’s theme.\n8. Backlinking Domain Having Numerals\nIf there are numbers in the domain name of a\nbacklinking site\n, it might set off alarm bells for spammy intent.\nSpammers sometimes use numbers to make their domain names seem unique or relevant.\nBut this often leads to domain names that feel artificial or unrelated to genuine content.\nTherefore, browsers might raise an eyebrow at websites with numbers in their domain names, tagging them as possible spammy candidates.\nLooking closely at backlinking domains with numbers is vital to determine if they’re real and relevant.\nThe Different Types of  Spam Links\nIn the vast and intricate realm of the internet, link spam poses a constant threat, lurking in the shadows and undermining the essence of online experiences.\nThese deceitful links come in various guises, each with the potential to erode user trust, manipulate search engine rankings, and even expose individuals to cybersecurity vulnerabilities.\nHere, we dive into some of the most prevalent types of spam links that pervade the digital landscape.\n1. Comment Link Spam\nA nefarious activity known as comment link spam thrives within websites and blog comment sections.\nLink building\ninvolves leaving irrelevant or generic blog comments and links to dubious websites.\nThese comments often have little to no relevance to the page’s content. However, the intention is to acquire a backlink to the link spammers’ site for SEO manipulation.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9772, "chunk_char_end": 11645}
{"id": "c4cb972b-5f94-48bd-ba35-8cc94d1051b2", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "To counteract this, website owners need to be vigilant in overseeing and curating comment sections to prevent comment spam from diluting the richness of discussions.\n2. Forum and Blog Profile Spam\nLink spammers have their sights set on online discussion forums and blogs, where they create user profiles that discreetly harbor links to their websites.\nThey attempt to give their links an air of legitimacy by engaging in discussions and interactions.\nHowever, more often than not, these blog spam links lead to unrelated or subpar sites, diminishing the platform’s overall value.\nDiscussion forums and blog administrators must enforce stringent user profile guidelines to thwart these efforts at link manipulation.\n3. Link Farms and Web Directories\nLink farms\nrepresent networks of websites created solely to host links. These farms contribute no meaningful content and exist solely to enhance linked website rankings.\nSimilarly, initially intended to aid users in discovering valuable websites, web directory links have fallen into disrepair, becoming storehouses of low-quality blog feed directories.\nAssociating with link farming and dubious directory spam links can result in penalties from browsers, tarnishing the reputation of the linked website in the process.\n4. Guest Post Spam\nGuest posting, when practiced ethically, entails offering valuable content to reputable websites in exchange for backlinks.\nYet, link spammers exploit this practice by submitting poorly written, irrelevant, or spun content to numerous sites to gain backlinks.\nThese links often lead to low-quality sites or even those with malicious intent, ultimately sowing doubt regarding the credibility of both the guest post host and the linked website.\n5. Paid Links and Link Exchanges\nThese links involve purchasing website backlinks to boost search engine rankings.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11646, "chunk_char_end": 13491}
{"id": "0d9a4e63-8306-462d-bf24-e7a40079ca60", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "While not all paid links are synonymous with link spam, those procured from disreputable sources or those that run afoul of search engine guidelines can yield unfavorable outcomes.\nLink exchanges\n, where two websites mutually agree to link to each other, can also spiral into some types of link spam territory if done excessively or with partners that lack relevance.\n6. Anchor Text Manipulation\nAnchor text, the clickable text within a hyperlink, is subject to manipulation by spammers who cram it with keywords or phrases to inflate search engine rankings artificially.\nThese post links frequently lead to content that bears no relation to the anchor text, hoodwinking both users and browsers.\nBrowsers are progressively becoming more adept at identifying and penalizing this form of manipulation.\n7. Cloaking and Hidden Links\nCloaking revolves around presenting disparate content to search engine crawlers and human visitors.\nHidden link schemes\n, however, deliberately elude easy visibility by blending with the background color or strategically placing in a page’s inconspicuous corners.\nBoth tactics share a common goal: deceiving browsers while funneling users toward undesirable or malicious content.\n8. Low-Quality Press Release Websites\nLow-quality press release websites host and distribute press releases without maintaining high editorial standards.\nThese platforms may publish content that lacks relevance, credibility, or originality. When websites obtain backlinks from such sources, they risk associating themselves with content that doesn’t add value.\nGoogle’s algorithms have evolved to detect and devalue links from these sites, as they contribute to manipulating links rather than providing genuine information dissemination.\n9. Excessive Links From Non-Niche Related Websites\nRelevance is a key factor in assessing the quality of backlinks. When websites accumulate many links from sources unrelated to their niche or industry, it raises suspicions of a manipulated link.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13492, "chunk_char_end": 15485}
{"id": "56dc4054-c67d-4c9c-bd26-aba96d070e4d", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Google’s algorithms prioritize links that come from authoritative and contextually relevant sources.\nExcessive non-niche-related backlinks can trigger penalties, leading to lower rankings.\nIt’s crucial to focus on quality over quantity and foster organic relationships within the relevant niche.\n10. Backlinks From Websites In Foreign Languages\nWhile backlinks from reputable foreign-language websites can be beneficial, low-quality ones can be problematic.\nThese links indicate attempts to manipulate search engine rankings without considering the value of the content.\nWebsites with an influx of spammy backlinks from foreign-language sites might experience penalties if those links are deemed irrelevant or manipulative.\nVigilance is required when engaging with websites in languages one doesn’t understand to ensure the quality of the linking source.\n11. Guest Posting At A Scale\nGuest posting is a legitimate practice when done responsibly, but it can lead to spammy link-building when taken to an extreme scale.\nIf websites excessively churn out guest posts on various platforms, often with low-quality or irrelevant content, it can raise flags.\nGoogle aims to reward authentic, valuable content, so guest posting solely to obtain backlinks can lead to penalties.\nA balanced approach, focusing on quality content that genuinely benefits the target audience, is essential.\n12. Links From Generally Spammy Websites\nLinks originating from a generally spammy web page come from platforms with low credibility, poor content quality, and little relevance to the topic at hand.\nThese websites often engage in aggressive link-building practices, seeking to manipulate search engine rankings.\nAcquiring links from such sources can raise red flags, as they may negatively impact a website’s credibility and search engine rankings.\nThese spammy websites often lack valuable content and user engagement, making it crucial to avoid associating with them to maintain a reputable online presence.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15486, "chunk_char_end": 17473}
{"id": "9a406b7b-d3a1-49e3-a3fb-1a6a822c467d", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "13. Article Marketing Spam\nArticle marketing spam involves distributing low-quality, often spun or duplicated articles across various online platforms to include backlinks to a target website.\nThis practice was once common as a link-building strategy but has since been deemed unethical by browsers.\nUsually found on content farms or low-quality sites, these articles in article directories offer minimal value, merely boosting link profiles.\nBrowsers have become adept at recognizing such spammy articles, making this approach ineffective and potentially harmful to a website’s reputation.\n14. Linkbait and Switch\nLinkbait and switch is a deceptive approach where a website initially presents engaging content to attract users and links but subsequently changes the content to something different.\nThe aim is to manipulate the link-building process by initially attracting legitimate backlinks and switching the content to something unrelated or spammy.\nThis practice erodes trust and can lead to penalties from browsers. Linkbait and switches tarnish the website’s reputation and undermine the credibility of link-building efforts.\n15. Social Bookmarking & Sharing Sites\nWhile social bookmarking and sharing sites can be legitimate platforms for content dissemination, they can also become a breeding ground for spammy links.\nCertain users misuse these platforms, using blog networks to submit subpar content with excessive and irrelevant backlinks.\nThis can result in spam content that diminishes the platform’s value and hampers users’ ability to find relevant and useful information.\nBrowsers are attentive to such practices, and links from these spammy submissions might not carry the desired positive impact on search engine rankings.\n16. Profile Spam\nProfile spam involves the creation of user profiles on various platforms, such as forums, social media, or online communities, with the primary purpose of inserting spammy links.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17474, "chunk_char_end": 19411}
{"id": "58c49754-e276-4c48-8726-ddbfe03b792e", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Deceptive spammers create authentic profiles and participate in discussions on fake blogs to evade identification.\nAfter creation, these profiles often incorporate links to subpar or unrelated sites within descriptions or posted content on blog networks.\nThe intention is to gain\nbacklinks for SEO\nwhile deceiving users into clicking on the links.\nProfile spam undermines the integrity of online communities and platforms and tarnishes user trust.\n17. Domain Purchase and Redirect/Canonical\nThrough this method, spammers buy expired domains with existing authority or backlinks to create backlinks.\nThese domains are then redirected or set up with canonical tags to point to the spammer’s target website.\nThis technique exploits the purchased domain’s existing reputation to pass authority to the spammer’s site.\nHowever, the content on the purchased domain is often unrelated to the target site, deceiving both users and search engines.\nThis practice is frowned upon by browsers and can lead to penalties if detected.\nWhat Do Spam Links Do?\nSpam links sneak their way into the vast digital landscape, causing chaos amidst the smooth flow of information and putting the credibility of online experiences at risk.\nThese links act as gateways to various undesirable outcomes, shaking the foundation of websites’ trustworthiness and potentially exposing users to security vulnerabilities.\n1. Erosion of Credibility\nSpam links play a significant role in eroding the credibility of websites, as they connect legitimate platforms with untrustworthy sources.\nWhen search engines detect a flood of these shady links pointing towards a website, they tend to view it as less reliable, which can lead to a downfall in its search ranking.\nThis not only tarnishes the website’s reputation but also shakes the faith users have in its content.\n2. Manipulating Search Engines\nInundating the digital space with spam links is often employed to manipulate search engine algorithms.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19412, "chunk_char_end": 21374}
{"id": "e4673008-39f5-4dde-9e2f-e3e0dcffda5b", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "By artificially inflating the number of backlinks to a website, a spam link hopes to boost its search engine ranking in the Google Search Console.\nHowever, this goes against the principles of fair competition and can result in subpar content gaining an unjustified spotlight, thereby sidelining genuine and valuable sources.\n3. Deteriorating User Experience\nSpam links can lead users down a rabbit hole of irrelevant and poor-quality content.\nWhen users click on a link, expecting to find relevant information, but instead come across deceitful or off-topic content, it chips away at their trust.\nThis can lead to frustration and a compromised online experience.\n4. Vulnerability to Malware and Phishing\nSome spam links are gateways for malware, ransomware, and phishing attacks.\nUsers who unknowingly click on such links might expose themselves to malicious software that can compromise their devices and personal data.\nCybercriminals often exploit human curiosity or urgency to lure users into clicking these treacherous links.\n5. Legal Consequences\nParticipating in spam link building and practices can have legal consequences for website owners and creators.\nBrowsers and legal authorities are becoming more vigilant in penalizing those who engage in such activities, as they undermine fair online competition and user safety.\n6. Undermining SEO Efforts\nLegitimate search engine optimization (SEO) endeavors can take a hit from spam links.\nWhen browsers notice an unnatural surge in low-quality links, they might penalize the website, leading to a sharp drop in its ranking.\nThis counterproductive outcome hampers genuine SEO efforts and can be a time-consuming challenge to rectify.\nHow to Identify Spam Links: Step-by-Step Guide\nSpotting spammy links takes a sharp eye and a methodical approach to sift through a website’s backlink profile.\nBy following these step-by-step guidelines, you can effectively identify and tackle those spammy links to safeguard your online presence:", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 11, "chunk_char_start": 21375, "chunk_char_end": 23359}
{"id": "a1981a57-b00a-4889-8e98-8e828ce73366", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Step 1: Gather Backlink Data\nGet hold of your website’s backlink data from tools like Bing Webmaster Tools or even some reliable third-party SEO tools.\nPut together a list of all the domains linking to your site.\nStep 2: Analyze Anchor Texts\nTake a close look at the anchor texts used in those backlinks. Watch for anchor texts that seem overly optimized or just irrelevant phrases.\nGenuine backlinks generally feature anchor texts that flow naturally and fit contextually.\nStep 3: Check Domain Quality\nSize up the quality of those domains that are linking to you. Seek out indicators of trustworthiness, like establishing a clean website layout and valuable content. Dodgy or low-quality domains might be signaling those spammy links.\nStep 4: Review Domain Relevance\nEnsure that the domains linking to you match the content and theme of your website.\nBacklinks from totally unrelated sites are usually red flags for spammy behavior.\nStep 5: Monitor Link Velocity\nKeep an eye on the rate at which new backlinks keep popping up. If you suddenly see a flood of new links, there’s a chance someone’s trying to play with rankings, which often hints at spammy link-building.\nStep 6: Check for Duplicate Content\nGo through the content surrounding these backlinks. Spotting the same or very similar content duplicated across multiple domains is a sign of manipulative link-building tactics.\nStep 7: Investigate Redirects and Canonicals\nLook out for situations where the links steer you to a different page or even a whole different domain.\nAlso, watch canonical tags, as they could suggest content duplication. Spammers use these tactics to trick users and other search engines.\nStep 8: Examine Social Media and Forum Links\nTake a deeper dive into links from social media profiles or forums. If most of these links come from accounts with low engagement or credibility, they might be spammy.\nStep 9: Check Link Placement", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 12, "chunk_char_start": 23360, "chunk_char_end": 25273}
{"id": "0d3e50ea-9b75-4de2-a9aa-90f7bfed20f8", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Think about where those backlinks are placed on the linking pages. Links buried in footers, sidebars, or author bios might not carry as much weight as links within the main content.\nStep 10: Utilize Google’s Disavow Tool\nWhen you do spot those spammy links, make a list of the domains involved and use Google’s Disavow Tool. This tells Google that you want to dissociate from those links. Remember, this tool is a last resort; you should try manually removing or disavowing it first.\nStep 11: Regularly Monitor and Audit\nRemember, spammy links can pop up over time, so monitoring your backlink profile is important. Keep reviewing new links and apply the same criteria to judge their quality.\nStep 12: Seek Professional Help\nIf you’re unsure about spotting spammy links or need help cleaning up your links, consider getting advice from a seasoned SEO professional. They can offer valuable guidance and expertise.\nThe Best Spam Link Checkers\nMaintaining the health of your website’s backlink profile is crucial, and having reliable spam backlink checkers at your disposal can make all the difference.\nLet’s dive into some top-notch tools perfect for this job. Each of them comes with unique features that will aid you in effectively spotting and tackling spam links.\n1. SEO Spyglass\nRegarding comprehensive backlink analysis, SEO Spyglass is a standout performer. Its ability to pinpoint spammy backlinks is impressive.\nThrough its detailed analysis, you can gain insights into the sheer quantity of links and the quality, relevance, and authority of the domains they originate from.\nOne of its standout features is the Penalty Risk metric, which helps you zero in on potentially harmful links, allowing you to take proactive measures.\nMoreover, SEO Spyglass is a great help when identifying unnatural anchor text distribution.\nIt even offers a disavow file generator, which simplifies the process of cleaning up your backlink profile.\n2. Raven Tools", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 13, "chunk_char_start": 25274, "chunk_char_end": 27223}
{"id": "ccc619ba-95aa-404b-b57c-98881e2b2f40", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Raven Tools could be your go-to choice for those who prefer an all-in-one SEO solution.\nIts Link Manager tool is a robust platform that aids in tracking and evaluating your backlink profile’s health, including identifying those sneaky spam links.\nThe reporting features that come with Raven Tools are particularly impressive.\nThey allow you to create detailed reports that can be shared with clients or team members, showcasing your progress in uncovering and disavowing those unwanted spam links.\n3. Google Search Console\nAlthough this feature isn’t exclusively designed for detecting link spamming. it’s a powerful free tool from Google that offers insights into how your website is performing.\nIt’s worth mentioning here because it provides information about your backlinks and allows you to disavow harmful links within the tool.\nIf you want to handle types of link spam without extra costs,\nGoogle Search Console is a good solution\n.\n4. Ahrefs\nAhrefs\nhas gained a solid reputation in the world of SEO, and its backlink analysis feature is a gem for uncovering spammy backlinks.\nTheir Site Explorer provides an extensive array of data related to referring domains, backlink profiles, and the distribution of anchor text.\nA standout feature is their built-in spam score that helps identify potential spammy domains.\nAhrefs goes beyond identification, providing a disavow feature to tackle link scheme issues. Export and address troublesome links easily.\nUsing Moz Spam Analysis to Test Links\nDelve into Moz’s Spam Analysis tool for insights into backlink profile quality, identifying more links and possible spam effectively.\nMoz offers a nifty metric called the ‘\nSpam Score\n,’ which shows how likely a backlink might be spam.\nLet’s walk through how you can effectively utilize Moz’s Spam Analysis tool to evaluate your links:\n1. Getting Started with Moz’s Spam Analysis Tool", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 14, "chunk_char_start": 27224, "chunk_char_end": 29103}
{"id": "9a833a86-e22a-470b-a4dd-391691ab929b", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "First, you’ll need to access Moz’s Spam Analysis tool. You can find this useful tool within Moz’s suite of SEO tools or by visiting the Moz website directly.\n2. Inputting the Link or Domain\nNow, you need to enter the link or domain that you want to put to the test for any spam indicators.\nThis could be a specific URL pointing to your website or even a backlink coming from another site altogether.\n3. Unveiling the Spam Score\nOnce you’ve entered the link or domain, Moz’s Spam Analysis tool will present you with a Spam Score.\nThis score is a number that estimates how likely a particular link or domain is to be associated with spam.\nThe Moz Spam Score range spans from 0 to 17, with higher scores suggesting a potentially higher chance of the link being spammy.\n4. Understanding the Spam Score\nThe Moz Spam Score incorporates factors and patterns frequently associated with link wheel tactics and spammy links.\nAlthough it’s not an absolute verdict on whether a link is spam, it is a valuable clue.\nUser comments play a role: A lower Spam Score, ideally near 0, indicates a cleaner, trusted link for your consideration.\nConversely, a higher Spam Score closer to 17 could be a red flag that deserves extra attention.\n5. Considering Other Aspects\nRemember, these spam scores are rooted in algorithms and patterns. So, it’s a smart move also to consider other elements.\nDelve into the link’s context, assess the credibility of the domain doing the linking, and factor in how relevant the content is.\nMoz’s Spam Analysis tool is part of your toolkit for spotting spam. Merging it with your judgment and other backlink analysis tools can paint a more complete picture.\n6. Making Educated Choices\nArmed with the Spam Score and your evaluation, you’re in a position to make informed decisions about whether you want to keep, disavow, or eliminate the link.\nLink wheels influence spam scores. Lower scores signal reliability, while higher ones demand attention and action.\n7. Routine Check-ins", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 15, "chunk_char_start": 29104, "chunk_char_end": 31093}
{"id": "8a773598-2f5d-4c59-bbed-6a5f18d5aa8e", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "It’s a good practice to conduct regular check-ups on your backlink profile using Moz’s Spam Analysis tool and similar resources.\nConsistently monitoring and addressing any improper links over time can contribute to a stronger backlink profile and boost your search engine rankings.\nWhat To Do Once You Have Identified A Spammy Link\nOnce you’ve identified a spammy link in your backlink profile, taking prompt and appropriate action is crucial to safeguard your website’s reputation and overall online visibility.\nHere’s a step-by-step guide on what to do once you’ve identified an incidence of link spamming:\n1. Document the Details\nRecord all relevant details about the spammy link, including the linking domain, the specific URL linking to your site, the anchor text used, and any other pertinent information.\nThis documentation will be useful for reference and communication.\n2. Assess the Severity\nEvaluate the severity of the spammy link. Consider factors like the source’s credibility, the spam score, the relevance of the link, and its potential impact on your site’s rankings and reputation.\n3. Attempt Contact\nIf the linking domain appears legitimate, but the link is irrelevant or appears to be an oversight, consider contacting the webmaster.\nPolitely request that they remove or give you link spam updates. Provide them with the necessary details for identification.\n4. Disavow the Link\nIf attempts to remove the link through direct communication are unsuccessful or if the link is from a spammy source, you can use Google’s Disavow Tool.\nThis tool informs Google that you want to disassociate your site from the spammy link.\nCreate a text file listing the URLs or domains you wish to disavow and submit it through Google Search Console.\n5. Remove or Replace the Link\nIf you have control over the content where the spammy link is located (e.g., guest post, forum comment), consider editing or removing the link.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 16, "chunk_char_start": 31094, "chunk_char_end": 33017}
{"id": "32606e69-8114-459a-a718-2e6f63c5b06d", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Replace it with a relevant and legitimate link, enhancing the value of your content.\n6. Monitor Progress\nRegularly monitor the status of the spammy link. If you’ve requested removal, check if the link has been taken down.\nIf you’ve disavowed the link, monitor your search console for rankings or penalty changes.\n7. Continue Auditing\nConduct periodic backlink audits to ensure that your efforts are effective and to identify any new junk links that may have appeared over time.\n8. Learn and Adapt\nUse the experience of identifying and addressing bad links to improve your ability to spot such links in the future.\nStay informed about evolving practices and search engine algorithms to protect your site from spam-related issues better.\n9. Seek Professional Help\nIf you’re unsure about the appropriate course of action or if you’re dealing with a significant number of junk links, consider seeking advice from experienced professionals who can guide you through the process.\nBottom Line on Factors Contributing to Spam Links\nUnderstanding the factors that contribute to the emergence of spammy links is essential to maintaining the integrity of online experiences.\nVarious elements play a role in the proliferation of these deceptive links, and their identification empowers website owners, administrators, and users to navigate the digital landscape more safely.\nEach factor sheds light on the tactics employed by spammers to manipulate search engine rankings, deceive users, and exploit platforms.\nRecognizing these factors empowers individuals to make informed decisions when assessing backlinks and their sources.\nWe can collectively contribute to a healthier online ecosystem by staying vigilant, engaging in ethical link-building practices, and employing reputable backlink analysis tools.\nAwareness of these contributing factors is essential in safeguarding our online journeys’ authenticity, credibility, and security.\n100% Satisfaction Guarantee\nReady to place your first backlink order?", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 17, "chunk_char_start": 33018, "chunk_char_end": 35013}
{"id": "ebe2168c-de35-4cd3-8427-82d5380c522c", "url": "https://www.rhinorank.io/blog/identify-and-avoid-spam-links/", "source_domain": "www.rhinorank.io", "title": "Spam Links: How To Remove Spammy Links", "section_path": [], "text": "Buy Now\nFrequently Asked Questions\nWhat are spam links?\nSpammy links are deceptive hyperlinks used to manipulate search rankings or mislead users. They can damage your site’s credibility, lead to security risks, and result in lower search engine rankings.\nHow to find spam links?\nYou can identify spammy links by checking your backlink profile for irrelevant or low quality domains, unnatural anchor texts, and suspicious link patterns.\nHow to remove spam links?\nYou should request the removal of spammy links from the source sites. If that’s not possible, use Google’s Disavow Tool to disassociate your site from those links.\nrelated Blog Posts\nHow To Master Natural Anchor Text\nThese Are The Best SEO Link Building Agencies Of 2025\n10 Link Building Services That Actually Work\nJoin 26,000+ Businesses Growing with Rhino Rank\nSign Up\nKeep Reading\nBlog Category\nB2C SEO: What Is Consumer-Focused Search Engine Optimization?\nBlog Category\nWhat Are Short Tail Keywords In SEO?\nBlog Category\nHow To Repurpose Content & Maximize Your ROI\n×", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 18, "chunk_char_start": 35014, "chunk_char_end": 36049}
{"id": "6ac7d45d-c634-4b6d-b1d1-2ea51931163c", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "The Complete List of Google Penalties - Manual Actions Guide | Zeo\nSkip to Main Content\nResources\n/\nDigital Marketing and SEO Blog\n/\nThe Complete List of Google Penalties - Manual Actions Guide\nSamet Özsüleyman\nin\nSearch Engine Optimization\n22 Jun 2021\nWeb Spam Overview\nManual Action Penalty\nWhat is the Duration?\nGoogle Penalties\nReconsideration Requests\nWhat Happens When a Penalty is Received from Google?\nSubscribe to Newsletter\nThe Complete List of Google Penalties - Manual Actions Guide\nWebsites whose actions violate Google's guidelines can get various penalties. While many webmasters report \"Google has banned my website\" or \"I am banned by Google\", this situation can be defined as a Google Penalty or manual action in general. Mentioning Google penalties and discussing the authorities' explanations about manual actions in it, I want this article to serve as a handbook.\nThe purpose of the penalties is to \"clean up\" search algorithms by removing the spammy content so that they become more useful to users. The release of the algorithm called\nPenguin\nin April 2012 has been a critical step towards comprehending web spam techniques.\nWeb Spam Overview\nGoogle has published several statistics to\nshow\nhow to deal with spam in the past years. It has occasionally\nreminded\nwebmasters of some warnings in the blogs.\nTo summarize\nthe spam statistics in 2019;\nMore than 25 billion spammy pages.\n230,000 spam reports were received every day, and Google was able to take action on 82% of those reports. This number was 180,000 in 2018, and action was taken on 64% of them.\n4.3 million manual action messages were generated to website owners.\nWhile the amount\nof spammy content produced by users\nincreased in 2018, it did not increase in 2019.\nBacklink spam has become the most popular spam type.\nThe effect of spammy content on users in 2019 was reduced in 2018 by 60%.\n90% of backlink violations were detected by algorithms.\nNew explanations\nwere made about the use of nofollow.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1985}
{"id": "06b0f41e-4c79-4838-b911-cdcca95b6ee5", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "The spam generated through hacked websites was stable compared to 2019.\nSince the\nstatistics page\nof Google fighting spam is directed, I've had a quick look at the latest data on the web archive to present you. A diagram showing that 0.22% of sites in the index were given manual action: (2018)\nPure spam is one of the most common types of spam. You can find out more about pure spam below in this article.\nIn the previous years, Google shared some examples of the websites that were given manual action:\nThe screenshot I took in February 2018 shows the reconsideration requests by week from 2006 to that date:\nWith the use of machine learning, Google was able to detect spam types through manual actions and algorithms more quickly. Google's explanations indicate that algorithms can't prevent all the spammy behavior and the system is not perfect:\nIt is explained in another blog post that the systems used to detect spam are not always flawless and open to improvement. You can access 2018 spam statistics\nhere\n. We will find out how Google will fight spam in 2021.\nGoogle uses spam reports to\nimprove its spam detection algorithms\n.\nYou can report webspam by filling out the\nspam report form\n. It is almost impossible to tell if the spam reports are processed. Google can take manual action only on specific URLs or website-wide.\nWhat is a Google Manual Action Penalty?\nAs the name indicates, manual action is the penalty given out to some pages or websites by a human reviewer.\nIt is almost impossible to detect the penalties given by Google. The owners of accounts in Search Console can see the penalties issued by Google. Because of this, I advise you not to rely on systems such as Google ban questioning.\nJ. Mueller\nstated\nthat most of the manual actions were done algorithmically and they were probably no more needed:\nWhat Happens When a Penalty is Received from Google?\nWhen Google applies a manual action to your site:\nYour site's ranking in search results may drop,", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1986, "chunk_char_end": 3965}
{"id": "5fd59013-bca9-4207-baa2-8857ef4b24f9", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Your rich results may not be displayed in SERP,\nYour site may be removed from the Google index,\nYour site's organic conversion rate may drop,\nYour site's organic traffic may decrease.\nIf you're on platforms such as Google News, the traffic may flatline.\nNoting the dates on which a manual action has been issued and removed will be helpful for future analyses. Your site's traffic may suddenly drop after a manual action.\nWhat is the Duration of a Manual Action Penalty?\nThere is not a time limit for manual actions. Some might be removed in a few days, while others might cost you 1 or 2 years. Reconsideration requests and fixing problems will shorten this period.\nYou can log into Search Console to find out if your site has been penalized in the previous years:\nManual Action Types (Google Penalties)\nGoogle's manual actions on websites differ. I would like to discuss it in detail. Unlike algorithmic penalties, the website owners receive an e-mail in case of a manual action.\nKeyword stuffing\nKeyword stuffing, which means using irrelevant keywords on a page just to increase traffic rather than inform the users, is a reason for manual action.\nExamples of keyword stuffing:\nPages that have only phone numbers but no content with value,\nPages that have been created for spammy purposes with random content such as the names of cities or districts.\nUsing the same keywords both on the page and in meta description tags to generate spam, as in:\n\"We sell custom coffee tables. Our coffee tables are handmade. If you're thinking of buying a custom coffee table, please contact our custom coffee table specialist at custom.coffee.tables.@thewebsitename.com.\n\"\nWebsite owners should create content in a way that the keywords they want to use to be listed with would not cause spam. Not to be penalized due to this spam type, which is a Black-hat SEO method, a page should not contain such activities. However, it does not necessarily mean that websites with such content comply with the guidelines.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3966, "chunk_char_end": 5964}
{"id": "04a3c825-5e41-4949-a7a7-a719ec72725c", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "In some cases, Googlebot can skip pages stuffed with keywords and\nscan other pages of the site instead\n. Also, if the site has some other kind of value, Google\ncan ignore\nkeyword stuffing.\nYour pages might rank low or underperform without being given manual action because Google may ignore them.\nThis type of spam matters especially to EAT.\nGuidelines\ninclude an example of \"Keyword Stuffed\":\nYou can see an example of keyword stuffing below:\nPlease be reminded that\nthere is no ideal keyword density\nfor the use of keywords.\nWhen he was asked why websites with stuffed keywords ranked highest, John Mueller explained \"Usually, we're just ranking them because of other factors. Getting one thing wrong doesn't mean you'll never show up in search\":\nUsually we're just ranking them because of other factors - getting one thing wrong doesn't mean you'll never show up in search (lots of sites get things wrong, but they're still useful).\n— 🍌 John 🍌 (@JohnMu)\nNovember 23, 2018\nSpammy behavior\nshould be avoided\nin Google My Business (GMB) names. Your business accounts might be suspended because of such manipulative behavior frequently observed in local SEO:\nIn a patent issued in 2016\n, Google explains how it detects fraudulent business profiles:\nComment Spam\nSome users that post comments might be ill-intentioned. Google started to make\nexplanations\nabout this type of spam in 2015. Such users may cause spam in your content with malicious links they add to their usernames or comments, which is against Google guidelines.\nIf there are spammy comments on each page of your website, it lets Google view those comments and may cause it to think that you encourage spammy content. If you allowed all the comments without checking before, I recommend that you go back to the relevant pages and read each comment. Not only the blogs but also the websites with a guest book should be careful about it.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5965, "chunk_char_end": 7863}
{"id": "af43c323-4679-4365-b8ea-259219846e13", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "If you allow spammy comments on your site without moderation and receive a manual action notification:\nYour pages might rank lower,\nYou direct your loyal users to irrelevant or malicious websites,\nPages with spammy comments may be removed from the index.\nYou can view comments on CMSs like Wordpress and delete the spammy ones before posting them to your website. If your site is attacked by spammy comments, you may consider disabling comments temporarily:\nYou may take additional safety measures for commenting on Wordpress. You may also consider using\nreCAPTCHA\n:\nYou can add nofollow attribute to comment links outbound from your site so that links that could be unnoticed no longer pass Pagerank and the spammers may stop commenting. It is default in systems such as Wordpress:\nIt could also be useful for forums to disable the crawling of pages such as profile pages and use robots.txt to block them. As the software solutions are improved, this spam type is getting less prevalent now:\nFor further information regarding this type of manual action, go to this page:\nWays to Prevent Comment Spam\n.\nUser-Generated Spam\nSometimes when necessary precautions are not taken, spammy pages generated by users can cause you to get manual action.\nWhat is user-generated spam?\nThis is the type of spam that happens on forums, blogs, or basically any page on which users are allowed to leave a comment or a link.\nExamples of user-generated content spam:\nMalicious sites generated within free hosting sites,\nUnmoderated forum messages,\nSignatures in forum profiles,\nMalicious links in Q&A sites.\nExample:\nTo Prevent UGC Spam:\nIf the forums on your site are too old or outdated, or not adding much value to your users, consider turning them off.\nIf you are complaining about a certain type of spam, you can add the related words such as \"watch movie\" to the blocklist.\nImprove your moderation team.\nSet limits to adding links to messages.\nMonitor the messages of the most active users at times.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7864, "chunk_char_end": 9850}
{"id": "240d6141-a7f7-44a7-b3f8-d6d9ed1bffc8", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "You can also add nofollow attribute to the links shared by users.\nPure Spam Notification on Google Search Console:\nYou should identify spam types that could be generated by users and take precautions. Websites that do not comply with those rules might be given manual action.\nGoogle has\npublished a study\non how user-generated content (UGC) spam affects the user experience. The study was conducted on 3300 participants from India, South Korea, and the USA through Google Play.  1100 people from each country participated in the study. I won't go into detail; the study included these 5 UGC spam types:\n•Gibberish: \"asdsad jksjfs sdhd\"\n•Irrelevant: \"Review of a movie app for a gaming app\"\n•Social Media: \"Follow me on social media @sss\"\n•Abusive language: \"e.g. idiot\"\n•Promotions: \"Instant cash discount, register now”\nYou can watch the video\nhere\n:\nThe effort required to reduce this spam type from 5% to 0% is a lot more than the effort required to reduce it from 10% to 5%. I am sure that Google will continue making quite an effort to improve UGC.\nAutomatically Generated Content\nAutomatically generated content, as the name suggests, includes actions on content that is generated through certain programs or systems without adding any value.\nThese include but are not limited to:\nTexts that are gibberish and translated by an automated tool,\nTexts that only include keywords within no comprehensible context,\nTexts generated through automatic processes such as\nMarkov Chains\n,\nMeaningless content generated using synonymous words,\nPages generated through retrieving data from RSS feeds,\nContent generated through combining other content from different websites without adding any value or moderation.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9851, "chunk_char_end": 11558}
{"id": "ad858a0c-2185-429c-adc5-3e1d3550fa83", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Google presents limited examples of automatically generated content and explains that manual action might be taken on completely different behavior. Also, note that Google has not made any updates on the document page of this manual action type for many years. There is an action type taken on\nautomatic queries\nas well.\nSneaky Redirects\nSneaky redirect is the act of sending a user to a different page than the one they saw on the search results. The underlying reason is to display different content to the user and Googlebot.\nThe first announcement\nregarding such behavior was made on April 30, 2014.\nWhen the user is redirected to a page related to \"x movie\" while the content they saw was about \"structured data\" in the search results, it means that the webmaster directed the user to a different content intentionally.\nAnother example that may lead to a similar mistake is redirecting mobile users to different pages than desktop users. You should make sure that the aim of redirecting (Javascript & 301) reflects the truth not to be affected by this type of manual action.\nYou can check whether your website has a sneaky redirect using the code below:\n<script type=\"text/javascript\">location.replace(\"https://domain.com\");</script>\nNot only the device used for web browsing but also the source from which the user is redirected, in other words, the search engine or social media platform, matters in sneaky redirects. The source should be detected before the redirect. I would like to remind you that unreliable advertising companies caused publisher websites such problems in the past.\nApart from being given manual action, you might lose your users' trust in your site because of this.\nAffiliate Programs\nAffiliation comes from the nature of web, which is quite normal. However, website owners in affiliate marketing business should be more sensitive.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11559, "chunk_char_end": 13419}
{"id": "1b2e91a7-12a3-433f-a0b8-97141e3a91ee", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "If affiliate websites create pages without adding value to users, Google might consider those pages unworthy. The existence of poor content with no value in SERP leads to a negative user experience.\nExamples:\nPages with product affiliate links on which the product descriptions and reviews are directly copied without any added value,\nAffiliate websites with a limited amount of original content.\nI recommend that you do your best for your site to perform better than the original merchant's website. Instead of copying content from the original merchant's website, you can produce original content with high added value to make more sales.\nClick here\nfor further information.\nScraped Content\nWebsites with no or very little original content copied from other sites without adding no values might be given manual action under the name of \"scraped content\".\nContent that is directly copied from even the most qualified sources such as Wikipedia will be poor. In addition, the act of such scraping can sometimes cause copyright problems.\nExamples:\nSites that copy content through bots or manually without adding any value,\nSites that copy content from different sites and modify it slightly or substituting synonyms,\nSites that take content such as images or videos from other sites without any other activity might be penalized with manual action.\nMatt Cutts from Google made an explanation about it on\nReddit\n7 years ago:\nThis is also\nagainst\n(Valuable Inventory: Scraped Content) Google Adsense policy:\nDoorway Pages\nDoorway pages can be best defined as poor intermediate pages created to rank highly for specific search queries, which violates quality guidelines.\nNew York Times has\npublished\nnews about poor search results for 10 years. This manual action, doorway pages,\nfirst started to be used\non March 16, 2015.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13420, "chunk_char_end": 15238}
{"id": "c4226e5e-996d-4026-963f-725525662961", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Doorway pages are malicious pages that only target certain queries and generated, for instance, with district names such as \"Chelsea SEO Agency\" or \"Westminster SEO Agency\". Actually, these pages direct users to the same purpose. The main problem of doorway pages emerges when no value is added to the users. This problem is usually caused by the act of constantly generating the same pages by slightly changing the keywords.\nThat sounds like doorway pages, not something I'd recommend.\n— 🍌 John 🍌 (@JohnMu)\nDecember 10, 2019\nWhile it can be seen on multiple pages of only one site, it is also possible to see it in different domains. At this point, it is important to fully understand the purpose of users and the content that is presented.\nClick here\nfor further details.\nCloaking / Hidden Texts and Links\nCloaking is the act of presenting different content to the users and search engines, which violates Google's guidelines.\nGoogle publishes these two violations separately but I would like to explain them together. Examples of cloaking:\nDisplaying HTML to Googlebot while the users can only see the images and videos,\nDisplaying search keywords to Googlebot only when the user agent is detected while not serving the keywords used in ranking to human users.\nDisplaying keywords to users as they expect to see, while displaying different words to Googlebot by overusing the keywords in HTML,\nHiding text behind an image intentionally,\nUsing white text on a white background to make sure it won't be realized,\nSetting the font size to 0.\nYou can watch the video prepared by Matt Cutts\nhere\n.\nIt is also important not to display a different version to Googlebot for IP redirect. It won't be a problem unless you serve different content to users from the USA and the G-bot, since G-bot crawls IP addresses based in the USA. Hidden keywords may also harm your site.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 8, "chunk_char_start": 15239, "chunk_char_end": 17105}
{"id": "2ceab58c-cf20-4547-a1c2-b4f9febbb66a", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Not every method of cloaking should be considered a violation. Sometimes extra text can be used to explain HTML text on Javascript-based sites, which may not be considered cloaking. It is best to look for manipulative behavior to detect cloaking as a violation.\nMueller was asked whether using hreflang is considered cloaking or not on\nReddit\n, his answer to this was a \"no\":\nYou should also be careful when you do an A/B test. It is\nemphasized\nthat it won't be a problem unless you treat Googlebot differently and that A/B testing should be given a limited lifetime:\nWebsites that offer subscription-based access to some of their content may consider using the\nrelevant structured data\nin order to avoid cloaking.\nBacklink Violations\nPaid backlinks and the link schemes violating the guidelines might cause your website to be penalized.\nGoogle finds it quite natural to pay for advertising websites. However, it recommends adding\nnofollow\nto backlinks or adding\nsponsored\nto sponsored links.\nExamples of link schemes that may be considered violation:\nThe act of exchanging links,\nPaid links without nofollow attribute,\nAggressive use of anchor text,\nNot stating that the backlinks are sponsored,\nGetting backlinks to your site, which are automatically generated through programs designed to do so,\nLow quality web directories,\nUnnatural backlinks from footers.\nYou can benefit from SEO tools that provide strong data for backlink, such as\nCognitiveSEO\n. They may help you differentiate between natural and unnatural links:\nYou can easily detect where backlinks come from. By checking \"unnatural link\" sites specified by such tools, you can also create the best link profile for you:\nYou may consider\nadding nofollow\nto links in your work such as press releases:\nUnnatural links or links without the nofollow attribute may be considered a violation by Google. While manual action can be taken in some cases, sometimes algorithms may prefer not to use the signals coming from these links in ranking.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 9, "chunk_char_start": 17106, "chunk_char_end": 19104}
{"id": "50375ccf-28dd-41da-aa0a-1bde6f330319", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Google explains that random backlinks to your site may be ignored:\nRandom links collected over the years aren't necessarily harmful, we've seen them for a long time too and can ignore all of those weird pieces of web-graffiti from long ago. Disavow links that were really paid for (or otherwise actively unnaturally placed), don't fret the cruft.\n— 🍌 John 🍌 (@JohnMu)\nJanuary 25, 2019\nSo, what does it mean to get natural backlinks? To explain it, I would like to refer to Google's definition:\nI would like to mention 2 manual action types which are unnatural links to your site and unnatural links from your site. In short, both spammy/low-quality links to and from your site may lead to manual action.\nAn explanation\nfor unnatural links to your site:\nAn\nexplanation\nfor unnatural links from your site:\nYou can detect backlinks to your site through Search Console. Click links and then \"top linking domains\" to see all data:\nYou can use tools to\ndisavow unnatural backlinks to your site\n. If unnatural backlinks are the reason for the manual action against your site, I recommend that you explain the details in your reconsideration request:\nSearch Console will still see the disavowed backlinks but Google will process them once it crawls the relevant pages.\nStructured Data Spam\nYou may get manual action with or without knowing it because of the structured data added to your site. Such manual action will cause only your rich search results to be removed from the search results. For instance, your pages that use product schema markup may no longer be displayed in SERP with review stars or price. If you ever happen to violate the guidelines, you are likely to receive a manual action e-mail such as below:\nYou can read the\nstructured data guidelines\nand check your schema paying attention to each guideline specified for each structured data. For example, the specific guidelines for fact check schema are as follows:", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 10, "chunk_char_start": 19105, "chunk_char_end": 21030}
{"id": "f7783bae-f814-43cf-a6c6-2ddaeccc2188", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "If you ever get manual action because of spammy structured data, what you only need to do is to fix the errors in relevant pages and send a reconsideration request.\nContent with Little Value / Thin Value\nIf you create pages or content with little value, all or some parts of your sites may get a manual action penalty because of little added value.\nExamples of content with little added value:\nAutomatically generated content,\nDoorway pages,\nIllegible pages full of punctuation and spelling errors,\nContent with no useful details and value,\nIllegible texts because of poor design,\nScraped content,\nInsufficient unique content.\nYou can watch Google's video on this subject\nhere\n:\nCheck your site for duplicate content to serve more unique and high-quality content. I recommend reading the blog on\nhow to create high-quality content\nif you haven't read it yet.\nSloppy content won't bring you sufficient organic traffic. Plus, the abundance of such content may lead to manual action. Remember that you are responsible for each content on your site and consider updating your content that were created years ago but add no value right now.\nPure Spam\nPure spam is one of the harshest manual actions applied by Google against a website. Sites using aggressive spam methods may get pure spam penalty.\nGetting constant manual actions on websites may also lead to pure spam penalty. For instance, if your site gets penalized for content with little value or unnatural links all the time, you are likely to see a pure spam notification in Search Console. Keyword stuffing or other aggressive spam types come under this category.\nWhen you get a pure spam penalty, your site is removed from Google's index. Within 12 to 24 hours of the manual action notification, Google removes your pages from the index. If you get traffic from platforms such as Google News, your pages may be removed from the news index as well.\nSecurity Issues", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 11, "chunk_char_start": 21031, "chunk_char_end": 22950}
{"id": "4ff40a13-7162-4cc2-bca7-1546f1e008ff", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Security issues are different than manual actions. In order to protect and inform the users and website owners, Google may reveal to the visitors a site or some pages of it being hacked. One of the main differences between manual actions and security issues is that the latter requires spam to be reviewed manually.\nYou can read the article on\nsites with malicious behavior\n. If you have problems such as content injection, you will have to fix them.\nYou may also encounter social engineering content problem. When you receive such notifications, you should follow the steps explained by Google and consider removing relevant pages if that is required:\nYou can also arrange pages mentioned in the malware or unwanted software list. Especially if you have infected files on your website, you must remove them:\nGoogle News & Discover Manual Actions\nYour site may get manual action when\nGoogle News policies are violated\n. The reason for manual action and the affected pages will be seen in Search Console. Your site may also get manual action because of\nGoogle Discover guidelines violations\n. These types of manual action\nhave been public\nsince February 8, 2021.\nThe URLs that cause such a violation will be removed from discover results. This is a warning that your site violates only the news and discover policies. It won't affect Google search results.\nIn addition, the content of the AMP version should have the same topic as that of the canonical web page. The content does not have to be identical but the users should be reaching the same information on both the AMP and the canonical page. If a violation is detected at this point, your site can get manual action for \"AMP content mismatch\".\nReconsideration Requests\nYou need to send a reconsideration request to Google after fixing the problems to remove the manual actions mentioned in this article. You'll see this form only when your site gets a manual action.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 12, "chunk_char_start": 22951, "chunk_char_end": 24873}
{"id": "713454e8-17c8-4ab0-ba51-b3e3d4947dda", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "In your request review form, you should explain all the details of how you have fixed the issues and provide evidence. After that, you need to wait for the reviewers' opinion. Reconsideration requests may take a few days or a week to result. In the end, your request will be either denied or accepted. An example of how to use the form:\nIf your request is denied, I recommend that you not re-send the form before fixing all the issues. Do not re-send the form before receiving the result e-mail because your site could still be reviewed. If your request is accepted, you'll probably receive a notification such as this one:\nAlthough rarely, reviewers may send you some notes to help fix your problems. These notes include requests such as \"provide additional evidence\".\nI would like to answer some frequently asked questions regarding manual actions:\n-My paid domain/site is banned. What should I do?\nIn such cases, consider explaining the situation in your reconsideration request.\n-Can I get multiple manual actions?\nYes, your site may get more than one manual action. You can see an example of it, which I took from a\nvideo\n:\n-How do I get my traffic back after recovering from a manual action?\nYou have got more traffic through spam. So, it is quite normal that you don't get your traffic back since you remove/edit the relevant pages to recover from the manual action.\n-X website has spammy behavior but does not get penalized. Why?\nGoogle is not perfect, of course. When you just go to a site, it is not possible to tell if there is a manual action against it. For instance, X website got manual action because of content with little value but this action may have applied only to its certain pages.\n-My manual action has been removed but I still have problems with indexing. What do I do?\nWhen you recover from penalties such as pure spam, it is possible to see fluctuations in indexing for a couple of days. You should check for problems on your site that could hinder indexing and fix them.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 13, "chunk_char_start": 24874, "chunk_char_end": 26873}
{"id": "367df46e-68ba-4827-9484-58b92932b899", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "While writing this article on Google penalties and manual actions, I tried to be as elaborative as possible and refer to evidence and specialists' explanations. In fact, each manual action is to be elaborated on its own but I wanted to present them all together. I hope it's been useful for dearest ZEO visitors who spared their time to read this article to the final lines.\nThat's it. I wish you all a Search Console account that says \"No issues detected\" :)\nSamet\nÖzsüleyman\nSEO Manager\nSubscribe to Zeo\nRelated\nArticles\nMUVERA - The Technology That Will Speed Up Google Search\nDiscover MUVERA, Google's revolutionary new search technology. Learn how it solves the speed vs. accuracy dilemma and why your SEO strategy must shift from keywords to user intent.\nEzgi Gülsen Yaylı\n,\nSearch Engine Optimization\nWhat is Llms.txt File and What Does It Do?\nLlms.txt is a special text file that allows websites to be understood by artificial intelligence systems and big language models. Click for a detailed review!\nZeo's Guest\n,\nSearch Engine Optimization\nHow to Use Facebook Groups for SEO with the Help of AI\nDiscover how to leverage Facebook groups in your SEO strategy. Check out our blog for tips on analyzing user questions and creating up-to-date content.\nSamet Özsüleyman\n,\nSearch Engine Optimization\nSEO Observations on Grok Web Searches\nExplore how Grok, xAI’s AI chatbot, differs from Google in SEO, offering unique search results, source tracking, and content updates. Learn more in this blog.\nSamet Özsüleyman\n,\nSearch Engine Optimization\nHow to Create a Successful SEO Strategy with Limited Resources?\nLearn how to build an effective SEO strategy on a limited budget. Explore time management, affordable tools, and prioritization to boost your organic traffic.\nSamet Özsüleyman\n,\nSearch Engine Optimization\nHow to Use Google Sheets as a Browser\nLearn how to do web crawl on Google Sheets with Apps Script. Check out our guide to analyze status code, canonical, hreflang, and more for free!", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 14, "chunk_char_start": 26874, "chunk_char_end": 28872}
{"id": "75c21ab7-3952-4cab-beae-9325aee66cb6", "url": "https://zeo.org/resources/blog/the-complete-list-of-google-penalties-manual-actions-guide", "source_domain": "zeo.org", "title": "The Complete List of Google Penalties - Manual Actions Guide | Zeo", "section_path": [], "text": "Yağmur Bayram\n,\nSearch Engine Optimization\n2025 SEO Trends\nDiscover the SEO trends for 2025 in our blog. Learn how to shape your SEO strategy with EEAT, AI, keyword intent, Core Web Vitals, and other key insights!\nZeo's Guest\n,\nSearch Engine Optimization\n8 Tips to Spotlight Your Spotify Podcast\nIn our SEO guide for Spotify Podcast, discover ways to increase your visibility by creating great titles, attention-grabbing descriptions, and compelling cover art, and grow your audience with advanced promotional strategies.\nZeo's Guest\n,\nSearch Engine Optimization\nMaking Efficient Decisions with Correlation Analysis in SEO\nIn this article, you can find out how you can use correlation in your SEO efforts and how to make efficient decisions based on the results.\nSamet Özsüleyman\n,\nSearch Engine Optimization\nHow to Submit a Reconsideration Request to Google\nAll the details about how to submit a reconsideration request against the penalties given by Google are in this article.\nSamet Özsüleyman\n,\nSearch Engine Optimization\nHow to Recover Traffic After Migration?\nYou can read how to bring back your SEO performance and traffic after migration in this article.\nSamet Özsüleyman\n,\nSearch Engine Optimization\nMost Common JSON-LD Schema Issues and Solutions\nIn this article, you can find schema errors and solutions for non-parsable structured data in Google Search Console.\nSamet Özsüleyman\n,\nSearch Engine Optimization", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 15, "chunk_char_start": 28873, "chunk_char_end": 30292}
{"id": "d05794db-ba44-499e-a4cf-a5bee1648e7b", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "What Is Cloaking in SEO: Risks and Consequences - BrandWell\nLooks like you came from one of our great affiliates! This means you'll get\n20% MORE\npost credits on any plan you choose.\nDiscover top guides, trends, tips and expertise from AIO Writers\nInbound Marketing\nWhat Is Cloaking in SEO: Risks and Consequences\nJulia McCoy\nTuesday, 21st May 2024\nIn the cutthroat world of search engine optimization where algorithms reign supreme and visibility is paramount, webmasters and marketers often resort to ingenious tactics to boost their rankings. One such tactic that’s shrouded in controversy is cloaking.\nWhat is cloaking in SEO?\nCloaking refers to the practice of presenting content optimized for search engines to improve rankings while delivering a different version of that content to users. It’s a double-edged sword that promises to boost visibility while flirting with penalties and blacklisting.\nHow does cloaking work and how do we navigate this fine line between optimization and deception?\nLet’s explore this black hat SEO technique called cloaking and its ethical implications.\nTable Of Contents:\nWhat Is Cloaking in SEO?\nWhy Is Cloaking Considered Black Hat?\nCommon Cloaking Techniques\nRisks and Consequences of Cloaking in SEO\nHow to Detect and Avoid Cloaking Practices\nFAQs: What is Cloaking in SEO?\nConclusion\nWhat Is Cloaking in SEO?\nIf you’ve been in the SEO game for a while, you’ve probably heard whispers about the dark art of cloaking. It’s a technique that’s been around since the early days of search engines, but it’s still a hot topic today.\nSo, what exactly is SEO cloaking?\nIn a nutshell, it’s when a website shows one version of a page to search engines and a completely different version to human visitors.\nHow does cloaking work?\nIt all comes down to the server that hosts the website.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1816}
{"id": "cee833a9-e2de-4f1e-8e9a-689ecd7f26fc", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "When a visitor requests a page, the server looks at the IP address or user agent to determine whether it’s a search engine bot or a human. If it’s a bot, the server dishes up a special “cloaked” version of the page that’s stuffed with keywords and optimized for rankings.\nThere are a few different types of website cloaking out there. Here are some common examples of cloaking practices:\nContent Cloaking\nContent cloaking comes in two forms: 1) Showing search engines a page filled with keywords to rank higher, while human visitors see a normal, readable page, and 2) Serving search engines one version of a page and human users a completely different one, such as a page about “best smartphones” for search engines and an unrelated product page for users.\nFor example, a website might show search engines a page with dense keyword usage like “cheap flights to New York,” while actual visitors see a travel blog post with minimal mention of these keywords.\nOr perhaps an affiliate marketer might hide links by showing search engines a page with high-quality content about a product but redirecting users to an affiliate page to generate commissions.\nThe worst type of content cloaking is when a website presents search engines with a highly relevant and authoritative page on a specific topic to gain higher rankings but redirects users to a spammy or malicious website once they click on the search result.\nIP Delivery\nIP delivery is another method of cloaking where different content is served based on the visitor’s IP address. Search engine bots, identified by their IPs, might see one version of the site, while regular users see another.\nUser-Agent Cloaking\nUser-agent cloaking detects the user-agent string of the visitor’s browser to determine if it’s a search engine crawler, delivering optimized content to crawlers and different content to other visitors.\nJavaScript Cloaking", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1817, "chunk_char_end": 3704}
{"id": "30706158-4349-41fc-ae32-075f61e6600c", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "JavaScript cloaking involves using JavaScript to show different content based on whether the user has JavaScript enabled. Search engines, which often do not execute JavaScript, see the original content, while users with JavaScript enabled see different content.\nHTTP Referer Cloaking\nHTTP referer cloaking shows different content based on the HTTP referer header, presenting optimized content to visitors coming from search engines and regular content to direct visitors.\nCSS Display Cloaking\nCSS display cloaking hides or displays different content using CSS, showing content meant for search engines while setting content for users to display.\nI will explain some of these cloaking techniques below.\nNow, you might be thinking,\n“Hey, if it boosts my rankings, what’s the harm?”\nTrust me, SEO cloaking is a risky game. Keep reading to find out why.\nWhy Is Cloaking Considered Black Hat?\nAlright, so we’ve covered what cloaking is and how it works. But why is it considered a big no-no in the world of SEO?\nBecause it doesn’t play by the rules.\nViolates Search Engine Guidelines\nThe major search engines like Google and Bing have strict guidelines about what they consider acceptable SEO practices.\nSpoiler alert: cloaking is definitely on the naughty list.\nGoogle’s webmaster guidelines\nexplicitly state that cloaking is a violation of its rules. They consider it a deceptive and manipulative tactic that goes against their core principles of providing relevant, trustworthy search results to users.\nHere Google lists cloaking as a violation of its spam policies:\nManipulates Search Results\nThe whole point of cloaking is to artificially boost a site’s rankings in the search results. By showing search engines one thing and users another, cloaking allows websites to rank for keywords and phrases that may not actually be relevant to their content.\nThis kind of manipulation goes against the spirit of SEO, which is all about creating great content that naturally attracts links and ranks well.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3705, "chunk_char_end": 5701}
{"id": "fcd80420-5379-45f1-9f78-8502980d28c5", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "With cloaking, it’s all smoke and mirrors.\nProvides Poor User Experience\nAt the end of the day, SEO is about improving the user experience. When someone clicks on a search result, they expect to find content that matches what they saw in the snippet.\nBut with controversial SEO techniques like cloaking, that’s often not the case.\nImagine clicking on a result that promises\n“10 Tips for Growing Your Business,”\nonly to land on a page that’s nothing but ads and\naffiliate links\n.\nTalk about a letdown.\nCloaking may boost rankings in the short term, but it ultimately leads to frustrated users and high bounce rates.\nCommon Cloaking Techniques\nNow that we’ve established why cloaking is a risky move, let’s take a closer look at some of the most common methods website owners use to pull it off.\nIP Cloaking\nOne of the sneakiest types of cloaking is\nIP cloaking\nor IP spoofing.\nThis is when a website serves up different content based on the visitor’s IP address.\nSo, if a search engine bot comes knocking from a known IP range, the server will show them the “optimized” version of the page. But if a regular user visits from a different IP, they’ll see something else entirely.\nUser-Agent Cloaking\nSimilar to IP cloaking, user agent cloaking involves showing different content based on the visitor’s user agent string.\nSearch engine bots have specific user agents that identify them as crawlers, so sneaky website owners can use this information to serve up cloaked pages.\nHTTP Accept-Language Cloaking\nAnother method is HTTP Accept-Language cloaking, which uses the HTTP Accept-Language header to determine what content to show.\nThis header tells the server what language the visitor prefers so cloakers can use it to display different versions of a page based on language settings.\nJavaScript Cloaking\nIn JavaScript cloaking, a website serves different content to users with JavaScript enabled or disabled.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5702, "chunk_char_end": 7609}
{"id": "5c3d80ac-da80-4244-839c-a3a2d424ecbf", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "Search engine crawlers often have limited JavaScript capabilities so showing them content that relies heavily on JavaScript while showing human visitors content that doesn’t can be considered cloaking.\nCSS Cloaking\nThis refers to manipulating the CSS styles of a webpage to hide certain content from search engines while displaying it to users.\nThe practice can involve setting the “display” property to “none” or using other CSS techniques to hide content.\nMeta Refresh Redirects\nSome websites use a meta refresh tag to redirect users to a different page after a certain amount of time.\nThis can be used to show search engines one page while redirecting human visitors to a different page.\nFlash or Image-Based Cloaking\nAnother questionable SEO technique is to present text to search engines in HTML format while displaying the same text as part of an image or Flash file to human visitors.\nSince search engines may have difficulty reading text embedded in images or Flash, this can be a form of cloaking.\nShady website owners weave their deceptive practices just to manipulate rankings on search engines. Whether through hidden text or displaying misleading information, they’re merely aimed at tricking search engine algorithms.\nRisks and Consequences of Cloaking in SEO\nAlright, so you’re probably getting the picture that cloaking is a pretty sketchy SEO tactic. But what are the actual risks and consequences of using it on your website?\nPenalties from Search Engines\nThe biggest risk of cloaking is getting slapped with a penalty from search engines like Google. And trust me, you do not want to be on their bad side.\nIf Google catches you using cloaking on your site, they can dish out some serious penalties. We’re talking about manual actions that can tank your rankings or even get your site\nremoved from the index entirely\n.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7610, "chunk_char_end": 9446}
{"id": "a137b1e3-bd69-4a8d-9050-2ceb7dc46f80", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "Even if you manage to fly under the radar for a while, cloaking is still a ticking time bomb. Google is constantly updating its algorithms to better detect and punish sneaky tactics like this. It’s not a matter of if you’ll get caught, but when.\nLoss of Organic Traffic\nOf course, the whole point of SEO is to drive organic traffic to your website. But if you’re using cloaking and get hit with a penalty, you can kiss that traffic goodbye.\nA manual action or algorithmic demotion can cause your rankings to plummet, which means your site will be buried deep in the search results where no one will find it.\nAnd even if you manage to recover from a penalty, it can take months or even years to claw your way back to the top.\nIs a short-term boost in rankings really worth the risk of losing all your organic traffic? I think not.\nDamage to Website Reputation\nBeyond the immediate consequences of penalties and traffic loss, cloaking can also do some serious damage to your website’s reputation.\nIf word gets out that you’re using shady tactics like cloaking, it can be hard to shake that stigma.\nOther websites will be less likely to link to you, and users will be wary of trusting your content.\nPlus, if you’re caught cloaking, you’ll have a big red flag on your record with search engines.\nEven if you clean up your act and try to play by the rules, it can be an uphill battle to regain their trust.\nThe bottom line? Cloaking just isn’t worth the risk. It may be tempting to take shortcuts and game the system, but in the long run, it’s a losing strategy.\nHow to Detect and Avoid Cloaking Practices\nSo, we’ve established that cloaking is a big no-no in the world of SEO. But how can you tell if a website is using these sneaky tactics?\nAnd more importantly, how can you avoid falling into the cloaking trap yourself?\nUse Cloaking Detection Tools", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9447, "chunk_char_end": 11294}
{"id": "ace350a6-d9f5-44e4-aacb-777ca5e62a95", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "One of the easiest ways to spot cloaking is to use a cloak checker tool. These handy little programs allow you to enter a URL and compare the content that’s served up to search engines versus what human users see.\nThere are plenty of cloaking detection tools out there, both free and paid.\nSome popular options include\nSitechecker\n,\nSmallSEOTools\n, and\nDuplichecker\n.\nJust enter the URL you want to check, and these tools will do the rest.\nMonitor Suspicious Activity\nAnother way to avoid falling victim to cloaking is to keep a close eye on your website’s analytics. If you notice any sudden spikes or drops in traffic, or if your bounce rate starts to skyrocket, it could be a sign that something fishy is going on.\nIt’s also a good idea to regularly check your website’s content to make sure it hasn’t been hacked or compromised in any way. Hackers can sometimes inject cloaked content into a site without the owner even realizing it, so it pays to be vigilant.\nFocus on White Hat SEO Strategies\nOf course, the best way to avoid cloaking is to focus on white-hat SEO strategies that play by the rules.\nInstead of trying to game the system with sneaky tactics, put your energy into creating high-quality, relevant content that\nnaturally attracts links\nand social shares.\nBuild relationships with other websites in your niche and focus on providing value to your audience. Over time, these legitimate SEO efforts will pay off with higher rankings, more traffic, and a solid reputation in your industry.\nThe key is to always prioritize the user experience over quick wins and shortcuts. By staying true to your audience and playing by the rules, you’ll be well on your way to sustainable, long-term SEO success — no cloaking required.\nFAQs: What is Cloaking in SEO?\nWhat does cloaking mean in SEO?\nCloaking in SEO is when a website shows different content to search engines than it does to users to manipulate rankings.\nWhat is an example of cloaking?", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11295, "chunk_char_end": 13246}
{"id": "37c5a5e2-5e22-4e9c-9001-cedad4de7b5c", "url": "https://brandwell.ai/blog/what-is-cloaking-in-seo/", "source_domain": "brandwell.ai", "title": "What Is Cloaking in SEO: Risks and Consequences - BrandWell", "section_path": [], "text": "An example of cloaking would be showing search engines optimized text while displaying flashy images or videos to users.\nWhy is cloaking not recommended by Google?\nCloaking violates Google’s webmaster guidelines because it deceives both the user and the search engine, leading to penalties.\nConclusion\nCloaking tricks search engines by showing different content to users than what’s indexed for ranking purposes. It’s tempting but dangerous ground.\nThe risks? Penalties from search engines can sink your rankings or worse — get you blacklisted entirely.\nAvoid the pitfalls of cloaking by sticking to ethical practices that focus on quality content and user experience.\nWritten by Julia McCoy\nSee more from Julia McCoy\nUNLOCK YOUR POTENTIAL\nLong Headline that highlights Value Proposition of Lead Magnet\nGrab a front row seat to our video masterclasses, interviews, case studies, tutorials, and guides.\ndownload the checklist\nExperience the power of BrandWell\nGet My Free Audit\nExperience the power of BrandWell\nGet My Free Audit\nGet Real Results with\nBrandWell\n“BrandWell is a\nnon-negotiable part of my business\n, saving me so much time and brain space so that I can\nwork on higher-impact projects.”\nSarah Greenberg\nMarketer & Agency Owner\nTo get started, enter your details below.\nWebsite*\nWork Email*\nPlease use a valid professional domain name and email\nIf your company email matches the domain you inputted, you will receive a second free post credit during your trial.\n😃\nYour website has been verified. You have unlocked a free premium post credit!\nUnlock a free premium post credit by using an email address that matches your website\nFirst Name*\nStart Free Trial\nBy continuing, you agree with our\nTerms and Conditions.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 7, "chunk_char_start": 13247, "chunk_char_end": 14971}
{"id": "6693d730-0087-432b-b908-273f1d569245", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "Avoid Google Penalties with These Key SEO Tips - eSEOspace\nSkip to navigation\nSkip to main content\nBlog\nWelcome to eSEOspace!\nLet us get to know you!\nInterested In?\nWeb Design & Development\nSoftware Design & Development\nApp Design & Development\nMonthly Maintenance\nSEO/GEO/AEO Packages\nI want to build my own package\nHome\n/\nSEO\n/\nHow to Avoid Google Penalties: An In-Depth Guide\nSEO\nHow to Avoid Google Penalties: An In-Depth Guide\nBy: Irina Shvaya\n|\nMarch 25, 2025\nTable of Contents\nMaintaining your website's ranking on Google is critical for any SEO professional, digital marketer, or website owner. However, one misstep can lead to the dreaded Google penalty, potentially sinking your site's visibility and traffic.\nBut what exactly is a Google penalty? It's a punishment applied when Google deems your website has violated its webmaster guidelines, either through algorithmic detection or a manual review. This penalty can result in lower rankings or even complete removal from search engine results pages (SERPs). To protect your site’s hard-earned reputation, it’s essential to avoid practices that might trigger one.\nThis in-depth guide will help you understand the types of penalties, how to identify them, and the best practices to ensure your website remains compliant with Google guidelines.\nUnderstanding Google Penalties\nWhat is a Google penalty?\nA Google penalty occurs when your website is flagged for not adhering to Google’s quality guidelines. It can either be algorithmic (triggered automatically by changes in Google’s ranking algorithms, such as updates like Panda, Penguin, or SpamBrain) or manual (issued by Google’s team after a review).\nAlgorithmic Penalties:\nCommonly caused by low-quality or spammy content, unnatural link profiles, or excessive keyword stuffing.\nManual Penalties:\nTypically result from more deliberate violations, such as cloaking, using hidden text, or purchasing spammy links.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1924}
{"id": "a4d33331-8af2-4f26-b87f-cd3dd7b15d77", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "These penalties can severely impact both your website's traffic and credibility. For example, complete de-indexation from Google means your site won’t appear on any searches, effectively rendering it invisible online.\nSigns Your Website Might Be Penalized\nKnowing when your site is penalized is critical. Here are common indicators:\nA sudden, unexplained drop in organic traffic\nKeywords that were ranking well disappear from SERPs\nA notification in Google Search Console (for manual penalties)\nYour site is completely de-indexed from Google\nUsing a reliable Google penalty checker can help identify penalty issues quickly so you can take action.\nCommon Reasons for Google Penalties\n1.\nSpammy Links\nOne of the most common reasons websites face penalties is due to spammy links. Google's Penguin algorithm specifically targets websites with manipulative or unnatural link-building practices.\nKey causes include:\nPurchased backlinks from low-quality, irrelevant websites\nExcessive link exchanges\nAutomatically generated backlinks (e.g., through bots)\nLinks from \"bad neighborhoods,\" such as spammy websites or link farms\n2.\nThin or Duplicate Content\nGoogle’s Panda algorithm focuses on promoting good content while suppressing low-quality pages. If your website relies on:\nDuplicate content stolen or syndicated from other sites\nShort, uninformative articles that add no value\nContent stuffed with target keywords\n…you risk triggering a penalty.\n3.\nKeyword Stuffing\nWhile keywords are vital for SEO optimization, abusing them can have the opposite effect. Keyword stuffing—an outdated practice where certain terms or phrases are unnaturally repeated—can lead to negative consequences.\nExample:\nInstead of writing naturally, like “Our software simplifies SEO for small businesses,” keyword stuffing might look like this:\n“Our SEO software is the best SEO software for SEO professionals who need SEO software.”\n4.\nCloaking and Sneaky Redirects", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1925, "chunk_char_end": 3864}
{"id": "f436f131-c417-461c-b07f-0716b8275731", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "Cloaking refers to showing different content to search engines and users, while sneaky redirects take users to unexpected (and often irrelevant) pages. These are both serious violations of Google’s guidelines.\n5.\nHacked or Compromised Websites\nHacked websites that spread malware or phishing attempts can inadvertently find themselves penalized. Regular security audits are essential for protecting your site.\n6.\nExcessive Ads\nSites filled with excessive ads, especially \"above-the-fold\" ads that disrupt user experience, may be penalized by Google's Page Layout Algorithm.\nHow to Avoid Google Penalties\n1. Develop High-Quality, Original Content\nGoogle rewards good content, so aim to write for humans first and search engines second. Some strategies include:\nConducting thorough research to provide new and valuable insights\nAvoiding duplicate or thin content by focusing on user-centric topics\nRegularly auditing your site to remove outdated or repetitive information\n2. Create a Healthy Backlink Profile\nLinks are an essential ranking factor, but they should be earned, not manipulated. To build a healthy link profile:\nFocus on getting backlinks from authoritative sources relevant to your niche\nUse varied anchor text that aligns naturally with the content\nAvoid using link-building services that promise \"instant” results\nYou can also use tools like Google’s Disavow Tool to handle spammy links pointing to your site.\n3. Prioritize UX and Technical SEO\nYour website’s layout, speed, and accessibility all contribute to compliance with Google's guidelines:\nEnsure that ads don’t overwhelm content.\nOptimize your website for desktop, mobile, and tablet devices.\nImprove site speed and reduce load times.\nImplement HTTPS encryption for data privacy and security.\n4. Use a Google Penalty Checker\nAutomated tools can help you spot potential issues before they lead to a penalty. These tools often analyze:\nBacklink quality\nOn-page SEO factors\nDuplicate content risks\nKeyword optimization", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3865, "chunk_char_end": 5853}
{"id": "36328e7f-aace-4555-9a93-c4ac0212ba20", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "An effective Google penalty checker allows you to stay proactive by identifying red flags in real-time.\n5. Follow Best Practices for SEO\nAvoid \"black-hat\" SEO practices such as hidden text, doorway pages, or automated queries.\nFocus on E-E-A-T (Expertise, Authoritativeness, Trustworthiness) when creating content.\nKeep up with Google updates to adapt your strategy as their algorithms evolve.\nRecovering from a Google Penalty\nIf you've already been penalized, here's how to begin recovery:\nIdentify the Root Cause:\nUse tools like Google Search Console or log file analysis to pinpoint the issue.\nFix the Problem:\nRemove spammy links, rewrite low-quality content, or fix the technical issues causing the penalty.\nSubmit a Reconsideration Request:\nFor manual penalties, explain the steps taken to resolve the issue and promise compliance going forward.\nKeep in mind that recovery isn't instantaneous. Patience and continual monitoring are key.\nWhy Choose eSEOspace to Help Reduce Penalty Risks?\nNavigating Google’s constantly evolving search algorithms can be daunting. eSEOspace specializes in\nSEO\n,\nweb development\n,\nand\napp/software solutions\n, making us your go-to partner for penalty-proofing your website.\nOur team will:\nAudit and optimize your website to align with Google’s current guidelines\nProactively monitor for harmful links or performance dips using state-of-the-art tools\nDeliver data-backed strategies to protect and scale your online presence\nMake Your Website\nCompetitive.\nLeverage our expertise in Website Design + SEO Marketing, and spend your time doing what you love to do!\nGet Started\nWant to Protect Your Business from Google Penalties?\nGet in touch with us today and ensure your site remains at the top of its game.\nTakeaways for the Modern SEO Professional", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5854, "chunk_char_end": 7636}
{"id": "327b9f73-bf99-43ae-b235-f9c2ac8bf41e", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "Google penalties can be devastating to your digital presence, but they are entirely avoidable with the right strategy and tools. By adhering to Google’s webmaster guidelines, using a Google penalty checker, and focusing on good content, you can safeguard your website from unnecessary risks. Remember, it’s always better to invest in compliance now than to recover from penalties later.\nFor professional-grade protection and optimization,\ncontact eSEOspace today\n. Together, we’ll keep you penalty-free and positioned for success.\nMake Your Website\nCompetitive.\nLeverage our expertise in Website Design + SEO Marketing, and spend your time doing what you love to do!\nGet Started\nMeet the Authors\nIrina Shvaya\nFounder & Marketing Consultant\nExpert in SEO, GEO and UI/UX\nBenjamin Gunther\nDirector of Projects & Growth\nExpert in GEO, Web Design, Business Growth Strategies\nYou Might Also like to Read\nWeb Maintenance\nHow WCAG Compliance Impacts Core Web Vitals\nBy: Irina Shvaya\n|\nNovember 28, 2025\nResources\nHow to Boost Your eCommerce Sales With Smarter Customer Insights\nBy: Irina Shvaya\n|\nNovember 28, 2025\nGEO\nHow To Future-Proof Your Business for AI Search\nBy: Irina Shvaya\n|\nNovember 19, 2025\nGEO\nHow To Use ChatGPT To Improve Your GEO\nBy: Irina Shvaya\n|\nNovember 19, 2025\nGEO\nHow To Turn Your Homepage Into an AI Hub\nBy: Irina Shvaya\n|\nNovember 19, 2025\nGEO\nHow To Rewrite an Old Blog for Better AI Ranking\nBy: Irina Shvaya\n|\nNovember 19, 2025\nReady to speak with a marketing expert?\nGive us a call! (916) 866-7893\n0\nWebsites Launched\n0\nMarketing Experts\n0\nHours of Dedicated Work\nHome\nCompany\nSolutions\nWebsite Design\nWordPress Web Design\nShopify Web Design\nWebsite Re-design\nUI/UX Web Design\nCRO for e-Commerce\nCRO for Lead Generation\nWebsite Development\nWordPress Web Development\nShopify Web Development\nUI/UX Web development\nWebsite Re-development\nCustom Website Development\nSearch Engine Optimization\nWordPress SEO\nShopify SEO\nCustom Web SEO\nLocal SEO\nOrganic SEO", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7637, "chunk_char_end": 9609}
{"id": "129e5e04-f06a-4b63-ba25-0c31e684757b", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "Generative Engine Optimization\nAnswer Engine Optimization\nSearch Engine Marketing\nApp Design & Development\niPhone App Development\nAndroid Development\nNative App Development\nCustom App Development\nSoftware Design & Development\nDatabase Development\nApp Development\nCRM Development\nWeb-based Software Development\nWebsite Pricing\nOur Works\nReviews\nInsights\nGEO ACADEMY\nContact Sales\nSidebar\nThank you for your interest!\nWe’ll send pricing soon.\nIn a Hurry? Schedule A Call\n×\nJust answer the following few questions, and\nwe will give you an instant quote!\nHow many services do you offer?\n1-10\n10-20\n20-50\nHow many locations do you service?\n1-5\n5-10\n10-20\n20-50\nGlobally\nClose\nThank you for your interest in our\nservices\n. Please fill out the form below, and we will be in touch shortly!\nService You are Looking For*\nWeb Design & Development\nSoftware Design & Development\nApp Design & Development\nMonthly Maintenance\nSEO/GEO/AEO Packages\nI want to build my own package\nClose\nJust answer the following few questions, and\nwe will give you an instant quote!\nHow many services do you offer?\n1-10\n10-20\n20-50\nHow many locations do you service?\n1-5\n5-10\n10-20\n20-50\nGlobally\nClose\nJust answer the following few questions, and\nwe will give you an instant quote!\nHow many services do you offer?\n1-10\n10-20\n20-50\nHow many locations do you service?\n1-5\n5-10\n10-20\n20-50\nGlobally\nClose\nInterested in\nService\n. Please fill out the form below, and we will give you the pricing right away!\nClose\nJust answer the following few questions, and\nwe will give you an instant quote!\nDo you have a current website?\nYes\nNo\nHow many pages will be on your new website?\n5+\n10+\n15+\n20+\n25+\n30+\n35+\n40+\nHow many leads are you currently generating on your website on a monthly basis? (We specialize in SEO Preservation to ensure you do not lose any rankings).\n0-100\n100-200\n200-500\n500-1000\nCompany type\nB2B Company\nB2C Company\nClose\nJust answer the following few questions, and\nwe will give you an instant quote!", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9610, "chunk_char_end": 11588}
{"id": "d53c46cf-140f-4cf7-b857-68e9939c16b0", "url": "https://eseospace.com/how-to-avoid-google-penalties/", "source_domain": "eseospace.com", "title": "Avoid Google Penalties with These Key SEO Tips - eSEOspace", "section_path": [], "text": "Do you have a current website?\nYes\nNo\nHow many pages will be on your new website?\n5+\n10+\n15+\n20+\n25+\n30+\n35+\n40+\nHow many leads are you currently generating on your website on a monthly basis? (We specialize in SEO Preservation to ensure you do not lose any rankings).\n0-100\n100-200\n200-500\n500-1000\nCompany type\nB2B Company\nB2C Company\nClose\nJust answer the following few questions, and\nwe will give you an instant quote!\nDo you have a current website?\nYes\nNo\nHow many pages will be on your new website?\n5+\n10+\n15+\n20+\n25+\n30+\n35+\n40+\nHow many leads are you currently generating on your website on a monthly basis? (We specialize in SEO Preservation to ensure you do not lose any rankings).\n0-100\n100-200\n200-500\n500-1000\nCompany type\nB2B Company\nB2C Company\nClose\nThank you for your interest in our\nService\n. Please fill out the form below, and someone will call you in a jiffy!\nClose\nSchedule a FREE Discovery Call\nClose\nSchedule A Discovery Call\n(916) 866-7893", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11589, "chunk_char_end": 12553}
{"id": "0f920803-89c4-4111-8ad5-75b9d044e438", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025\nSkip to content\nBad SEO Practices and Mistakes You Want to Avoid in 2025\nLocal SEO\nJuly 19, 2025\nDon’t Let Google Punish You for Bad SEO Practices and Mistakes\nDo you know the bad SEO practices and mistakes that will cause your organic page rankings to plummet? If you’re working hard to improve your SEO results, read on. Here’s a valuable list of bad SEO practices and mistakes you can make that can downgrade your ranking, or even get your webpage removed entirely!\nDon’t Waste Your Efforts with Bad SEO Practices\nSome companies will do almost anything to get to the top of the SERP (Search Engine Results Page). It’s not hard to see why:\nThe top three Google search results get\n54.4%\nof all clicks.\nLeads from SEO have a\n14.6%\nclose rate.\n53.3%\nof all website traffic comes from organic searches.\nAs the real estate on the SERP for organic results shrinks, some companies take on risky SEO techniques. While some companies do so knowingly, While some companies do so knowingly, many are unaware of Google’s policies and the severe penalties it can impose on those who try to game the system. In this article, we share SEO techniques that can send you to the bottom of the SEO heap—and how to avoid Google’s wrath.\n20 Top B2B SEO Trends for your Website in 2025\nGoogle Watches Out for Unfair SEO Techniques and\nBad SEO Practices\nGoogle’s goal is to provide the most relevant, high-quality results for every user search. It doesn’t take\nunfair bad SEO practices\nlightly and has two types of penalties it can take to teach the offending website owner a lesson. Google’s policy manuals call these algorithmic and manual actions.\nAlgorithm Actions\nThe Googlebot web crawler detects about 99% of spam content or about\n40 billion spam pages per day", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 0, "chunk_char_start": 0, "chunk_char_end": 1806}
{"id": "e48b776f-e5b3-41df-9b47-20d27a142646", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": ". When Googlebot spots violations, it can automatically downrank or delist a page. With algorithmic actions, once the issue is fixed, Googlebot will remove the penalty the next time it revisits your page.\nManual Actions\nThis article focuses on the 1% of penalties manually imposed by Google’s human webspam review team for violating\nGoogle Search Essentials\n. If a website owner receives a manual action, they must fix the violation and send Google a reconsideration request. If Google denies the request, the website owner can make further efforts and submit another request. Lifting a manual penalty can take weeks or longer, and you may not be able to regain your ranking and traffic.\nYes, manual actions have serious consequences. How can you avoid them?\nThe Best defense is a good offense. Below are common reasons for manual actions and some ways to address them. This is not a complete list, nor does it provide all the solutions, but it’s a solid start for avoiding damaging penalties for SEO violations.\nMarrying SEO with UX: A Winning Combination\n1. Spam\nThe many types of spam listed in\nGoogle spam policies\ncan be divided into several categories. Sites can be abused through third-party spam when, for example, people put up spam links in a user comment section. User-generated scams occur when people add spam content or links in areas such as user profiles and forums. Spam can also occur if a website has a free host that supports spammy websites that violate Google guidelines.\nWhen building or modifying your website, always hire a reputable developer or agency\nto build a compliant site\nand use a host that offers fast, safe, and secure hosting when building or modifying a website.\n2. Keyword Stuffing and Hidden Text\nGoogle can spot keyword stuffing when key phrases are excessive, seem unnatural, or are out of context. Hidden text, a form of keyword stuffing, contains text or links that contain keywords that are visible to search engines but not to visitors.", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 1, "chunk_char_start": 1807, "chunk_char_end": 3789}
{"id": "78277640-d5d6-49be-a7e4-d9d2ccc52c2a", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": "Here are some ways to avoid keyword stuffing:\nUse the\ntarget keyword\nonly once in the title, sparingly in H2 tags and metatags, and evenly sprinkle them—in context—in the content.\nReplace exact-match keywords with synonyms, long-tail keywords, and rephrased terms.\nProperly format your text with headings and paragraphs and strictly limit the use of bold, italics, and all caps.\nOn-Page B2B SEO Trends To Implement in 2025\n3. Unnatural Links\nGoogle Search Console will send a notification if it detects “a pattern of unnatural, artificial, deceptive, or manipulative links pointing to your site,” which can occur from buying links or participating in linking schemes. Google also penalizes sites with spammy outbound links.\nDon’t participate in any linking scheme, undo all the forum spam, and remove low-quality links to and from your site. If you can’t remove unnatural links, add them to your disavow file to deactivate them.\n4. Thin Content\nAnother top cause of manual actions is thin content, with lots of pages that provide visitors with little value.\nAvoid affiliate cookie-cutter pages, poorly written syndicated copy, and keyword-stuffed, low-value doorway pages that only exist to manipulate search rankings. You can avoid this violation by removing low-quality or shallow content and replacing it with high-quality, valuable content.\n5. Duplicate Content\nDuplicate content is when more than one URL has very similar or exact copies of the same content. When this occurs, web crawlers become confused about which pages to present. In Google’s words,\n“Google tries hard to index and show pages with distinct information,” so duplicate pages can tank rankings and, in the most extreme cases, can cause Google to remove the entire site from search results.\nCommon causes of duplicate content include:\nImproperly managing WWW and non-WWW variations\nGranting access with both HTTP and HTTPS\nUsing both trailing slashes and non-trailing slashes\nIncluding scraped or copied content", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 2, "chunk_char_start": 3790, "chunk_char_end": 5774}
{"id": "b48753a3-cf47-4c5a-b2be-2d43fc1def6e", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": "Having separate mobile and desktop versions\nTips to avoid duplicate content are auditing your site for duplicate content, monitoring indexed pages in the Console, and implementing canonical tags and 301 redirects as needed.\nFront End and Back End SEO: The Secret to Search Engine Rankings\n6. Spammy Auto-Generated Content\nChat GPT, Open AI, and other AI systems can serve up low-quality or incorrect content that people don’t bother to verify. Lack of quality and value causes the pages to suffer in rankings. Always review your site to identify and prune low-quality pages, carefully spell check and proofread copy, and write in a human-friendly way.\n7. Incorrectly Structured Data\nStructured data based on\nSchema.org technical and search guidelines\nhelps Google more easily access, understand, and interpret webpages. However, inappropriate structured data used to manipulate user behavior can lead to manual penalties, including the page becoming ineligible to appear in valuable rich results. Web developers should strictly conform to Schema.org guidelines for each type of structured data.\n8.  Paid Links\nCompanies can purposefully or unknowingly fall prey to link schemes, the black hat SEO practice of buying or selling links to artificially inflate search rankings. One notorious example is link farming, which uses a network of low-quality, auto-generated sham sites that all link to each other. The best backlinking strategy is to build relationships with authoritative sources that publish high-quality content.\n9. Cloaking, Cloaked Images, and Sneaky Redirects\nCloaking is the deceptive practice of showing one landing page to search engines and a significantly different one to users. Similarly, sneaky redirects send users to content they don’t expect, often a highly commercialized page. When detected, Google will likely demote the page or even completely drop it from search results.\n10. Sneaky Mobile Redirects", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 3, "chunk_char_start": 5775, "chunk_char_end": 7703}
{"id": "92bcc730-09a9-4f91-8f28-b72c3c5bb72f", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": "In sneaky mobile redirects, computer and mobile see the same URL in search results. However, the link sends computer users to the correct page and mobile users to another—often a WAP-click affiliate program created for mobile traffic monetization. Website owners may also be unaware of mobile redirects caused by hackers and third-party scripts. Avoid Google potentially removing these pages from the index by choosing advertisers and partners who are transparent on how they handle user traffic.\nYour MUST Have Monster SEO Checklist for 2025\n11. AMP Content Mismatch\nAMP (Accelerated Mobile Page) content is a stripped-down HTML version of a webpage that loads quickly on mobile devices—leading to better user experience and SEO. If the AMP version and the canonical master version of the page don’t match, Google will drop the mobile-optimized AMP page. Always verify that the AMP page is associated with the correct canonical page and confirm that what Google and the mobile user see is essentially the same.\n12. News and Discover Policy Violations\nGoogle’s manual actions used to be limited to search-related violations. As of 2021, websites can receive manual penalties for violating Google News and Google Discover guidelines. Examples of such violations include content that is adult-themed, misleading, terrorist, harassing, dangerous, harassing, or lacks transparency.\nDon’t Let Bad SEO Practices and SEO Mistakes Hurt Your Business\nIt’s better to be safe than sorry with your\nSEO efforts\nand hard-won top rankings. Keeping up with all of Google’s policies and changing search algorithms is difficult, but it pays to take the necessary precautions to avoid manual actions. When choosing a web design or SEO agency, be sure they are above-board, reputable, and knowledgeable about Google’s SEO policies—and how to avoid costly penalties.\nNeed some help fixing bad seo practices and mistakes?\ncontact us for an estimate\nLysa Miller", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 4, "chunk_char_start": 7704, "chunk_char_end": 9642}
{"id": "186d8908-c9e7-4771-afb1-21cdbc764549", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": "Lysa Miller is the powerhouse behind Ladybugz Interactive, a nationally recognized Boston web design and digital marketing agency. Known for her bold leadership and no-nonsense approach, Lysa has built an award-winning agency that's caught national attention — landing features on the Boston Business Journal’s Book of Lists, Agency Vista’s Women-Owned Agencies to Watch, Cloudways’ Top Ten Women-Owned Agencies, and ranking among Clutch.co’s Top 3 Women-Owned Agencies in the U.S. in 2023.\nIn just over four years, Lysa has led Ladybugz to launch more than 40 websites, support over 55 ongoing clients, and grow 10 strategic digital partnerships — all while building real community connections. She’s also the founder and president of the MetroWest Women’s Network, uniting more than 5,000 women entrepreneurs and leaders.\nA passionate advocate for business growth and community impact, Lysa serves on the board of Fresh Start Furniture Bank and as an elected corporator for Main Street Bank — proving that success is about lifting others up along the way.\nLadybuzz: The Website Design Agency Blog\nGet the latest in website design and digital marketing tips and trends for 2024 from our Boston seasononed experts.\nsee more highlights\nThe Complete Guide to SEO vs AEO vs GEO: Search, Answers & AI Optimization for 2026\nWhat’s the difference between SEO vs AEO vs GEO in digital marketing? These terms are shorthand for Search Engine Optimization (SEO), Answer Engine Optimization (AEO), and Generative Engine Optimization (GEO)....\nIs Your Performance Arts Website Prepared for Holiday Traffic Spikes?\nThis season is prime time for performing arts websites to prepare for holiday traffic spikes. While this article focuses on dance studios, it also applies to music and theater centers....\nThe Complete Guide to Accessible Women’s Healthcare Website Design (WCAG, ADA, UX & HIPAA)", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 5, "chunk_char_start": 9643, "chunk_char_end": 11523}
{"id": "63f75b9b-ea37-4038-a603-b25e16e9593f", "url": "https://www.ladybugz.com/bad-seo-practices-and-seo-mistakes-you-want-to-avoid/", "source_domain": "www.ladybugz.com", "title": "Bad SEO Practices and SEO Mistakes You Want to Avoid in 2025", "section_path": [], "text": "Introduction: Why accessibility is essential in women’s healthcare website design Accessibility in women’s healthcare website design is not optional; it is a moral, ethical, and legal responsibility that directly affects...\nWe’re creative, agile, flexible and fast.\nlet us create your website\nSubtraction-31-svg\nWe’ve earned our way onto some of Boston’s top digital agency lists.\nPrevious\nNext\nSearch\nPDF Subscribers\nFull Name\nEmail Address\nSubscribe\nWeb Design + Dev\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Ut elit tellus, luctus nec ullamcorper mattis, pulvinar dapibus leo.\nLearn More\nDigital Marketing\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Ut elit tellus, luctus nec ullamcorper mattis, pulvinar dapibus leo.\nLearn More\nWeb Support\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Ut elit tellus, luctus nec ullamcorper mattis, pulvinar dapibus leo.\nLearn More", "engine": "generic", "topic": "bad_practices", "doc_type": "guideline", "lang": "en", "version_date": null, "crawled_at": "2025-12-06T17:16:08.284296Z", "chunk_index": 6, "chunk_char_start": 11524, "chunk_char_end": 12433}
